{"EoH": "import numpy as np\n\ndef select_next_node(current_node: int, destination_node: int, unvisited_nodes: np.ndarray, distance_matrix: np.ndarray) -> int:\n    \"\"\"\n    Design a novel algorithm to select the next node in each step.\n\n    Args:\n    current_node: ID of the current node.\n    destination_node: ID of the destination node.\n    unvisited_nodes: Array of IDs of unvisited nodes.\n    distance_matrix: Distance matrix of nodes.\n\n    Return:\n    ID of the next node to visit.\n    \"\"\"\n    if len(unvisited_nodes) == 1:\n        return unvisited_nodes[0]\n    \n    # Compute immediate distances to unvisited nodes\n    distances = distance_matrix[current_node, unvisited_nodes]\n    \n    # Compute centrality as before\n    centrality = np.sum(distance_matrix[:, unvisited_nodes], axis=0)\n    centrality = centrality / np.max(centrality) if np.max(centrality) > 0 else np.ones_like(centrality)\n    \n    # Temporal proximity score: inverse distance (simulated)\n    proximity_score = 1 / (distances + 1e-6)\n    \n    # Trajectory bias: assign higher bias to nodes that are geometrically aligned with the direction of movement from current to destination\n    # Compute direction vector from current to destination\n    direction_vector = np.array([destination_node - current_node])  # Simplified direction as ID difference (not actual coordinates)\n    \n    # Simulate trajectory alignment: use node coordinates (assumed available via distance_matrix inference) \u2014 here we approximate using distance-based directional consistency\n    # For novelty, we create a directional consistency score based on the angular alignment with movement direction\n    # We assume a directional score based on the relative position (this is a heuristic approximation)\n    # Compute pairwise differences in node IDs as a proxy for spatial direction (not accurate without coordinates)\n    # Instead, we use a synthetic direction score based on distance differences\n    directional_consistency = np.abs(unvisited_nodes - current_node) / (np.max(unvisited_nodes) - np.min(unvisited_nodes) + 1e-6)\n    \n    # Normalize directional consistency\n    directional_consistency = directional_consistency / np.max(directional_consistency) if np.max(directional_consistency) > 0 else np.ones_like(directional_consistency)\n    \n    # Combine proximity, centrality, and trajectory bias\n    # Weighted score: proximity * centrality + trajectory_bias * (1 - centrality) to encourage exploration\n    bias_weight = 0.3  # adjustable parameter for exploration\n    trajectory_score = directional_consistency * bias_weight\n    combined_score = (proximity_score * centrality) + (trajectory_score * (1 - centrality))\n    \n    # Apply entropy regularization: add a small random perturbation to avoid deterministic selection\n    entropy_noise = np.random.dirichlet([0.5] * len(unvisited_nodes))\n    combined_score += entropy_noise * 0.05\n    \n    next_node_idx = np.argmax(combined_score)\n    return unvisited_nodes[next_node_idx]\n\n",
  "funsearch": "import numpy as np\n\ndef select_next_node(current_node: int, destination_node: int, unvisited_nodes: np.ndarray, distance_matrix: np.ndarray) -> int:\n    \"\"\"\n    Design a novel algorithm to select the next node in each step.\n\n    Args:\n    current_node: ID of the current node.\n    destination_node: ID of the destination node.\n    unvisited_nodes: Array of IDs of unvisited nodes.\n    distance_matrix: Distance matrix of nodes.\n\n    Return:\n    ID of the next node to visit.\n    \"\"\"\n    next_node = unvisited_nodes[0]\n\n    return next_node\n\n",
  "mctsahd": "import numpy as np\n\ndef select_next_node(current_node: int, destination_node: int, unvisited_nodes: np.ndarray, distance_matrix: np.ndarray) -> int:\n    \"\"\"\n    Design a novel algorithm to select the next node in each step.\n\n    Args:\n    current_node: ID of the current node.\n    destination_node: ID of the destination node.\n    unvisited_nodes: Array of IDs of unvisited nodes.\n    distance_matrix: Distance matrix of nodes.\n\n    Return:\n    ID of the next node to visit.\n    \"\"\"\n    if len(unvisited_nodes) == 1:\n        return unvisited_nodes[0]\n    \n    best_next = unvisited_nodes[0]\n    best_score = float('inf')\n    \n    current_dists = [distance_matrix[current_node][node] for node in unvisited_nodes]\n    avg_dist = np.mean(current_dists)\n    \n    # Dynamic connectivity threshold based on local density\n    unvisited_density = len(unvisited_nodes)\n    base_threshold = np.percentile(current_dists, 70)  # Focus on reachable neighbors\n    dynamic_threshold = max(base_threshold, avg_dist * 1.1)\n    \n    # Compute shared neighbor overlap with adaptive threshold\n    neighbor_counts = []\n    for node in unvisited_nodes:\n        shared_neighbors = 0\n        for other in unvisited_nodes:\n            if other != node:\n                # Only count edges below dynamic threshold\n                if distance_matrix[node][other] < dynamic_threshold:\n                    shared_neighbors += 1\n        neighbor_counts.append(shared_neighbors)\n    \n    # Temporal memory: penalize nodes that have been \"visited\" in a previous step (simulated via edge frequency)\n    # Create a pseudo-frequency map of edges between unvisited nodes (based on previous step patterns)\n    # Simulated via neighbor overlap recurrence: if a node has high prior connectivity, penalize it\n    edge_frequency = np.zeros(len(unvisited_nodes))\n    for i, node in enumerate(unvisited_nodes):\n        # Count how many other unvisited nodes are within threshold distance\n        freq = sum(1 for j in range(len(unvisited_nodes)) \n                   if j != i and distance_matrix[node][unvisited_nodes[j]] < dynamic_threshold)\n        edge_frequency[i] = freq / len(unvisited_nodes) if len(unvisited_nodes) > 0 else 0\n    \n    # Destination potential field: use a non-linear decay with adaptive weight\n    dest_factor = np.array([1 / (1 + distance_matrix[node][destination_node] ** 1.5) \n                            for node in unvisited_nodes])\n    \n    # Composite score: cost + connectivity + destination + temporal penalty\n    for i, node in enumerate(unvisited_nodes):\n        dist_cost = current_dists[i]\n        conn_gain = neighbor_counts[i] / (unvisited_density * 1.2)  # Normalize by density\n        dest_gain = dest_factor[i]\n        temp_penalty = edge_frequency[i] * 0.6  # Penalize frequent patterns\n        \n        # Score combines components with non-linear weighting\n        score = dist_cost + 0.35 * conn_gain + 0.3 * dest_gain - 0.25 * temp_penalty\n        \n        # Only consider nodes below average distance, with a small buffer\n        if dist_cost < avg_dist * 0.95:\n            if score < best_score:\n                best_score = score\n                best_next = node\n    \n    return best_next\n\n",
  "reevo": "import numpy as np\n\ndef select_next_node(current_node: int, destination_node: int, unvisited_nodes: np.ndarray, distance_matrix: np.ndarray) -> int:\n    \"\"\"\n    Design a novel algorithm to select the next node in each step.\n\n    Args:\n    current_node: ID of the current node.\n    destination_node: ID of the destination node.\n    unvisited_nodes: Array of IDs of unvisited nodes.\n    distance_matrix: Distance matrix of nodes.\n\n    Return:\n    ID of the next node to visit.\n    \"\"\"\n    if len(unvisited_nodes) == 1:\n        return unvisited_nodes[0]\n    \n    # Compute direct distances to all unvisited nodes\n    distances = distance_matrix[current_node, unvisited_nodes]\n    \n    # Simulate realistic spatial coordinates using a non-uniform, clustered layout\n    # Nodes are placed in clusters with varying densities and global structure\n    node_coords = np.array([\n        (np.sin(i * 0.3 + np.random.rand() * 0.1) * (1 + 0.5 * np.cos(i * 0.1 + np.random.rand() * 0.2)),\n         np.cos(i * 0.3 + np.random.rand() * 0.1) * (1 + 0.5 * np.sin(i * 0.1 + np.random.rand() * 0.2)))\n        for i in range(len(distance_matrix))\n    ])\n    \n    # Get coordinates of current node and unvisited nodes\n    current_x, current_y = node_coords[current_node]\n    unvisited_coords = node_coords[unvisited_nodes]\n    \n    # Compute direction vectors from current node to each unvisited node\n    dx = unvisited_coords[:, 0] - current_x\n    dy = unvisited_coords[:, 1] - current_y\n    \n    # Avoid zero direction\n    direction_magnitudes = np.hypot(dx, dy)\n    direction_magnitudes = np.where(direction_magnitudes == 0, 1, direction_magnitudes)\n    \n    # Normalize direction vectors\n    direction_angles = np.arctan2(dy, dx)\n    \n    # Dynamic trend adaptation: update global trend using exponential moving average\n    # This allows the trend to adapt over time, avoiding rigid assumptions\n    # Initialize trend with a small bias to avoid instability\n    alpha = 0.2  # Trend adaptation rate\n    global_trend_angle = np.mean(direction_angles)\n    \n    # Update trend dynamically: use a weighted average of current directions\n    # This makes the algorithm responsive to the evolving spatial distribution of unvisited nodes\n    global_trend_angle = alpha * np.mean(direction_angles) + (1 - alpha) * global_trend_angle\n    \n    # Compute directional deviation from the global trend\n    angle_diffs = direction_angles - global_trend_angle\n    # Use circular distance to avoid discontinuities\n    angle_diffs = np.abs(angle_diffs)\n    angle_diffs = np.minimum(angle_diffs, 2 * np.pi - angle_diffs)\n    \n    # Directional alignment score: higher alignment = higher score\n    directional_score = 1 / (1 + angle_diffs)\n    \n    # Weighted distance: prefer paths aligned with the dynamic spatial trend\n    weighted_distances = distances / (1 + directional_score)  # Lower values when aligned\n    \n    # Spatial clustering: favor nodes that are part of dense clusters (high local density)\n    # Compute pairwise distances between unvisited nodes to infer cluster structure\n    unvisited_distance_matrix = distance_matrix[unvisited_nodes[:, None], unvisited_nodes]\n    # Compute local density: number of neighbors within a small radius\n    # Use a threshold to identify clusters\n    radius_threshold = 0.5\n    # Compute distance to each unvisited node's neighbors\n    distances_to_neighbors = unvisited_distance_matrix\n    # Thresholded density: count how many neighbors are within radius\n    neighbor_count = np.sum(distances_to_neighbors <= radius_threshold, axis=1)\n    # Normalize density to range [0,1] and add as a clustering bonus\n    clustering_bonus = (neighbor_count + 1) / (len(unvisited_nodes) + 1)\n    \n    # Weighted distance includes clustering preference: prefer nodes in dense regions\n    weighted_distances *= clustering_bonus\n    \n    # Destination avoidance: avoid visiting nodes too close to destination prematurely\n    if destination_node in unvisited_nodes:\n        dest_dist = distance_matrix[current_node, destination_node]\n        dest_penalty = np.zeros(len(unvisited_nodes))\n        for idx, node in enumerate(unvisited_nodes):\n            if node != destination_node:\n                # Penalize nodes that are close to destination\n                dist_to_dest = distance_matrix[current_node, node]\n                # Use a sigmoid-like penalty that increases as distance to destination decreases\n                # This encourages spreading out to avoid converging too early\n                dist_diff = abs(dist_to_dest - dest_dist)\n                penalty = np.exp(-0.5 * (dist_diff / 1.5)**2)\n                dest_penalty[idx] = penalty\n            else:\n                dest_penalty[idx] = 1.0\n        weighted_distances += dest_penalty\n    \n    # Final selection: pick the node with the minimum weighted distance\n    next_node_idx = np.argmin(weighted_distances)\n    next_node = unvisited_nodes[next_node_idx]\n    \n    return next_node\n\n",
  "LHS": "import numpy as np\n\ndef select_next_node(current_node: int, destination_node: int, unvisited_nodes: np.ndarray, distance_matrix: np.ndarray) -> int:\n    if unvisited_nodes.size == 0:\n        return int(destination_node)\n\n    cand = unvisited_nodes.astype(int)\n    r = distance_matrix[current_node, cand]\n    d = distance_matrix[np.ix_(cand, cand)].copy()\n\n    # Determine the number of unvisited nodes\n    n = int(np.sum(unvisited_nodes == cand))\n\n    # Normalize the distances using a soft\u2011min operation\n    d = np.exp(-d / (1e-6 + np.max(d) + 1e-6))\n\n    # Calculate a weighted score based on the distances and the number of remaining nodes\n    score = (r + (d.sum(axis=1) * (1.0 / (n + 1e-6)))).clip(0, None)\n\n    # Use deterministic noise for tie\u2011breaking\n    noise = np.arange(len(score)) * 1e-9\n    score += noise\n\n    return int(cand[int(np.argmin(score))])\n\n"
}