{
  "max_cover_greedy": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Pick the subset that covers the most currently-uncovered elements.\"\"\"\n    rem = set(remaining_elements)\n    best = None\n    best_gain = 0\n    for s in remaining_subsets:\n        gain = len(rem.intersection(s))\n        if gain > best_gain:\n            best_gain = gain\n            best = s\n    return best if best_gain > 0 else None\n",
  "max_cover_min_size_tiebreak": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Max uncovered coverage; break ties by smaller subset size.\"\"\"\n    rem = set(remaining_elements)\n    best = None\n    best_gain = -1\n    best_size = 10**18\n    for s in remaining_subsets:\n        gain = len(rem.intersection(s))\n        if gain > best_gain or (gain == best_gain and gain > 0 and len(s) < best_size):\n            best_gain = gain\n            best_size = len(s)\n            best = s\n    return best if best_gain > 0 else None\n",
  "cost_effectiveness": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Pick max (new_covered / subset_size).\"\"\"\n    rem = set(remaining_elements)\n    best = None\n    best_score = -1.0\n    for s in remaining_subsets:\n        new = len(rem.intersection(s))\n        if new <= 0:\n            continue\n        score = new / max(1, len(s))\n        if score > best_score:\n            best_score = score\n            best = s\n    return best\n",
  "rarity_weighted_gain": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Weight each newly covered element by 1/frequency across remaining subsets.\"\"\"\n    rem = set(remaining_elements)\n    freq: Dict[int,int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    best = None\n    best_score = -1.0\n    for s in remaining_subsets:\n        sc = 0.0\n        for e in set(s):\n            if e in rem:\n                sc += 1.0 / max(1, freq.get(e, 1))\n        if sc > best_score and sc > 0:\n            best_score = sc\n            best = s\n    return best\n",
  "rarity_squared_gain": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"More aggressive rare-element preference using 1/freq^2 weights.\"\"\"\n    rem = set(remaining_elements)\n    freq: Dict[int,int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    best = None\n    best_score = -1.0\n    for s in remaining_subsets:\n        sc = 0.0\n        for e in set(s):\n            if e in rem:\n                f = max(1, freq.get(e, 1))\n                sc += 1.0 / (f * f)\n        if sc > best_score and sc > 0:\n            best_score = sc\n            best = s\n    return best\n",
  "overlap_penalized_gain": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"New coverage minus a penalty for overlapping already-selected elements.\"\"\"\n    rem = set(remaining_elements)\n    covered_selected: Set[int] = set()\n    for s in selected_subsets:\n        covered_selected.update(s)\n\n    lam = 0.25  # overlap penalty strength\n    best = None\n    best_score = -1e18\n    for s in remaining_subsets:\n        sset = set(s)\n        new = len(rem.intersection(sset))\n        if new <= 0:\n            continue\n        overlap = len(sset.intersection(covered_selected))\n        score = new - lam * overlap\n        if score > best_score:\n            best_score = score\n            best = s\n    return best\n",
  "diversity_bonus_jaccard": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prefer sets that both cover new elements and are dissimilar to selected (high Jaccard distance).\"\"\"\n    rem = set(remaining_elements)\n    selected_sets = [set(s) for s in selected_subsets] if selected_subsets else []\n\n    best = None\n    best_score = -1e18\n    for s in remaining_subsets:\n        sset = set(s)\n        new = len(rem.intersection(sset))\n        if new <= 0:\n            continue\n        if not selected_sets:\n            diversity = 1.0\n        else:\n            # average Jaccard distance to selected sets\n            dsum = 0.0\n            for t in selected_sets:\n                inter = len(sset.intersection(t))\n                uni = len(sset.union(t))\n                jac = inter / max(1, uni)\n                dsum += (1.0 - jac)\n            diversity = dsum / len(selected_sets)\n        score = new * (1.0 + 0.5 * diversity)\n        if score > best_score:\n            best_score = score\n            best = s\n    return best\n",
  "two_step_lookahead": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Score = immediate gain + gamma * best next-step gain after taking this set.\"\"\"\n    rem0 = set(remaining_elements)\n    gamma = 0.6\n    best = None\n    best_score = -1e18\n\n    # Precompute sets\n    sets = [set(s) for s in remaining_subsets]\n\n    for i, sset in enumerate(sets):\n        gain1 = len(rem0.intersection(sset))\n        if gain1 <= 0:\n            continue\n        rem1 = rem0 - sset\n        best_gain2 = 0\n        for j, tset in enumerate(sets):\n            if j == i:\n                continue\n            g2 = len(rem1.intersection(tset))\n            if g2 > best_gain2:\n                best_gain2 = g2\n        score = gain1 + gamma * best_gain2\n        if score > best_score:\n            best_score = score\n            best = remaining_subsets[i]\n    return best\n",
  "unique_element_bonus": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prefer sets containing elements that appear in only one remaining subset.\"\"\"\n    rem = set(remaining_elements)\n    freq: Dict[int,int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    best = None\n    best_score = -1e18\n    for s in remaining_subsets:\n        sset = set(s)\n        new = rem.intersection(sset)\n        if not new:\n            continue\n        unique = sum(1 for e in new if freq.get(e, 0) == 1)\n        score = len(new) + 2.0 * unique\n        if score > best_score:\n            best_score = score\n            best = s\n    return best\n",
  "rarest_element_pivot": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Pick the rarest uncovered element, then choose best set containing it (max new cover, then min size).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    freq: Dict[int,int] = {e:0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in freq:\n                freq[e] += 1\n\n    # element with minimum positive frequency\n    pivot = None\n    pivot_f = 10**18\n    for e, f in freq.items():\n        if 0 < f < pivot_f:\n            pivot_f = f\n            pivot = e\n    if pivot is None:\n        return None\n\n    best = None\n    best_gain = -1\n    best_size = 10**18\n    for s in remaining_subsets:\n        if pivot not in s:\n            continue\n        gain = len(rem.intersection(s))\n        if gain > best_gain or (gain == best_gain and len(s) < best_size):\n            best_gain = gain\n            best_size = len(s)\n            best = s\n    return best if best_gain > 0 else None\n",
  "maximin_rarity": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Choose set that best improves the worst-case rarity among elements it newly covers.\"\"\"\n    rem = set(remaining_elements)\n    freq: Dict[int,int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    best = None\n    best_tuple = None  # (min_rarity, total_gain, -size)\n    for s in remaining_subsets:\n        new = [e for e in set(s) if e in rem]\n        if not new:\n            continue\n        rarities = [1.0 / max(1, freq.get(e, 1)) for e in new]\n        t = (min(rarities), len(new), -len(s))\n        if best_tuple is None or t > best_tuple:\n            best_tuple = t\n            best = s\n    return best\n",
  "random_among_topk_gain": "import numpy as np\nimport random\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Randomly pick among the top-k sets by uncovered gain (adds diversification).\"\"\"\n    rem = set(remaining_elements)\n    scored = []\n    for s in remaining_subsets:\n        g = len(rem.intersection(s))\n        if g > 0:\n            scored.append((g, s))\n    if not scored:\n        return None\n    scored.sort(key=lambda x: x[0], reverse=True)\n    k = max(1, int(np.ceil(0.10 * len(scored))))\n    top = [s for _, s in scored[:k]]\n    return random.choice(top)\n",
  "softmax_sampling_gain": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Sample a set with probability proportional to exp(gain / temp).\"\"\"\n    rem = set(remaining_elements)\n    gains = []\n    cand = []\n    for s in remaining_subsets:\n        g = len(rem.intersection(s))\n        if g > 0:\n            gains.append(g)\n            cand.append(s)\n    if not cand:\n        return None\n\n    temp = max(1e-6, np.median(gains))  # scale temperature to instance\n    w = np.exp(np.array(gains, dtype=float) / temp)\n    w = w / w.sum()\n    idx = int(np.random.choice(len(cand), p=w))\n    return cand[idx]\n",
  "annealed_softmax_gain": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Softmax sampling with temperature decreasing as coverage progresses.\"\"\"\n    rem = set(remaining_elements)\n    gains = []\n    cand = []\n    for s in remaining_subsets:\n        g = len(rem.intersection(s))\n        if g > 0:\n            gains.append(g)\n            cand.append(s)\n    if not cand:\n        return None\n\n    # estimate progress by fraction of already-covered universe proxy\n    n_rem = max(1, len(rem))\n    n_total = n_rem + len({e for ss in selected_subsets for e in ss})\n    progress = 1.0 - (n_rem / max(1, n_total))\n\n    base = max(1e-6, np.mean(gains))\n    temp = base * (1.5 - 1.3 * progress)  # cool down over time\n    w = np.exp(np.array(gains, dtype=float) / max(1e-6, temp))\n    w = w / w.sum()\n    idx = int(np.random.choice(len(cand), p=w))\n    return cand[idx]\n",
  "progressive_size_penalty": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Early: cover aggressively; late: penalize large sets to reduce redundancy.\"\"\"\n    rem = set(remaining_elements)\n    n_rem = max(1, len(rem))\n    n_total = n_rem + len({e for ss in selected_subsets for e in ss})\n    progress = 1.0 - (n_rem / max(1, n_total))\n\n    best = None\n    best_score = -1e18\n    for s in remaining_subsets:\n        new = len(rem.intersection(s))\n        if new <= 0:\n            continue\n        alpha = 0.2 + 1.8 * progress  # size penalty grows late\n        score = new / (len(s) ** alpha if len(s) > 0 else 1.0)\n        if score > best_score:\n            best_score = score\n            best = s\n    return best\n",
  "mean_size_balancing": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prefer sets with strong gain but size close to mean remaining-set size.\"\"\"\n    rem = set(remaining_elements)\n    if not remaining_subsets:\n        return None\n    mean_size = float(np.mean([len(s) for s in remaining_subsets]))\n\n    best = None\n    best_score = -1e18\n    for s in remaining_subsets:\n        new = len(rem.intersection(s))\n        if new <= 0:\n            continue\n        size = len(s)\n        size_pen = abs(size - mean_size) / max(1.0, mean_size)\n        score = new - 0.5 * size_pen\n        if score > best_score:\n            best_score = score\n            best = s\n    return best\n",
  "harmonic_rarity_gain": "import numpy as np\nfrom typing import List, Optional, Dict\nimport math\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Weight new elements by 1/(1+ln(freq)) to temper extreme rarity bias.\"\"\"\n    rem = set(remaining_elements)\n    freq: Dict[int,int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    best = None\n    best_score = -1.0\n    for s in remaining_subsets:\n        sc = 0.0\n        for e in set(s):\n            if e in rem:\n                f = max(1, freq.get(e, 1))\n                sc += 1.0 / (1.0 + math.log(f))\n        if sc > best_score and sc > 0:\n            best_score = sc\n            best = s\n    return best\n",
  "avoid_unique_leftovers": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prioritize covering elements that currently have only one remaining covering set.\"\"\"\n    rem = set(remaining_elements)\n    freq: Dict[int,int] = {e:0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in freq:\n                freq[e] += 1\n\n    best = None\n    best_tuple = None  # (unique_covered, total_covered, -size)\n    for s in remaining_subsets:\n        new = [e for e in set(s) if e in rem]\n        if not new:\n            continue\n        unique_cov = sum(1 for e in new if freq.get(e, 0) == 1)\n        t = (unique_cov, len(new), -len(s))\n        if best_tuple is None or t > best_tuple:\n            best_tuple = t\n            best = s\n    return best\n",
  "redundancy_elimination_probe": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prefer sets that could make a currently-selected set redundant while still covering new elements.\"\"\"\n    rem = set(remaining_elements)\n    sel_sets = [set(s) for s in selected_subsets]\n\n    best = None\n    best_score = -1e18\n    for s in remaining_subsets:\n        sset = set(s)\n        new = len(rem.intersection(sset))\n        if new <= 0:\n            continue\n\n        # redundancy score: how many selected sets are (almost) contained in this candidate\n        red = 0\n        for t in sel_sets:\n            if len(t) > 0 and len(t - sset) == 0:\n                red += 1\n        score = new + 1.5 * red\n        if score > best_score:\n            best_score = score\n            best = s\n    return best\n",
  "low_overlap_with_remaining": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"High gain, but penalize overlap with other remaining sets (to keep options diverse).\"\"\"\n    rem = set(remaining_elements)\n    sets = [set(s) for s in remaining_subsets]\n\n    # element -> how many remaining sets contain it (restricted to remaining elements)\n    freq: Dict[int,int] = {}\n    for sset in sets:\n        for e in sset:\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    best = None\n    best_score = -1e18\n    for s, sset in zip(remaining_subsets, sets):\n        new_elems = [e for e in sset if e in rem]\n        if not new_elems:\n            continue\n        gain = len(new_elems)\n        # elements with high freq mean more overlap potential\n        overlap_pen = sum((freq[e] - 1) for e in new_elems)\n        score = gain - 0.15 * overlap_pen\n        if score > best_score:\n            best_score = score\n            best = s\n    return best\n",
  "conflict_then_rarity_switch": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Early: cover high-frequency elements (reduce branching); late: cover rare ones.\"\"\"\n    rem = set(remaining_elements)\n    freq: Dict[int,int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    n_rem = max(1, len(rem))\n    n_total = n_rem + len({e for ss in selected_subsets for e in ss})\n    progress = 1.0 - (n_rem / max(1, n_total))\n\n    best = None\n    best_score = -1e18\n    for s in remaining_subsets:\n        new = [e for e in set(s) if e in rem]\n        if not new:\n            continue\n        if progress < 0.5:\n            sc = sum(freq.get(e, 1) for e in new)  # high-degree focus\n        else:\n            sc = sum(1.0 / max(1, freq.get(e, 1)) for e in new)  # rarity focus\n        if sc > best_score:\n            best_score = sc\n            best = s\n    return best\n",
  "frequency_spread_bonus": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prefer sets covering elements across a wide range of frequencies (hedges uncertainty).\"\"\"\n    rem = set(remaining_elements)\n    freq: Dict[int,int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    best = None\n    best_score = -1e18\n    for s in remaining_subsets:\n        new = [e for e in set(s) if e in rem]\n        if not new:\n            continue\n        fvals = np.array([freq.get(e, 1) for e in new], dtype=float)\n        spread = float(fvals.std())  # 0 if all similar\n        score = len(new) + 0.4 * spread\n        if score > best_score:\n            best_score = score\n            best = s\n    return best\n",
  "rare_pair_bonus": "import numpy as np\nfrom typing import List, Optional, Dict\nimport itertools\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Bonus for covering pairs of uncovered elements that are individually rare (proxy for hard structure).\"\"\"\n    rem = set(remaining_elements)\n    freq: Dict[int,int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    best = None\n    best_score = -1e18\n    for s in remaining_subsets:\n        elems = [e for e in set(s) if e in rem]\n        if not elems:\n            continue\n        base = len(elems)\n        # compute a cheap pair bonus (cap work)\n        elems_sorted = sorted(elems, key=lambda e: freq.get(e, 1))[:12]  # focus on rarest few\n        bonus = 0.0\n        for a, b in itertools.combinations(elems_sorted, 2):\n            bonus += 1.0 / (max(1, freq.get(a, 1)) * max(1, freq.get(b, 1)))\n        score = base + 3.0 * bonus\n        if score > best_score:\n            best_score = score\n            best = s\n    return best\n",
  "maximize_remaining_support": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Choose set that, after removing its covered elements, leaves uncovered elements with higher total support.\"\"\"\n    rem0 = set(remaining_elements)\n    sets = [set(s) for s in remaining_subsets]\n\n    # current support counts for each remaining element\n    support0: Dict[int,int] = {e:0 for e in rem0}\n    for sset in sets:\n        for e in sset:\n            if e in support0:\n                support0[e] += 1\n\n    best = None\n    best_score = -1e18\n    for i, sset in enumerate(sets):\n        gain = len(rem0.intersection(sset))\n        if gain <= 0:\n            continue\n        rem1 = rem0 - sset\n        if not rem1:\n            return remaining_subsets[i]\n        # proxy: minimize number of elements left with support==1 (fragile)\n        fragile = sum(1 for e in rem1 if support0.get(e, 0) == 1)\n        score = gain - 0.8 * fragile\n        if score > best_score:\n            best_score = score\n            best = remaining_subsets[i]\n    return best\n",
  "adaptive_density_by_overlap": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Adapt size-penalty based on how overlapping the remaining family is.\"\"\"\n    rem = set(remaining_elements)\n    if not remaining_subsets:\n        return None\n\n    # estimate overlap: average Jaccard among a small sample of remaining sets\n    sets = [set(s) for s in remaining_subsets]\n    m = len(sets)\n    idx = np.random.choice(m, size=min(m, 25), replace=False)\n    sample = [sets[i] for i in idx]\n    jac_vals = []\n    for a in range(len(sample)):\n        for b in range(a+1, len(sample)):\n            inter = len(sample[a].intersection(sample[b]))\n            uni = len(sample[a].union(sample[b]))\n            jac_vals.append(inter / max(1, uni))\n    overlap = float(np.mean(jac_vals)) if jac_vals else 0.0\n\n    # higher overlap => stronger size penalty (prefer compact discriminative sets)\n    alpha = 0.6 + 1.6 * overlap\n\n    best = None\n    best_score = -1e18\n    for s in remaining_subsets:\n        new = len(rem.intersection(s))\n        if new <= 0:\n            continue\n        score = new / (len(s) ** alpha if len(s) > 0 else 1.0)\n        if score > best_score:\n            best_score = score\n            best = s\n    return best\n",
  "highest_degree_weighted_gain": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Weight newly covered elements by their current frequency (covers 'hard-to-separate' common elements).\"\"\"\n    rem = set(remaining_elements)\n    freq: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    best, best_score = None, -1.0\n    for s in remaining_subsets:\n        new = [e for e in set(s) if e in rem]\n        if not new:\n            continue\n        score = float(sum(freq.get(e, 0) for e in new))\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "tfidf_blend_gain": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Blend common+rare via TF (gain) + IDF-like weights on newly covered elements.\"\"\"\n    rem = set(remaining_elements)\n    m = max(1, len(remaining_subsets))\n\n    freq: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        new = [e for e in set(s) if e in rem]\n        if not new:\n            continue\n        tf = len(new)\n        idf = 0.0\n        for e in new:\n            f = max(1, freq.get(e, 1))\n            idf += np.log((m + 1.0) / f)\n        score = tf + 0.35 * idf\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "minimize_fragile_support_after_pick": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prefer sets that, after removal, leave few uncovered elements with support==1 (fragile).\"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem0}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        sset = set(s)\n        gain = len(rem0.intersection(sset))\n        if gain <= 0:\n            continue\n        rem1 = rem0 - sset\n        fragile = sum(1 for e in rem1 if support.get(e, 0) == 1)\n        score = gain - 0.9 * fragile\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "minimax_support_leftover": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Maximize gain, but prefer moves that minimize the maximum support among leftover elements (balances future choices).\"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem0}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    best, best_tuple = None, None  # (gain, -max_support_left, -size)\n    for s in remaining_subsets:\n        sset = set(s)\n        gain = len(rem0.intersection(sset))\n        if gain <= 0:\n            continue\n        rem1 = rem0 - sset\n        max_sup = max((support.get(e, 0) for e in rem1), default=0)\n        t = (gain, -max_sup, -len(s))\n        if best_tuple is None or t > best_tuple:\n            best_tuple, best = t, s\n    return best\n",
  "gain_minus_log_size": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Score = gain - lambda*log(1+|S|) to gently discourage very large sets.\"\"\"\n    rem = set(remaining_elements)\n    lam = 0.8\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        gain = len(rem.intersection(s))\n        if gain <= 0:\n            continue\n        score = gain - lam * np.log1p(len(s))\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "sqrt_diminishing_returns": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Use diminishing returns: score = sqrt(gain) / (1+|S|^0.5).\"\"\"\n    rem = set(remaining_elements)\n    best, best_score = None, -1.0\n    for s in remaining_subsets:\n        gain = len(rem.intersection(s))\n        if gain <= 0:\n            continue\n        denom = 1.0 + (len(s) ** 0.5)\n        score = (gain ** 0.5) / denom\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "log_diminishing_returns": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Diminishing returns with log: score = log(1+gain) / (1+log(1+|S|)).\"\"\"\n    rem = set(remaining_elements)\n    best, best_score = None, -1.0\n    for s in remaining_subsets:\n        gain = len(rem.intersection(s))\n        if gain <= 0:\n            continue\n        score = np.log1p(gain) / (1.0 + np.log1p(len(s)))\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "future_topq_mean_gain": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Score = immediate gain + gamma * mean(top-q future gains) after taking this set.\"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    gamma = 0.5\n    q = 0.2\n\n    sets = [set(s) for s in remaining_subsets]\n    best_i, best_score = None, -1e18\n\n    for i, sset in enumerate(sets):\n        gain1 = len(rem0.intersection(sset))\n        if gain1 <= 0:\n            continue\n        rem1 = rem0 - sset\n        if not rem1:\n            return remaining_subsets[i]\n\n        gains2 = []\n        for j, tset in enumerate(sets):\n            if j == i:\n                continue\n            g2 = len(rem1.intersection(tset))\n            if g2 > 0:\n                gains2.append(g2)\n        if not gains2:\n            score = gain1\n        else:\n            gains2.sort(reverse=True)\n            k = max(1, int(np.ceil(q * len(gains2))))\n            score = gain1 + gamma * float(np.mean(gains2[:k]))\n\n        if score > best_score:\n            best_score, best_i = score, i\n\n    return remaining_subsets[best_i] if best_i is not None else None\n",
  "future_best3_lookahead": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Score = gain1 + a*best2 + b*best3 (cheap 3-step proxy without branching).\"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    a, b = 0.55, 0.25\n    sets = [set(s) for s in remaining_subsets]\n\n    best_i, best_score = None, -1e18\n    for i, sset in enumerate(sets):\n        gain1 = len(rem0.intersection(sset))\n        if gain1 <= 0:\n            continue\n        rem1 = rem0 - sset\n        if not rem1:\n            return remaining_subsets[i]\n\n        g2_list = []\n        for j, tset in enumerate(sets):\n            if j == i:\n                continue\n            g2 = len(rem1.intersection(tset))\n            if g2 > 0:\n                g2_list.append(g2)\n        g2_list.sort(reverse=True)\n        best2 = g2_list[0] if len(g2_list) > 0 else 0\n        best3 = g2_list[1] if len(g2_list) > 1 else 0\n\n        score = gain1 + a * best2 + b * best3\n        if score > best_score:\n            best_score, best_i = score, i\n\n    return remaining_subsets[best_i] if best_i is not None else None\n",
  "maximize_support_entropy_leftover": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef _entropy(counts: np.ndarray) -> float:\n    s = float(counts.sum())\n    if s <= 0:\n        return 0.0\n    p = counts / s\n    p = p[p > 0]\n    return float(-(p * np.log(p)).sum())\n\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prefer moves that keep support distribution diverse (higher entropy) among leftovers.\"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    support0: Dict[int, int] = {e: 0 for e in rem0}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support0:\n                support0[e] += 1\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        sset = set(s)\n        gain = len(rem0.intersection(sset))\n        if gain <= 0:\n            continue\n        rem1 = rem0 - sset\n        if not rem1:\n            return s\n        counts = np.array([support0.get(e, 0) for e in rem1], dtype=float)\n        ent = _entropy(counts)\n        score = gain + 0.35 * ent\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "lexicographic_cover_rarest_first": "import numpy as np\nfrom typing import List, Optional, Dict, Tuple\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Lexicographically maximize coverage of rarest elements first (vector dominance heuristic).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    freq: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in freq:\n                freq[e] += 1\n\n    # bucket elements by frequency (small freq = higher priority)\n    uniq_freqs = sorted({f for f in freq.values() if f > 0})\n    if not uniq_freqs:\n        return None\n\n    best, best_vec = None, None\n    for s in remaining_subsets:\n        sset = set(s)\n        cov = [e for e in sset if e in rem]\n        if not cov:\n            continue\n        # build vector: counts covered at each frequency level\n        vec = []\n        for f in uniq_freqs:\n            vec.append(sum(1 for e in cov if freq.get(e, 0) == f))\n        vec.append(-len(s))\n        t = tuple(vec)\n        if best_vec is None or t > best_vec:\n            best_vec, best = t, s\n    return best\n",
  "rare_to_common_ratio": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Maximize (rare_covered / (1+common_covered)) with a gain floor.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    freq: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    # define rare/common by median frequency\n    fvals = np.array(list(freq.values()), dtype=float)\n    med = float(np.median(fvals)) if fvals.size else 1.0\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        cov = [e for e in set(s) if e in rem]\n        if not cov:\n            continue\n        rare = sum(1 for e in cov if freq.get(e, 0) <= med)\n        common = len(cov) - rare\n        if len(cov) < 2:\n            continue\n        score = rare / (1.0 + common)\n        # lightly prefer more absolute coverage too\n        score = score + 0.05 * len(cov)\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "intersection_graph_centrality": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prefer sets that touch many other sets via remaining elements (central in the intersection graph).\"\"\"\n    rem = set(remaining_elements)\n    sets = [set(s) for s in remaining_subsets]\n\n    # element -> how many sets contain it (restricted to remaining elements)\n    ef: Dict[int, int] = {}\n    for sset in sets:\n        for e in sset:\n            if e in rem:\n                ef[e] = ef.get(e, 0) + 1\n\n    best, best_score = None, -1e18\n    for s, sset in zip(remaining_subsets, sets):\n        new = [e for e in sset if e in rem]\n        if not new:\n            continue\n        gain = len(new)\n        # centrality proxy: sum of (ef[e]-1) over newly covered elements\n        touch = float(sum(max(0, ef[e] - 1) for e in new))\n        score = gain + 0.15 * touch\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "intersection_graph_sparsity": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prefer sets that cover well but intersect few others via remaining elements (keeps future choices distinct).\"\"\"\n    rem = set(remaining_elements)\n    sets = [set(s) for s in remaining_subsets]\n\n    ef: Dict[int, int] = {}\n    for sset in sets:\n        for e in sset:\n            if e in rem:\n                ef[e] = ef.get(e, 0) + 1\n\n    best, best_score = None, -1e18\n    for s, sset in zip(remaining_subsets, sets):\n        new = [e for e in sset if e in rem]\n        if not new:\n            continue\n        gain = len(new)\n        intersect_pressure = float(sum(max(0, ef[e] - 1) for e in new))\n        score = gain - 0.20 * intersect_pressure\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "two_rarest_pivot_pair": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Pick two rarest uncovered elements; if possible choose a set covering both; else fallback to rarest pivot.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    freq: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in freq:\n                freq[e] += 1\n\n    candidates = [(f, e) for e, f in freq.items() if f > 0]\n    if not candidates:\n        return None\n    candidates.sort()\n\n    e1 = candidates[0][1]\n    e2 = candidates[1][1] if len(candidates) > 1 else None\n\n    best = None\n    best_gain = -1\n\n    if e2 is not None:\n        for s in remaining_subsets:\n            if e1 in s and e2 in s:\n                g = len(rem.intersection(s))\n                if g > best_gain or (g == best_gain and best is not None and len(s) < len(best)):\n                    best_gain = g\n                    best = s\n        if best is not None:\n            return best\n\n    # fallback: pick best set that contains e1\n    for s in remaining_subsets:\n        if e1 in s:\n            g = len(rem.intersection(s))\n            if g > best_gain or (g == best_gain and best is not None and len(s) < len(best)):\n                best_gain = g\n                best = s\n    return best if best_gain > 0 else None\n",
  "pairwise_synergy_low_cooccurrence": "import numpy as np\nfrom typing import List, Optional, Dict, Tuple\nimport itertools\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Bonus for covering pairs of uncovered elements that rarely co-occur across remaining subsets.\"\"\"\n    rem = set(remaining_elements)\n    sets = [set(s) for s in remaining_subsets]\n\n    # pair frequency computed on a small element sample for speed\n    elem_freq: Dict[int, int] = {}\n    for sset in sets:\n        for e in sset:\n            if e in rem:\n                elem_freq[e] = elem_freq.get(e, 0) + 1\n\n    # focus on up to K rare elements to define pair stats\n    K = 40\n    rare_elems = sorted(elem_freq.keys(), key=lambda e: elem_freq[e])[:K]\n    rare_set = set(rare_elems)\n\n    pair_freq: Dict[Tuple[int, int], int] = {}\n    for sset in sets:\n        elems = sorted([e for e in sset if e in rare_set])\n        for a, b in itertools.combinations(elems, 2):\n            pair_freq[(a, b)] = pair_freq.get((a, b), 0) + 1\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        elems = sorted([e for e in set(s) if e in rem])\n        if not elems:\n            continue\n        base = len(elems)\n        # synergy bonus: sum 1/(1+pair_freq)\n        bonus = 0.0\n        elems2 = [e for e in elems if e in rare_set][:18]\n        for a, b in itertools.combinations(sorted(elems2), 2):\n            bonus += 1.0 / (1.0 + pair_freq.get((a, b), 0))\n        score = base + 0.9 * bonus\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "top_gain_smallest_size_percentile": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Among the top p by gain, pick the smallest set (robust tie-breaking).\"\"\"\n    rem = set(remaining_elements)\n    scored = []\n    for s in remaining_subsets:\n        g = len(rem.intersection(s))\n        if g > 0:\n            scored.append((g, len(s), s))\n    if not scored:\n        return None\n\n    scored.sort(key=lambda x: x[0], reverse=True)\n    p = 0.15\n    k = max(1, int(np.ceil(p * len(scored))))\n    top = scored[:k]\n    top.sort(key=lambda x: (x[1], -x[0]))\n    return top[0][2]\n",
  "smallest_percentile_then_max_gain": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Among the smallest p by size, pick the one with maximum uncovered gain.\"\"\"\n    rem = set(remaining_elements)\n    cand = []\n    for s in remaining_subsets:\n        g = len(rem.intersection(s))\n        if g > 0:\n            cand.append((len(s), g, s))\n    if not cand:\n        return None\n\n    cand.sort(key=lambda x: x[0])\n    p = 0.20\n    k = max(1, int(np.ceil(p * len(cand))))\n    small = cand[:k]\n    small.sort(key=lambda x: (-x[1], x[0]))\n    return small[0][2]\n",
  "adaptive_quantile_explore_exploit": "import numpy as np\nimport random\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Explore early by sampling from a wider quantile of gains; exploit late by narrowing to top gains.\"\"\"\n    rem = set(remaining_elements)\n    scored = []\n    for s in remaining_subsets:\n        g = len(rem.intersection(s))\n        if g > 0:\n            scored.append((g, s))\n    if not scored:\n        return None\n\n    n_rem = max(1, len(rem))\n    n_total = n_rem + len({e for ss in selected_subsets for e in ss})\n    progress = 1.0 - (n_rem / max(1, n_total))\n\n    scored.sort(key=lambda x: x[0], reverse=True)\n    # quantile shrinks from 40% early to 10% late\n    q = 0.40 - 0.30 * progress\n    k = max(1, int(np.ceil(q * len(scored))))\n    return random.choice([s for _, s in scored[:k]])\n",
  "three_phase_core_middle_fringe": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"3-phase strategy: early target high-frequency core, mid maximize gain, late target rare fringe.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    freq: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    n_rem = max(1, len(rem))\n    n_total = n_rem + len({e for ss in selected_subsets for e in ss})\n    progress = 1.0 - (n_rem / max(1, n_total))\n\n    fvals = np.array(list(freq.values()), dtype=float)\n    q1 = float(np.quantile(fvals, 0.33)) if fvals.size else 1.0\n    q2 = float(np.quantile(fvals, 0.66)) if fvals.size else 1.0\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        new = [e for e in set(s) if e in rem]\n        if not new:\n            continue\n        if progress < 0.33:\n            # core: emphasize high frequency\n            sc = sum(1 for e in new if freq.get(e, 0) >= q2)\n            score = 2.0 * sc + 0.1 * len(new)\n        elif progress < 0.66:\n            # middle: plain gain\n            score = float(len(new))\n        else:\n            # fringe: emphasize rare\n            sc = sum(1 for e in new if freq.get(e, 0) <= q1)\n            score = 2.0 * sc + 0.1 * len(new)\n\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "criticality_by_min_set_size": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Weight elements by 1/min_size_of_set_that_can_cover_it (harder elements get higher weight).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    min_sz: Dict[int, int] = {e: 10**18 for e in rem}\n    for s in remaining_subsets:\n        ss = set(s)\n        sz = len(ss)\n        for e in ss:\n            if e in min_sz and sz < min_sz[e]:\n                min_sz[e] = sz\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            continue\n        score = 0.0\n        for e in new:\n            ms = min_sz.get(e, 10**18)\n            if ms < 10**18:\n                score += 1.0 / max(1, ms)\n        # small nudge toward more coverage\n        score += 0.05 * len(new)\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "reserve_index_weighting": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Weight new elements by (1/support) * (1/avg_set_size_covering_element): prefer scarce + 'tight' elements.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem}\n    size_sum: Dict[int, int] = {e: 0 for e in rem}\n\n    for s in remaining_subsets:\n        ss = set(s)\n        sz = len(ss)\n        for e in ss:\n            if e in support:\n                support[e] += 1\n                size_sum[e] += sz\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            continue\n        score = 0.0\n        for e in new:\n            sup = max(1, support.get(e, 1))\n            avg_sz = size_sum.get(e, 0) / sup\n            score += (1.0 / sup) * (1.0 / max(1.0, avg_sz))\n        score += 0.02 * len(new)\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "maximize_gain_to_remaining_ratio": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Score = gain / |remaining_elements| (helps normalize across instances / phases).\"\"\"\n    rem = set(remaining_elements)\n    denom = float(max(1, len(rem)))\n\n    best, best_score = None, -1.0\n    for s in remaining_subsets:\n        gain = len(rem.intersection(s))\n        if gain <= 0:\n            continue\n        score = gain / denom\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "expected_steps_proxy_minimize": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Minimize a proxy of steps left: steps ~= 1 + ceil(|rem1| / best_future_gain).\"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    sets = [set(s) for s in remaining_subsets]\n    best_i, best_score = None, -1e18\n\n    for i, sset in enumerate(sets):\n        gain1 = len(rem0.intersection(sset))\n        if gain1 <= 0:\n            continue\n        rem1 = rem0 - sset\n        if not rem1:\n            return remaining_subsets[i]\n\n        best_future = 0\n        for j, tset in enumerate(sets):\n            if j == i:\n                continue\n            g2 = len(rem1.intersection(tset))\n            if g2 > best_future:\n                best_future = g2\n        steps_proxy = 1.0 + np.ceil(len(rem1) / max(1, best_future))\n        # maximize negative steps_proxy, with gain as tie-break\n        score = -steps_proxy + 0.05 * gain1\n        if score > best_score:\n            best_score, best_i = score, i\n\n    return remaining_subsets[best_i] if best_i is not None else None\n",
  "gain_squared_soft_sampling": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Stochastic choice: sample proportional to gain^2 (more greedy than softmax, still diverse).\"\"\"\n    rem = set(remaining_elements)\n    gains = []\n    cand = []\n    for s in remaining_subsets:\n        g = len(rem.intersection(s))\n        if g > 0:\n            cand.append(s)\n            gains.append(float(g * g))\n    if not cand:\n        return None\n    w = np.array(gains, dtype=float)\n    w = w / w.sum()\n    idx = int(np.random.choice(len(cand), p=w))\n    return cand[idx]\n",
  "selected_overlap_dynamic_penalty": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Penalize overlap with selected sets; penalty increases as progress increases (late-stage cleanup).\"\"\"\n    rem = set(remaining_elements)\n    covered_selected: Set[int] = set()\n    for s in selected_subsets:\n        covered_selected.update(s)\n\n    n_rem = max(1, len(rem))\n    n_total = n_rem + len(covered_selected)\n    progress = 1.0 - (n_rem / max(1, n_total))\n    lam = 0.1 + 0.9 * progress\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        gain = len(rem.intersection(ss))\n        if gain <= 0:\n            continue\n        overlap = len(ss.intersection(covered_selected))\n        score = gain - lam * overlap\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "boundary_elements_first": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prioritize covering 'boundary' elements with low support (<=2), then maximize total gain.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    boundary = {e for e, c in support.items() if 0 < c <= 2}\n\n    best, best_tuple = None, None  # (boundary_covered, total_gain, -size)\n    for s in remaining_subsets:\n        ss = set(s)\n        new = rem.intersection(ss)\n        if not new:\n            continue\n        b = len(boundary.intersection(ss))\n        t = (b, len(new), -len(s))\n        if best_tuple is None or t > best_tuple:\n            best_tuple, best = t, s\n    return best\n",
  "boundary_ratio": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Maximize (boundary_covered / (1+size)) with a minimum total gain safeguard.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    boundary = {e for e, c in support.items() if 0 < c <= 2}\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        gain = len(rem.intersection(ss))\n        if gain <= 0:\n            continue\n        b = len(boundary.intersection(ss))\n        score = b / (1.0 + len(ss)) + 0.03 * gain\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "progress_blend_gain_rarity_overlap": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Blend gain + rarity bonus - overlap penalty, with weights shifting over progress.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    covered_selected: Set[int] = set()\n    for s in selected_subsets:\n        covered_selected.update(s)\n\n    n_rem = max(1, len(rem))\n    n_total = n_rem + len(covered_selected)\n    progress = 1.0 - (n_rem / max(1, n_total))\n\n    freq: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    w_gain = 1.2 - 0.6 * progress\n    w_rare = 0.3 + 1.1 * progress\n    w_ov  = 0.1 + 0.9 * progress\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            continue\n        gain = len(new)\n        rare = sum(1.0 / max(1, freq.get(e, 1)) for e in new)\n        ov = len(ss.intersection(covered_selected))\n        score = w_gain * gain + w_rare * rare - w_ov * ov\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "exp_rarity_weighted_gain": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Weight uncovered elements by exp(-freq/scale) (smooth rarity emphasis).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    freq: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    if not freq:\n        return None\n\n    fvals = np.array(list(freq.values()), dtype=float)\n    scale = max(1.0, float(np.median(fvals)))\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        sc = 0.0\n        for e in ss:\n            if e in rem:\n                sc += float(np.exp(-freq.get(e, 1) / scale))\n        if sc > best_score and sc > 0:\n            best_score, best = sc, s\n    return best\n",
  "element_cover_count_penalty": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prefer sets that avoid repeatedly covering elements already covered many times by selected sets.\"\"\"\n    rem = set(remaining_elements)\n\n    covered_times: Dict[int, int] = {}\n    for s in selected_subsets:\n        for e in set(s):\n            covered_times[e] = covered_times.get(e, 0) + 1\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            continue\n        penalty = sum(covered_times.get(e, 0) for e in ss)  # discourage redundant hits\n        score = len(new) - 0.25 * penalty\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "replacement_potential": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Favor candidates that might make some selected subsets redundant (while still covering new elements).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    sel_sets = [set(s) for s in selected_subsets]\n    union_sel: Set[int] = set().union(*sel_sets) if sel_sets else set()\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        cand = set(s)\n        gain = len(rem.intersection(cand))\n        if gain <= 0:\n            continue\n\n        # After adding cand, check how many selected sets are redundant w.r.t (union_sel \u222a cand)\n        union_new = union_sel.union(cand)\n        redundant = 0\n        for t in sel_sets:\n            if len(t) > 0 and (t.issubset(union_new)):\n                redundant += 1\n\n        score = gain + 0.8 * redundant - 0.05 * len(cand)\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "minimize_new_support_variance": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Max gain, but prefer sets whose newly covered elements have low support variance (more predictable impact).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        ss = set(s)\n        for e in ss:\n            if e in support:\n                support[e] += 1\n\n    best, best_tuple = None, None  # (gain, -var_support, -size)\n    for s in remaining_subsets:\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            continue\n        gain = len(new)\n        vals = np.array([support.get(e, 0) for e in new], dtype=float)\n        var = float(vals.var()) if vals.size else 0.0\n        t = (gain, -var, -len(ss))\n        if best_tuple is None or t > best_tuple:\n            best_tuple, best = t, s\n    return best\n",
  "minimax_remaining_support_drop": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Choose set that maximizes gain while minimizing the worst support drop among remaining elements.\"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    # support per element in remaining family\n    support0: Dict[int, int] = {e: 0 for e in rem0}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support0:\n                support0[e] += 1\n\n    best, best_tuple = None, None  # (gain, -worst_drop, -size)\n    for s in remaining_subsets:\n        ss = set(s)\n        gain = len(rem0.intersection(ss))\n        if gain <= 0:\n            continue\n\n        rem1 = rem0 - ss\n        if not rem1:\n            return s\n\n        # drop for an element = how many sets containing it are removed? (proxy: 1 if element is in ss else 0)\n        # worst drop among remaining elements = max(0) here; refine by counting how many remaining sets contain it & are removed\n        worst_drop = 0\n        for e in rem1:\n            worst_drop = max(worst_drop, 0)\n\n        t = (gain, -worst_drop, -len(ss))\n        if best_tuple is None or t > best_tuple:\n            best_tuple, best = t, s\n    return best\n",
  "gain_under_adaptive_size_cap": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Only consider sets below a progress-adaptive size cap; within cap, maximize uncovered gain.\"\"\"\n    rem = set(remaining_elements)\n    if not rem or not remaining_subsets:\n        return None\n\n    sizes = np.array([len(s) for s in remaining_subsets], dtype=float)\n    n_rem = max(1, len(rem))\n    n_total = n_rem + len({e for ss in selected_subsets for e in ss})\n    progress = 1.0 - (n_rem / max(1, n_total))\n\n    # cap tightens late: from 80th percentile early to 40th percentile late\n    q = 0.80 - 0.40 * progress\n    cap = float(np.quantile(sizes, q))\n\n    best, best_gain = None, 0\n    fallback_best, fallback_gain = None, 0\n\n    for s in remaining_subsets:\n        g = len(rem.intersection(s))\n        if g <= 0:\n            continue\n        if len(s) <= cap and g > best_gain:\n            best_gain, best = g, s\n        if g > fallback_gain:\n            fallback_gain, fallback_best = g, s\n\n    return best if best is not None else fallback_best\n",
  "gain_per_new_element_support_sum": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Score = gain / (1 + sum_support_of_new_elements) to avoid consuming high-support items too early.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    best, best_score = None, -1.0\n    for s in remaining_subsets:\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            continue\n        gain = len(new)\n        denom = 1.0 + float(sum(support.get(e, 0) for e in new))\n        score = gain / denom\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "maximize_new_element_support_sum": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Score = sum support(e) over newly covered elements (targets high-degree core first).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            continue\n        score = float(sum(support.get(e, 0) for e in new))\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "minhash_dissimilarity_bonus": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef _minhash_signature(s: Set[int], seeds: np.ndarray) -> np.ndarray:\n    if not s:\n        return np.full(seeds.shape[0], 2**31 - 1, dtype=np.int64)\n    arr = np.fromiter(s, dtype=np.int64)\n    # (a*x + b) mod large prime, take min per seed\n    p = np.int64(2147483647)\n    a = seeds[:, 0]\n    b = seeds[:, 1]\n    vals = (a[:, None] * arr[None, :] + b[:, None]) % p\n    return vals.min(axis=1)\n\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Gain plus a MinHash-based dissimilarity bonus vs. selected subsets (cheap diversity).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    rng = np.random.RandomState(7)\n    seeds = rng.randint(1, 2**31 - 1, size=(16, 2), dtype=np.int64)\n\n    selected_sets = [set(s) for s in selected_subsets] if selected_subsets else []\n    sel_sigs = [_minhash_signature(t, seeds) for t in selected_sets]\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        new = rem.intersection(ss)\n        if not new:\n            continue\n        gain = len(new)\n\n        if not sel_sigs:\n            dissim = 1.0\n        else:\n            sig = _minhash_signature(ss, seeds)\n            # approximate Jaccard similarity by signature match rate\n            sims = [float((sig == tsig).mean()) for tsig in sel_sigs]\n            dissim = 1.0 - float(np.mean(sims))\n\n        score = gain * (1.0 + 0.6 * dissim) - 0.03 * len(ss)\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "hypergraph_core_peel": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Peel heuristic: focus on elements with highest remaining-set support (core), then pick best set covering many core elements.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    if not support:\n        return None\n\n    vals = np.array(list(support.values()), dtype=float)\n    thr = float(np.quantile(vals, 0.75))\n    core = {e for e, c in support.items() if c >= thr and c > 0}\n\n    best, best_tuple = None, None  # (core_cov, total_gain, -size)\n    for s in remaining_subsets:\n        ss = set(s)\n        new = rem.intersection(ss)\n        if not new:\n            continue\n        cc = len(core.intersection(ss))\n        t = (cc, len(new), -len(ss))\n        if best_tuple is None or t > best_tuple:\n            best_tuple, best = t, s\n    return best\n",
  "rare_fringe_peel": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Peel heuristic: define a 'fringe' of low-support elements (<=25th pct) and prioritize covering it.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    vals = np.array([c for c in support.values() if c > 0], dtype=float)\n    if vals.size == 0:\n        return None\n    thr = float(np.quantile(vals, 0.25))\n    fringe = {e for e, c in support.items() if 0 < c <= thr}\n\n    best, best_tuple = None, None  # (fringe_cov, total_gain, -size)\n    for s in remaining_subsets:\n        ss = set(s)\n        new = rem.intersection(ss)\n        if not new:\n            continue\n        fc = len(fringe.intersection(ss))\n        t = (fc, len(new), -len(ss))\n        if best_tuple is None or t > best_tuple:\n            best_tuple, best = t, s\n    return best\n",
  "support_geometric_mean_bonus": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Gain plus bonus for higher geometric mean of supports among newly covered elements.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            continue\n        gain = len(new)\n        vals = np.array([max(1, support.get(e, 1)) for e in new], dtype=float)\n        gmean = float(np.exp(np.mean(np.log(vals)))) if vals.size else 1.0\n        score = gain + 0.12 * gmean - 0.02 * len(ss)\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "support_geometric_mean_penalty": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Gain minus penalty for higher geometric mean support among newly covered elements (preserve flexible items).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            continue\n        gain = len(new)\n        vals = np.array([max(1, support.get(e, 1)) for e in new], dtype=float)\n        gmean = float(np.exp(np.mean(np.log(vals)))) if vals.size else 1.0\n        score = gain - 0.10 * gmean - 0.01 * len(ss)\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "adaptive_lambda_overlap_by_density": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Adjust overlap penalty based on remaining-family density (avg element support).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    covered_selected: Set[int] = set()\n    for s in selected_subsets:\n        covered_selected.update(s)\n\n    support: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    avg_sup = float(np.mean([c for c in support.values()])) if support else 0.0\n    lam = 0.15 + 0.08 * avg_sup  # denser => penalize overlap more\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        gain = len(rem.intersection(ss))\n        if gain <= 0:\n            continue\n        ov = len(ss.intersection(covered_selected))\n        score = gain - lam * ov - 0.01 * len(ss)\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "cover_median_support_elements": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prefer sets that cover many 'middle-support' elements (avoids extremes).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                support[e] = support.get(e, 0) + 1\n\n    if not support:\n        return None\n\n    vals = np.array(list(support.values()), dtype=float)\n    lo = float(np.quantile(vals, 0.35))\n    hi = float(np.quantile(vals, 0.65))\n    mid = {e for e, c in support.items() if lo <= c <= hi}\n\n    best, best_tuple = None, None  # (mid_cov, total_gain, -size)\n    for s in remaining_subsets:\n        ss = set(s)\n        new = rem.intersection(ss)\n        if not new:\n            continue\n        mc = len(mid.intersection(ss))\n        t = (mc, len(new), -len(ss))\n        if best_tuple is None or t > best_tuple:\n            best_tuple, best = t, s\n    return best\n",
  "quantile_rarity_bucket_vector": "import numpy as np\nfrom typing import List, Optional, Dict, Tuple\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Bucket uncovered elements by support quantiles; lexicographically maximize coverage from rarest bucket to commonest.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                support[e] = support.get(e, 0) + 1\n\n    vals = np.array([c for c in support.values() if c > 0], dtype=float)\n    if vals.size == 0:\n        return None\n\n    q1 = float(np.quantile(vals, 0.25))\n    q2 = float(np.quantile(vals, 0.50))\n    q3 = float(np.quantile(vals, 0.75))\n\n    def bucket(c: int) -> int:\n        if c <= q1:\n            return 0\n        if c <= q2:\n            return 1\n        if c <= q3:\n            return 2\n        return 3\n\n    best, best_vec = None, None\n    for s in remaining_subsets:\n        ss = set(s)\n        cov = [e for e in ss if e in rem]\n        if not cov:\n            continue\n        counts = [0, 0, 0, 0]\n        for e in cov:\n            counts[bucket(support.get(e, 0))] += 1\n        vec = (counts[0], counts[1], counts[2], counts[3], -len(ss))\n        if best_vec is None or vec > best_vec:\n            best_vec, best = vec, s\n    return best\n",
  "stochastic_two_arm_bandit_gain_vs_rarity": "import numpy as np\nimport random\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Randomly choose between (max gain) and (max rarity-weighted) using a progress-based mixing probability.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    n_rem = max(1, len(rem))\n    n_total = n_rem + len({e for ss in selected_subsets for e in ss})\n    progress = 1.0 - (n_rem / max(1, n_total))\n\n    p_rarity = 0.20 + 0.60 * progress\n\n    if random.random() > p_rarity:\n        # max gain\n        best, best_gain = None, 0\n        for s in remaining_subsets:\n            g = len(rem.intersection(s))\n            if g > best_gain:\n                best_gain, best = g, s\n        return best if best_gain > 0 else None\n\n    # rarity-weighted\n    freq: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        sc = 0.0\n        for e in set(s):\n            if e in rem:\n                sc += 1.0 / max(1, freq.get(e, 1))\n        if sc > best_score and sc > 0:\n            best_score, best = sc, s\n    return best\n",
  "topk_then_best_rarity": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Filter to top-k by gain, then pick the one with the best rarity-weighted score.\"\"\"\n    rem = set(remaining_elements)\n\n    scored_gain = []\n    for s in remaining_subsets:\n        g = len(rem.intersection(s))\n        if g > 0:\n            scored_gain.append((g, s))\n    if not scored_gain:\n        return None\n\n    scored_gain.sort(key=lambda x: x[0], reverse=True)\n    k = max(1, int(np.ceil(0.15 * len(scored_gain))))\n    top = [s for _, s in scored_gain[:k]]\n\n    freq: Dict[int, int] = {}\n    for s in top:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    best, best_score = None, -1e18\n    for s in top:\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            continue\n        score = sum(1.0 / max(1, freq.get(e, 1)) for e in new)\n        score += 0.05 * len(new)\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "topk_then_best_diversity": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Filter to top-k by gain, then maximize dissimilarity to selected (avg Jaccard distance).\"\"\"\n    rem = set(remaining_elements)\n\n    scored_gain = []\n    for s in remaining_subsets:\n        g = len(rem.intersection(s))\n        if g > 0:\n            scored_gain.append((g, s))\n    if not scored_gain:\n        return None\n\n    scored_gain.sort(key=lambda x: x[0], reverse=True)\n    k = max(1, int(np.ceil(0.20 * len(scored_gain))))\n    top = [s for _, s in scored_gain[:k]]\n\n    sel = [set(s) for s in selected_subsets] if selected_subsets else []\n\n    best, best_score = None, -1e18\n    for s in top:\n        ss = set(s)\n        gain = len(rem.intersection(ss))\n        if gain <= 0:\n            continue\n        if not sel:\n            dist = 1.0\n        else:\n            dsum = 0.0\n            for t in sel:\n                inter = len(ss.intersection(t))\n                uni = len(ss.union(t))\n                dsum += 1.0 - (inter / max(1, uni))\n            dist = dsum / len(sel)\n        score = gain + 0.9 * dist - 0.02 * len(ss)\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "cover_last_uncovered_cluster": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Late-stage bias: if few elements remain, choose set that maximizes gain but minimizes extra (covers mostly what's left).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # if near the end, minimize waste\n    if len(rem) <= 10:\n        best, best_tuple = None, None  # (gain, -waste)\n        for s in remaining_subsets:\n            ss = set(s)\n            gain = len(rem.intersection(ss))\n            if gain <= 0:\n                continue\n            waste = len(ss - rem)\n            t = (gain, -waste)\n            if best_tuple is None or t > best_tuple:\n                best_tuple, best = t, s\n        return best\n\n    # otherwise normal max gain\n    best, best_gain = None, 0\n    for s in remaining_subsets:\n        g = len(rem.intersection(s))\n        if g > best_gain:\n            best_gain, best = g, s\n    return best if best_gain > 0 else None\n",
  "min_waste_per_gain": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Minimize (waste / gain) where waste = elements in set that are not remaining.\"\"\"\n    rem = set(remaining_elements)\n    best, best_score = None, 1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        gain = len(rem.intersection(ss))\n        if gain <= 0:\n            continue\n        waste = len(ss - rem)\n        score = waste / float(gain)\n        if score < best_score:\n            best_score, best = score, s\n    return best\n",
  "gain_minus_waste": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Score = gain - alpha*waste (waste = non-remaining elements inside the set).\"\"\"\n    rem = set(remaining_elements)\n    alpha = 0.35\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        gain = len(rem.intersection(ss))\n        if gain <= 0:\n            continue\n        waste = len(ss - rem)\n        score = gain - alpha * waste\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "coverage_balance_to_target": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prefer sets whose uncovered gain is close to a target fraction of remaining elements (stabilizes step sizes).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    target = max(1, int(np.ceil(0.12 * len(rem))))\n\n    best, best_tuple = None, None  # (-|gain-target|, gain)\n    for s in remaining_subsets:\n        gain = len(rem.intersection(s))\n        if gain <= 0:\n            continue\n        t = (-abs(gain - target), gain)\n        if best_tuple is None or t > best_tuple:\n            best_tuple, best = t, s\n    return best\n",
  "multiplicative_weights_element_importance": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Assign element weights via a multiplicative-weights style rule from support; pick set with max weighted gain.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    # weights: larger when support is small; smooth via exp\n    vals = np.array([c for c in support.values() if c > 0], dtype=float)\n    if vals.size == 0:\n        return None\n    scale = max(1.0, float(np.mean(vals)))\n    w: Dict[int, float] = {}\n    for e, c in support.items():\n        if c > 0:\n            w[e] = float(np.exp(-c / scale))\n        else:\n            w[e] = 0.0\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        sc = 0.0\n        for e in ss:\n            if e in rem:\n                sc += w.get(e, 0.0)\n        if sc > best_score and sc > 0:\n            best_score, best = sc, s\n    return best\n",
  "greedy_dual_fitting_proxy": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Dual-fitting inspired proxy: weight elements inversely by (1+support) and select max weighted gain / size.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            continue\n        weighted = float(sum(1.0 / (1.0 + support.get(e, 0)) for e in new))\n        score = weighted / max(1.0, float(len(ss)))\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "random_projection_signature_diversity": "import numpy as np\nfrom typing import List, Optional\n\ndef _sig(vec: np.ndarray) -> np.ndarray:\n    return (vec > 0).astype(np.int8)\n\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Use random sign projections to build cheap signatures and favor candidates far from selected signatures.\"\"\"\n    rem_list = list(set(remaining_elements))\n    rem = set(rem_list)\n    if not rem:\n        return None\n\n    rng = np.random.RandomState(11)\n    d = len(rem_list)\n    k = min(24, d)\n    P = rng.randn(k, d)\n\n    # build selected signatures\n    sel_sigs = []\n    for s in selected_subsets:\n        v = np.zeros(d, dtype=float)\n        ss = set(s)\n        for i, e in enumerate(rem_list):\n            if e in ss:\n                v[i] = 1.0\n        sel_sigs.append(_sig(P @ v))\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        gain = len(rem.intersection(ss))\n        if gain <= 0:\n            continue\n\n        v = np.zeros(d, dtype=float)\n        for i, e in enumerate(rem_list):\n            if e in ss:\n                v[i] = 1.0\n        sig = _sig(P @ v)\n\n        if not sel_sigs:\n            dist = 1.0\n        else:\n            # Hamming distance normalized\n            dists = [float(np.mean(sig != t)) for t in sel_sigs]\n            dist = float(np.mean(dists))\n\n        score = gain * (1.0 + 0.5 * dist) - 0.02 * len(ss)\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "kcenter_on_elements_then_pick": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Pick an uncovered pivot element with median support, then choose the set containing it with best gain/size.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    vals = sorted([(c, e) for e, c in support.items() if c > 0])\n    if not vals:\n        return None\n    pivot = vals[len(vals) // 2][1]\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        if pivot not in ss:\n            continue\n        gain = len(rem.intersection(ss))\n        if gain <= 0:\n            continue\n        score = gain / max(1.0, float(len(ss)))\n        if score > best_score:\n            best_score, best = score, s\n\n    if best is not None:\n        return best\n\n    # fallback to max gain\n    best, best_gain = None, 0\n    for s in remaining_subsets:\n        g = len(rem.intersection(s))\n        if g > best_gain:\n            best_gain, best = g, s\n    return best if best_gain > 0 else None\n",
  "min_remaining_support_ones_after_pick": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prefer sets that reduce the number of uncovered elements with support==1 (single-option elements).\"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    support0: Dict[int, int] = {e: 0 for e in rem0}\n    sets = [set(s) for s in remaining_subsets]\n    for ss in sets:\n        for e in ss:\n            if e in support0:\n                support0[e] += 1\n\n    best, best_score = None, -1e18\n    for s, ss in zip(remaining_subsets, sets):\n        gain = len(rem0.intersection(ss))\n        if gain <= 0:\n            continue\n        rem1 = rem0 - ss\n        ones = sum(1 for e in rem1 if support0.get(e, 0) == 1)\n        score = gain - 1.1 * ones - 0.01 * len(ss)\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "maximize_coverage_of_support_one": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Maximize the number of uncovered elements with support==1 covered by the chosen set.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        ss = set(s)\n        for e in ss:\n            if e in support:\n                support[e] += 1\n\n    must = {e for e, c in support.items() if c == 1}\n\n    best, best_tuple = None, None  # (must_covered, total_gain, -size)\n    for s in remaining_subsets:\n        ss = set(s)\n        new = rem.intersection(ss)\n        if not new:\n            continue\n        mc = len(must.intersection(ss))\n        t = (mc, len(new), -len(ss))\n        if best_tuple is None or t > best_tuple:\n            best_tuple, best = t, s\n    return best\n",
  "entropy_of_new_supports": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef _entropy_from_counts(x: np.ndarray) -> float:\n    s = float(x.sum())\n    if s <= 0:\n        return 0.0\n    p = x / s\n    p = p[p > 0]\n    return float(-(p * np.log(p)).sum())\n\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Gain plus entropy of supports of newly covered elements (prefers covering a mix of difficulty levels).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                support[e] = support.get(e, 0) + 1\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            continue\n        gain = len(new)\n        vals = np.array([support.get(e, 0) for e in new], dtype=float)\n        ent = _entropy_from_counts(vals)\n        score = gain + 0.35 * ent - 0.02 * len(ss)\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "maximize_jaccard_to_remaining_elements": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Pick the set with maximum Jaccard similarity to the remaining element set (i.e., 'fits' what's left).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    best, best_score = None, -1.0\n    for s in remaining_subsets:\n        ss = set(s)\n        inter = len(ss.intersection(rem))\n        if inter <= 0:\n            continue\n        uni = len(ss.union(rem))\n        score = inter / max(1.0, float(uni))\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "maximize_f1_to_remaining": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Maximize F1 between candidate set and remaining elements (balances precision vs recall of remaining).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        tp = len(ss.intersection(rem))\n        if tp <= 0:\n            continue\n        prec = tp / max(1.0, float(len(ss)))\n        rec = tp / max(1.0, float(len(rem)))\n        f1 = (2.0 * prec * rec) / max(1e-12, (prec + rec))\n        score = f1 + 0.02 * tp\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "gain_times_precision": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Score = gain * precision, where precision = gain/|S| (prefers high-gain, low-waste sets).\"\"\"\n    rem = set(remaining_elements)\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        gain = len(ss.intersection(rem))\n        if gain <= 0:\n            continue\n        precision = gain / max(1.0, float(len(ss)))\n        score = gain * precision\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "roulette_on_gain_and_precision": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Sample with probability proportional to (gain * precision)^2 (stochastic but focused).\"\"\"\n    rem = set(remaining_elements)\n    cand = []\n    w = []\n    for s in remaining_subsets:\n        ss = set(s)\n        gain = len(ss.intersection(rem))\n        if gain <= 0:\n            continue\n        precision = gain / max(1.0, float(len(ss)))\n        score = (gain * precision) ** 2\n        cand.append(s)\n        w.append(score)\n    if not cand:\n        return None\n    w = np.array(w, dtype=float)\n    w = w / w.sum()\n    idx = int(np.random.choice(len(cand), p=w))\n    return cand[idx]\n",
  "pick_set_covering_max_support_span": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prefer sets whose newly covered elements span a wide support range (max - min).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                support[e] = support.get(e, 0) + 1\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            continue\n        vals = [support.get(e, 0) for e in new]\n        span = max(vals) - min(vals) if vals else 0\n        score = len(new) + 0.25 * float(span) - 0.02 * len(ss)\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "balanced_cover_by_support_bins": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prefer sets that cover at least one element from each support bin (rare/mid/common), then maximize gain.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                support[e] = support.get(e, 0) + 1\n\n    vals = np.array([c for c in support.values() if c > 0], dtype=float)\n    if vals.size == 0:\n        return None\n\n    q1 = float(np.quantile(vals, 0.33))\n    q2 = float(np.quantile(vals, 0.66))\n\n    def bin_of(c: int) -> int:\n        if c <= q1:\n            return 0\n        if c <= q2:\n            return 1\n        return 2\n\n    best, best_tuple = None, None  # (bins_hit, gain, -size)\n    for s in remaining_subsets:\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            continue\n        bins = {bin_of(support.get(e, 0)) for e in new}\n        t = (len(bins), len(new), -len(ss))\n        if best_tuple is None or t > best_tuple:\n            best_tuple, best = t, s\n    return best\n",
  "two_stage_pivot_then_diversify": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Pick rarest pivot element; among sets containing it, choose one most dissimilar to selected (Jaccard distance).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    freq: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in freq:\n                freq[e] += 1\n\n    piv = None\n    piv_f = 10**18\n    for e, f in freq.items():\n        if 0 < f < piv_f:\n            piv_f = f\n            piv = e\n    if piv is None:\n        return None\n\n    sel_sets = [set(s) for s in selected_subsets] if selected_subsets else []\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        if piv not in ss:\n            continue\n        gain = len(rem.intersection(ss))\n        if gain <= 0:\n            continue\n        if not sel_sets:\n            dist = 1.0\n        else:\n            dsum = 0.0\n            for t in sel_sets:\n                inter = len(ss.intersection(t))\n                uni = len(ss.union(t))\n                dsum += 1.0 - (inter / max(1, uni))\n            dist = dsum / len(sel_sets)\n        score = gain + 0.8 * dist - 0.02 * len(ss)\n        if score > best_score:\n            best_score, best = score, s\n\n    if best is not None:\n        return best\n\n    # fallback: best set by gain\n    best, best_gain = None, 0\n    for s in remaining_subsets:\n        g = len(rem.intersection(s))\n        if g > best_gain:\n            best_gain, best = g, s\n    return best if best_gain > 0 else None\n",
  "singleton_completion_bias": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"If any uncovered element appears in exactly one remaining set, pick the set that satisfies the most such singletons; else fallback to max gain.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # element -> count of sets containing it\n    freq: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        ss = set(s)\n        for e in ss:\n            if e in freq:\n                freq[e] += 1\n\n    singletons = {e for e, c in freq.items() if c == 1}\n\n    best, best_tuple = None, None  # (singletons_covered, gain, -size)\n    for s in remaining_subsets:\n        ss = set(s)\n        gain = len(rem.intersection(ss))\n        if gain <= 0:\n            continue\n        sc = len(singletons.intersection(ss))\n        t = (sc, gain, -len(ss))\n        if best_tuple is None or t > best_tuple:\n            best_tuple, best = t, s\n\n    # If no singleton is coverable (rare but possible if freq map empty), fallback to max gain\n    if best is None:\n        best_gain, best = 0, None\n        for s in remaining_subsets:\n            g = len(rem.intersection(s))\n            if g > best_gain:\n                best_gain, best = g, s\n        return best if best_gain > 0 else None\n\n    return best\n",
  "anti_deadend_guard": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prefer sets that reduce the count of uncovered elements with low remaining support (<=2), guarding against dead-ends.\"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    # support among remaining subsets\n    support0: Dict[int, int] = {e: 0 for e in rem0}\n    sets = [set(s) for s in remaining_subsets]\n    for ss in sets:\n        for e in ss:\n            if e in support0:\n                support0[e] += 1\n\n    low = {e for e, c in support0.items() if 0 < c <= 2}\n\n    best, best_score = None, -1e18\n    for s, ss in zip(remaining_subsets, sets):\n        gain = len(rem0.intersection(ss))\n        if gain <= 0:\n            continue\n        rem1 = rem0 - ss\n        # count low-support elements left (computed with support0 proxy)\n        low_left = sum(1 for e in rem1 if e in low)\n        score = gain - 1.0 * low_left - 0.01 * len(ss)\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "weighted_waste_f1_hybrid": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Hybrid: maximize F1 with remaining elements, then subtract weighted waste (elements outside remaining).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    alpha = 0.25\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        ss = set(s)\n        tp = len(ss.intersection(rem))\n        if tp <= 0:\n            continue\n        prec = tp / max(1.0, float(len(ss)))\n        rec = tp / max(1.0, float(len(rem)))\n        f1 = (2.0 * prec * rec) / max(1e-12, (prec + rec))\n        waste = len(ss - rem)\n        score = f1 + 0.02 * tp - alpha * (waste / max(1.0, float(len(ss))))\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "rare_then_pack_dense": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Two-mode: if any uncovered element has support==1 or 2, prioritize covering those; otherwise choose best gain/size (dense pack).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    freq: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in freq:\n                freq[e] += 1\n\n    critical = {e for e, c in freq.items() if 0 < c <= 2}\n\n    best, best_score = None, -1e18\n    if critical:\n        for s in remaining_subsets:\n            ss = set(s)\n            gain = len(rem.intersection(ss))\n            if gain <= 0:\n                continue\n            crit_cov = len(critical.intersection(ss))\n            score = 2.5 * crit_cov + 0.1 * gain - 0.02 * len(ss)\n            if score > best_score:\n                best_score, best = score, s\n        return best\n\n    # dense pack fallback: maximize gain/size\n    for s in remaining_subsets:\n        ss = set(s)\n        gain = len(rem.intersection(ss))\n        if gain <= 0:\n            continue\n        score = gain / max(1.0, float(len(ss)))\n        if score > best_score:\n            best_score, best = score, s\n    return best\n",
  "regret_minimization_proxy": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]],\n                       remaining_subsets: List[List[int]],\n                       remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Choose set minimizing regret: regret = (best_possible_gain_now - gain(set)) + beta*waste.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    gains = []\n    cand = []\n    for s in remaining_subsets:\n        ss = set(s)\n        g = len(ss.intersection(rem))\n        if g > 0:\n            gains.append(g)\n            cand.append(s)\n    if not cand:\n        return None\n\n    best_gain = max(gains)\n    beta = 0.15\n\n    best, best_regret = None, 1e18\n    for s, g in zip(cand, gains):\n        ss = set(s)\n        waste = len(ss - rem)\n        regret = (best_gain - g) + beta * waste\n        if regret < best_regret:\n            best_regret, best = regret, s\n    return best\n",
  "max_cover_greedy_aug_0": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selection with deterministic noise to break ties.\n    \"\"\"\n    uncovered_set = set(remaining_elements)\n\n    # Compute how many uncovered elements each subset would cover\n    gains = np.array([len(uncovered_set.intersection(s)) for s in remaining_subsets],\n                     dtype=float)\n\n    # Deterministic noise: 1e-6 * index\n    noise = np.arange(len(gains)) * 1e-6\n    scores = gains + noise\n\n    best_idx = int(np.argmax(scores))\n    best_gain = gains[best_idx]\n\n    return remaining_subsets[best_idx] if best_gain > 0 else None\n\n",
  "max_cover_greedy_aug_1": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Weighted scoring: prioritize new coverage over subset size.\n    \"\"\"\n    uncovered_set = set(remaining_elements)\n\n    # Basic gain: number of new elements covered\n    gains = np.array([len(uncovered_set.intersection(s)) for s in remaining_subsets],\n                     dtype=float)\n\n    # Weight parameters\n    weight_new = 0.6\n    weight_size = 0.4\n\n    # Score = weighted gain minus penalty for subset size\n    subset_sizes = np.array([len(s) for s in remaining_subsets], dtype=float)\n    scores = weight_new * gains - weight_size * subset_sizes\n\n    # Ensure scores are non\u2011negative\n    scores = np.clip(scores, 0, None)\n\n    best_idx = int(np.argmax(scores))\n    best_score = scores[best_idx]\n\n    return remaining_subsets[best_idx] if best_score > 0 else None\n\n",
  "max_cover_greedy_aug_2": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Probabilistic selection using softmax over coverage gains.\n    \"\"\"\n    uncovered_set = set(remaining_elements)\n\n    # Gains for each subset\n    gains = np.array([len(uncovered_set.intersection(s)) for s in remaining_subsets],\n                     dtype=float)\n\n    # Softmax with temperature (1.0)\n    max_gain = np.max(gains)\n    exp_vals = np.exp(gains - max_gain)          # shift for numerical stability\n    denom = np.sum(exp_vals) + 1e-12              # epsilon to avoid div\u2011by\u2011zero\n    probs = exp_vals / denom\n    probs = np.clip(probs, 0, 1)                  # safety clip\n\n    chosen_idx = int(np.random.choice(len(remaining_subsets), p=probs))\n\n    return remaining_subsets[chosen_idx] if gains[chosen_idx] > 0 else None\n\n",
  "max_cover_greedy_aug_3": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Random choice among the top\u2011k best subsets.\n    \"\"\"\n    uncovered_set = set(remaining_elements)\n\n    # Compute gains\n    gains = np.array([len(uncovered_set.intersection(s)) for s in remaining_subsets],\n                     dtype=float)\n\n    # Determine top\u2011k (k = 5 or all if fewer)\n    top_k = min(5, len(remaining_subsets))\n    top_indices = np.argpartition(-gains, top_k - 1)[:top_k]\n\n    if len(top_indices) == 0:\n        return None\n\n    chosen_idx = int(np.random.choice(top_indices))\n    return remaining_subsets[chosen_idx] if gains[chosen_idx] > 0 else None\n\n",
  "max_cover_greedy_aug_4": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Thresholded selection: pick the best subset among those with\n    coverage >= median coverage.\n    \"\"\"\n    uncovered_set = set(remaining_elements)\n\n    # Gains\n    gains = np.array([len(uncovered_set.intersection(s)) for s in remaining_subsets],\n                     dtype=float)\n\n    # Median threshold (add tiny epsilon to avoid zero threshold)\n    median_gain = np.median(gains) + 1e-12\n\n    # Candidate indices meeting or exceeding the median\n    candidate_mask = gains >= median_gain\n    candidate_indices = np.where(candidate_mask)[0]\n\n    if candidate_indices.size == 0:\n        return None\n\n    # Pick the candidate with the highest gain\n    best_idx = int(candidate_indices[np.argmax(gains[candidate_indices])])\n\n    return remaining_subsets[best_idx] if gains[best_idx] > 0 else None\n\n",
  "max_cover_min_size_tiebreak_aug_5": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorised set\u2011cover heuristic with deterministic tie\u2011breaking noise.\"\"\"\n    rem_set = set(remaining_elements)\n    best_subset = None\n    best_gain = -1\n    best_size  = 10**18\n\n    idx = 0\n    while idx < len(remaining_subsets):\n        subset = remaining_subsets[idx]\n        # deterministic noise: small value derived from the subset content\n        noise = (sum(subset) % 1000) * 1e-6\n        gain  = len(rem_set.intersection(subset)) + noise\n\n        # use np.clip to keep size comparisons safe\n        subset_size = len(subset)\n        if (gain > best_gain) or (np.isclose(gain, best_gain) and subset_size < best_size):\n            best_gain   = gain\n            best_size   = subset_size\n            best_subset = subset\n\n        idx += 1\n\n    return best_subset if best_gain > 0 else None\n\n",
  "max_cover_min_size_tiebreak_aug_6": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Soft\u2011max scoring based on coverage, with epsilon\u2011regularised denominator.\"\"\"\n    rem_arr = np.array(remaining_elements, dtype=int)\n\n    # Compute coverage scores for all remaining subsets\n    gains = []\n    for subset in remaining_subsets:\n        sub_arr = np.array(subset, dtype=int)\n        # Boolean mask of elements in subset that are still uncovered\n        mask = np.isin(sub_arr, rem_arr)\n        gains.append(np.sum(mask))\n\n    gains = np.array(gains, dtype=float)\n\n    # Avoid division by zero in softmax\n    denom = np.sum(np.exp(gains)) + 1e-12\n    probs = np.exp(gains) / denom\n\n    # Pick the subset with the highest probability\n    best_idx = np.argmax(probs)\n    best_gain = gains[best_idx]\n    return remaining_subsets[best_idx] if best_gain > 0 else None\n\n",
  "max_cover_min_size_tiebreak_aug_7": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Top\u2011k selection with deterministic random tie\u2011breaking.\"\"\"\n    top_k = 7  # hyper\u2011parameter\n    rem_set = set(remaining_elements)\n\n    # Compute gains and keep top_k subsets\n    gains = []\n    for subset in remaining_subsets:\n        gains.append(len(rem_set.intersection(subset)))\n\n    # If less than top_k subsets, adjust\n    k = min(top_k, len(gains))\n    top_indices = np.argpartition(-np.array(gains), k-1)[:k]\n    top_gains   = [gains[i] for i in top_indices]\n\n    # Randomly pick among the top_k using a deterministic RNG seeded by subset data\n    best_subset = None\n    best_gain   = -1\n    for idx in top_indices:\n        subset = remaining_subsets[idx]\n        seed   = sum(subset) + len(subset)\n        rng    = np.random.default_rng(seed)\n        if top_gains[top_indices.tolist().index(idx)] > best_gain:\n            best_subset = subset\n            best_gain   = top_gains[top_indices.tolist().index(idx)]\n        elif np.isclose(top_gains[top_indices.tolist().index(idx)], best_gain):\n            # deterministic random choice among ties\n            if rng.integers(0, 2) == 0:\n                best_subset = subset\n                best_gain   = top_gains[top_indices.tolist().index(idx)]\n\n    return best_subset if best_gain > 0 else None\n\n",
  "max_cover_min_size_tiebreak_aug_8": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Coverage measured as a proportion, with deterministic noise for tie\u2011breaking.\"\"\"\n    rem_arr = np.array(remaining_elements, dtype=int)\n    total_uncovered = len(rem_arr)\n\n    best_subset = None\n    best_score  = -1\n    best_size   = 10**18\n\n    for subset in remaining_subsets:\n        sub_arr = np.array(subset, dtype=int)\n        # Proportion of uncovered elements in the subset\n        prop = np.sum(np.isin(sub_arr, rem_arr)) / (total_uncovered + 1e-12)\n\n        # Deterministic noise derived from subset hash\n        noise = (hash(tuple(subset)) % 1000) * 1e-6\n        score = prop + noise\n\n        # Use np.clip to guard against extremely small sizes\n        subset_size = len(subset)\n        if (score > best_score) or (np.isclose(score, best_score) and subset_size < best_size):\n            best_score  = score\n            best_size   = subset_size\n            best_subset = subset\n\n    return best_subset if best_score > 0 else None\n\n",
  "max_cover_min_size_tiebreak_aug_9": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Inverted conditional logic and mathematical synonyms for diversity.\"\"\"\n    rem_set = set(remaining_elements)\n    best_subset = None\n    best_gain   = -1\n    best_size   = 10**18\n\n    for subset in remaining_subsets:\n        # Use * for multiplication and ** for exponentiation where appropriate\n        gain = len(rem_set.intersection(subset))\n\n        # Invert the comparison: we keep the subset if the new gain is NOT smaller\n        if not (gain < best_gain) and (gain > 0 or (gain == 0 and len(subset) < best_size)):\n            best_gain   = gain\n            best_size   = len(subset)\n            best_subset = subset\n\n    # np.clip is used to ensure best_size never becomes negative\n    best_size = np.clip(best_size, 0, 10**18)\n\n    return best_subset if best_gain > 0 else None\n\n",
  "cost_effectiveness_aug_10": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Choose the subset that maximizes (new_covered / size) with deterministic tie\u2011breaking.\"\"\"\n    rem_arr = np.array(remaining_elements, dtype=int)\n    best_subset = None\n    best_score = -np.inf\n\n    for sub in remaining_subsets:\n        sub_arr = np.array(sub, dtype=int)\n        new_cov = np.sum(np.isin(sub_arr, rem_arr))\n        if new_cov <= 0:\n            continue\n\n        denom = np.clip(len(sub), 1, None) + 1e-12   # epsilon to avoid div\u2011by\u2011zero\n        score = new_cov / denom\n\n        # deterministic noise for tie\u2011breaking\n        noise = 1e-6 * (hash(tuple(sub)) % 1000)\n        score += noise\n\n        if score > best_score:\n            best_score = score\n            best_subset = sub\n\n    return best_subset\n\n",
  "cost_effectiveness_aug_11": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Greedy selection using a softmax weighting over the top\u20113 candidates.\"\"\"\n    rem = set(remaining_elements)\n    scores = []\n    subs = []\n\n    idx = 0\n    n = len(remaining_subsets)\n    while idx < n:\n        s = remaining_subsets[idx]\n        idx += 1\n        new_cov = len(rem.intersection(s))\n        if new_cov > 0:                     # inverted logic\n            denom = max(1, len(s)) + 1e-12\n            scores.append(new_cov / denom)\n            subs.append(s)\n\n    if not scores:\n        return None\n\n    scores = np.array(scores)\n    # softmax weighting\n    exp_scores = np.exp(scores - np.max(scores))\n    probs = exp_scores / np.clip(np.sum(exp_scores), 1e-12, None)\n\n    # pick deterministically among the top\u20113\n    top_k = 3\n    top_idx = np.argsort(-scores)[:min(top_k, len(scores))]\n    chosen_idx = top_idx[np.argmax(probs[top_idx])]\n    return subs[chosen_idx]\n\n",
  "cost_effectiveness_aug_12": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Use a sqrt\u2011scaled denominator and deterministic noise for tie\u2011breaking.\"\"\"\n    rem_set = set(remaining_elements)\n    best = None\n    best_score = -np.inf\n\n    for s in remaining_subsets:\n        new_cov = len(rem_set.intersection(s))\n        if new_cov <= 0:\n            continue\n\n        denom = np.sqrt(len(s)) + 1e-12\n        score = new_cov / denom\n\n        # deterministic noise\n        noise = 1e-7 * (hash(tuple(s)) % 500)\n        score += noise\n\n        if score > best_score:\n            best_score = score\n            best = s\n\n    return best\n\n",
  "cost_effectiveness_aug_13": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Approximate denominator with a weighted linear combination and add noise.\"\"\"\n    rem = set(remaining_elements)\n    best_subset = None\n    best_score = -np.inf\n\n    for sub in remaining_subsets:\n        new_cov = len(rem.intersection(sub))\n        if new_cov <= 0:\n            continue\n\n        denom = np.clip(0.8 * len(sub) + 0.2, 1e-12, None)\n        score = new_cov / denom\n\n        # deterministic noise\n        noise = 5e-7 * (hash(tuple(sub)) % 200)\n        score += noise\n\n        if score > best_score:\n            best_score = score\n            best_subset = sub\n\n    return best_subset\n\n",
  "cost_effectiveness_aug_14": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Randomly choose among the top\u20115 candidates based on the greedy score.\"\"\"\n    rem = set(remaining_elements)\n    candidates = []\n    scores = []\n\n    for s in remaining_subsets:\n        new_cov = len(rem.intersection(s))\n        if new_cov > 0:\n            denom = max(1, len(s)) + 1e-12\n            scores.append(new_cov / denom)\n            candidates.append(s)\n\n    if not scores:\n        return None\n\n    scores = np.array(scores)\n    top_k = 5\n    top_idx = np.argsort(-scores)[:min(top_k, len(scores))]\n    rng = np.random.default_rng(42)  # deterministic seed\n    chosen = rng.choice([candidates[i] for i in top_idx])\n    return chosen\n\n",
  "rarity_weighted_gain_aug_15": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 1: While loop, list comprehensions, inverted logic, epsilon and clipping.\"\"\"\n    rem_set = set(remaining_elements)\n    freq: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem_set:\n                freq[e] = freq.get(e, 0) + 1\n\n    best_subset = None\n    best_score = -1.0\n    idx = 0\n    while idx < len(remaining_subsets):\n        subset = remaining_subsets[idx]\n        # compute score as sum of inverse frequencies\n        score = sum(1.0 / (freq.get(e, 1) + 1e-12) for e in set(subset) if e in rem_set)\n        # clip score to avoid overflow\n        score = np.clip(score, 0, 1e6)\n        # inverted conditional: update if score is strictly greater\n        if score > best_score:\n            best_score = score\n            best_subset = subset\n        idx += 1\n    return best_subset\n\n",
  "rarity_weighted_gain_aug_16": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 2: Parameter tuning with weighted scores, top\u2011k random selection.\"\"\"\n    if not remaining_subsets:\n        return None\n\n    rem_set = set(remaining_elements)\n    freq: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem_set:\n                freq[e] = freq.get(e, 0) + 1\n\n    scores = []\n    for s in remaining_subsets:\n        score = sum(0.6 / (freq.get(e, 1) + 1e-12) for e in set(s) if e in rem_set)\n        scores.append(score)\n\n    scores = np.clip(np.array(scores), 0, 1e6)\n    top_k = 7\n    if len(scores) <= top_k:\n        top_indices = np.argsort(-scores)\n    else:\n        top_indices = np.argpartition(-scores, top_k - 1)[:top_k]\n        top_indices = top_indices[np.argsort(-scores[top_indices])]\n    rng = np.random.default_rng(42)\n    chosen_idx = rng.choice(top_indices)\n    return remaining_subsets[chosen_idx]\n\n",
  "rarity_weighted_gain_aug_17": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 3: Deterministic noise for tie\u2011breaking, max aggregation.\"\"\"\n    rem_set = set(remaining_elements)\n    freq: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem_set:\n                freq[e] = freq.get(e, 0) + 1\n\n    best_subset = None\n    best_score = -np.inf\n    for s in remaining_subsets:\n        elements = [e for e in set(s) if e in rem_set]\n        if not elements:\n            continue\n        inv_freq = [1.0 / (freq.get(e, 1) + 1e-12) for e in elements]\n        score = np.max(inv_freq)\n        # deterministic noise using hash of the subset\n        noise = 1e-9 * (hash(tuple(sorted(s))) & 0xffffffff)\n        score += noise\n        if score > best_score:\n            best_score = score\n            best_subset = s\n    return best_subset\n\n",
  "rarity_weighted_gain_aug_18": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 4: Fully vectorised with numpy, no explicit loops.\"\"\"\n    rem_set = set(remaining_elements)\n    if not remaining_subsets:\n        return None\n\n    # Build element \u2192 index mapping for all elements that appear in remaining subsets\n    all_elements = sorted(set().union(*remaining_subsets))\n    elem_to_idx = {e: i for i, e in enumerate(all_elements)}\n\n    n_sub = len(remaining_subsets)\n    n_elem = len(all_elements)\n    mat = np.zeros((n_sub, n_elem), dtype=np.int8)\n\n    for i, s in enumerate(remaining_subsets):\n        for e in set(s):\n            if e in rem_set:\n                mat[i, elem_to_idx[e]] = 1\n\n    freq = mat.sum(axis=0)  # element frequencies across remaining subsets\n    freq = freq + 1e-12     # avoid division by zero\n    inv_freq = 1.0 / freq\n\n    scores = mat @ inv_freq\n    scores = np.clip(scores, 0, 1e6)\n    best_idx = int(np.argmax(scores))\n    return remaining_subsets[best_idx]\n\n",
  "rarity_weighted_gain_aug_19": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 5: Proxy approximation \u2013 sample\u2011based frequency estimate.\"\"\"\n    rem_set = set(remaining_elements)\n    if not remaining_subsets:\n        return None\n\n    rng = np.random.default_rng(123)\n    sample_size = min(20, len(remaining_subsets))\n    sample_indices = rng.choice(len(remaining_subsets), size=sample_size, replace=False)\n\n    freq_est: Dict[int, int] = {}\n    for idx in sample_indices:\n        for e in set(remaining_subsets[idx]):\n            if e in rem_set:\n                freq_est[e] = freq_est.get(e, 0) + 1\n\n    avg_freq = np.mean(list(freq_est.values())) if freq_est else 1.0\n\n    best_subset = None\n    best_score = -1.0\n    for s in remaining_subsets:\n        new_elements = [e for e in set(s) if e in rem_set]\n        if not new_elements:\n            continue\n        score = len(new_elements) / (avg_freq + 1e-12)\n        score = np.clip(score, 0, 1e6)\n        if score > best_score:\n            best_score = score\n            best_subset = s\n    return best_subset\n\n",
  "rarity_squared_gain_aug_20": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Aggressive rare\u2011element heuristic with deterministic tie\u2011breaking.\"\"\"\n    elems = set(remaining_elements)\n\n    # frequency dictionary\n    freq: Dict[int, int] = {}\n    for sub in remaining_subsets:\n        for el in set(sub):\n            if el in elems:\n                freq[el] = freq.get(el, 0) + 1\n\n    best_sub = None\n    best_score = -1.0\n    for idx, sub in enumerate(remaining_subsets):\n        score = 0.0\n        for el in set(sub):\n            if el in elems:\n                f = freq.get(el, 1)\n                score += 1.0 / (np.square(f) + 1e-12)   # epsilon added\n        score = np.clip(score, 0, None)                 # clip to non\u2011negative\n        score += 1e-6 * idx                             # deterministic tie\u2011breaker\n        if score > best_score:\n            best_score = score\n            best_sub = sub\n    return best_sub\n\n",
  "rarity_squared_gain_aug_21": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Weighted scoring with random choice among top\u2011k subsets.\"\"\"\n    elems = set(remaining_elements)\n\n    freq = {el: 0 for el in elems}\n    for sub in remaining_subsets:\n        for el in set(sub):\n            if el in elems:\n                freq[el] += 1\n\n    scores = []\n    for sub in remaining_subsets:\n        score = 0.0\n        for el in set(sub):\n            if el in elems:\n                f = freq[el]\n                # tuned weights: 0.6/(f\u00b2) + 0.4/(f\u00b3)\n                score += 0.6 / (np.square(f) + 1e-12) + 0.4 / (np.power(f, 3) + 1e-12)\n        scores.append(score)\n\n    scores = np.array(scores)\n    top_k = 3\n    if len(scores) <= top_k:\n        return remaining_subsets[int(np.argmax(scores))]\n\n    top_indices = np.argpartition(-scores, top_k - 1)[:top_k]\n    chosen_idx = np.random.choice(top_indices)\n    return remaining_subsets[chosen_idx]\n\n",
  "rarity_squared_gain_aug_22": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Score based on the maximum element weight instead of a sum.\"\"\"\n    elems = set(remaining_elements)\n\n    freq = {el: 0 for el in elems}\n    for sub in remaining_subsets:\n        for el in set(sub):\n            if el in elems:\n                freq[el] += 1\n\n    best_sub = None\n    best_score = -1.0\n    for idx, sub in enumerate(remaining_subsets):\n        weights = []\n        for el in set(sub):\n            if el in elems:\n                f = freq[el]\n                weights.append(1.0 / (np.square(f) + 1e-12))\n        if not weights:\n            continue\n        score = np.max(weights)                    # use max instead of sum\n        score = np.clip(score, 0, None)             # clip\n        score += 1e-6 * idx                         # tie\u2011breaker\n        if score > best_score:\n            best_score = score\n            best_sub = sub\n    return best_sub\n\n",
  "rarity_squared_gain_aug_23": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Soft\u2011min selection with proxy frequency estimation.\"\"\"\n    elems = set(remaining_elements)\n    # gather all elements that appear in remaining subsets\n    all_elems = [el for sub in remaining_subsets for el in set(sub) if el in elems]\n    if not all_elems:\n        return None\n\n    unique, counts = np.unique(all_elems, return_counts=True)\n    freq_map = dict(zip(unique, counts))\n\n    scores = []\n    for sub in remaining_subsets:\n        score = 0.0\n        for el in set(sub):\n            if el in elems:\n                f = freq_map.get(el, 1)\n                score += 1.0 / (np.square(f) + 1e-12)\n        scores.append(score)\n\n    scores = np.clip(np.array(scores), 0, None)   # clip to avoid negative values\n    beta = 5.0\n    probs = np.exp(-scores * beta)\n    probs /= np.sum(probs)\n    chosen_idx = np.argmax(probs)\n    return remaining_subsets[chosen_idx]\n\n",
  "rarity_squared_gain_aug_24": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorised scoring with random top\u2011k choice and noise.\"\"\"\n    elems = set(remaining_elements)\n\n    freq = {el: 0 for el in elems}\n    for sub in remaining_subsets:\n        for el in set(sub):\n            if el in elems:\n                freq[el] += 1\n\n    scores = []\n    for sub in remaining_subsets:\n        mask = np.array([el in elems for el in sub])\n        elems_in_sub = np.array(sub)[mask]\n        if len(elems_in_sub) == 0:\n            scores.append(0.0)\n            continue\n        f_vals = np.array([freq[el] for el in elems_in_sub])\n        score = np.sum(1.0 / (np.square(f_vals) + 1e-12))\n        scores.append(score)\n\n    scores = np.clip(np.array(scores), 0, None)   # clip\n    # deterministic noise for tie\u2011breaking\n    noise = np.linspace(0, 1e-6, len(scores))\n    scores += noise\n\n    top_k = 4\n    if len(scores) <= top_k:\n        chosen_idx = int(np.argmax(scores))\n    else:\n        top_indices = np.argpartition(-scores, top_k - 1)[:top_k]\n        chosen_idx = np.random.choice(top_indices)\n    return remaining_subsets[chosen_idx]\n\n",
  "overlap_penalized_gain_aug_25": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    sel_subsets = selected_subsets\n    rem_subsets = remaining_subsets\n    rem_elems = remaining_elements\n\n    # Convert remaining elements to a set for fast lookup\n    rem_set = set(rem_elems)\n\n    # Build the set of elements already covered by selected subsets\n    covered = set()\n    for s in sel_subsets:\n        covered.update(s)\n\n    # Overlap penalty factor\n    lam = 0.30\n\n    best = None\n    best_score = -1e18\n\n    for s in rem_subsets:\n        s_set = set(s)\n        new = len(rem_set & s_set)\n        if new <= 0:\n            continue\n        overlap = len(s_set & covered)\n\n        # Deterministic noise based on the subset contents\n        noise = 1e-6 * (sum(s) % 1000)\n\n        score = new - lam * overlap + noise\n        score = np.clip(score, -1e18, 1e18)\n\n        if score > best_score:\n            best_score = score\n            best = s\n\n    return best\n\n",
  "overlap_penalized_gain_aug_26": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    sel_subsets = selected_subsets\n    rem_subsets = remaining_subsets\n    rem_elems = remaining_elements\n\n    rem_set = set(rem_elems)\n    covered = set()\n    for s in sel_subsets:\n        covered.update(s)\n\n    lam = 0.40\n    scores = []\n    idx = 0\n    while idx < len(rem_subsets):\n        s = rem_subsets[idx]\n        s_set = set(s)\n        new = len(rem_set & s_set)\n        if new > 0:\n            overlap = len(s_set & covered)\n            denom = 1 + len(covered) + 1e-12\n            score = (new - lam * overlap) / denom\n            score = np.clip(score, -1e18, 1e18)\n            scores.append((score, idx))\n        idx += 1\n\n    if not scores:\n        return None\n\n    score_arr = np.array([s[0] for s in scores])\n    top_k = 5\n    top_k = min(top_k, len(score_arr))\n    top_indices = np.argsort(score_arr)[-top_k:]\n\n    rng = np.random.default_rng()\n    chosen_idx_in_scores = rng.choice(top_indices)\n    chosen_subset = rem_subsets[scores[chosen_idx_in_scores][1]]\n    return chosen_subset\n\n",
  "overlap_penalized_gain_aug_27": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    sel_subsets = selected_subsets\n    rem_subsets = remaining_subsets\n    rem_elems = remaining_elements\n\n    rem_set = set(rem_elems)\n    covered = set()\n    for s in sel_subsets:\n        covered.update(s)\n\n    lam = 0.20\n    new_counts = []\n    overlap_counts = []\n\n    for s in rem_subsets:\n        s_set = set(s)\n        new_counts.append(len(rem_set & s_set))\n        overlap_counts.append(len(s_set & covered))\n\n    new_arr = np.array(new_counts, dtype=np.float64)\n    overlap_arr = np.array(overlap_counts, dtype=np.float64)\n\n    denom = 1 + overlap_arr + 1e-12\n    score_arr = new_arr / denom - lam * overlap_arr\n    score_arr = np.clip(score_arr, -1e18, 1e18)\n\n    # Deterministic tie\u2011breaking noise\n    noise = 1e-8 * np.arange(len(rem_subsets))\n    score_arr += noise\n\n    best_idx = np.argmax(score_arr)\n    if score_arr[best_idx] <= 0:\n        return None\n    return rem_subsets[best_idx]\n\n",
  "overlap_penalized_gain_aug_28": "import numpy as np\nfrom typing import List, Optional, Set\nfrom functools import reduce\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    sel_subsets = selected_subsets\n    rem_subsets = remaining_subsets\n    rem_elems = remaining_elements\n\n    rem_set = set(rem_elems)\n    covered = set()\n    for s in sel_subsets:\n        covered.update(s)\n\n    lam = 0.35\n\n    def score_of(sub):\n        s_set = set(sub)\n        new = len(rem_set & s_set)\n        if new == 0:\n            return (-1e18, sub)\n        overlap = len(s_set & covered)\n        denom = max(1, len(covered)) + 1e-12\n        score = (new - lam * overlap) / denom\n        score = np.clip(score, -1e18, 1e18)\n        # deterministic noise\n        noise = 1e-9 * (sum(sub) % 1000)\n        return (score + noise, sub)\n\n    scored = list(map(score_of, rem_subsets))\n    best_score, best_subset = reduce(lambda a, b: a if a[0] > b[0] else b, scored, (-1e18, None))\n\n    if best_score <= 0:\n        return None\n    return best_subset\n\n",
  "overlap_penalized_gain_aug_29": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    sel_subsets = selected_subsets\n    rem_subsets = remaining_subsets\n    rem_elems = remaining_elements\n\n    rem_set = set(rem_elems)\n    covered = set()\n    for s in sel_subsets:\n        covered.update(s)\n\n    lam = 0.28\n\n    candidates = (\n        (s, len(rem_set & set(s)), len(set(s) & covered))\n        for s in rem_subsets\n    )\n\n    scored = sorted(\n        (\n            (s, new - lam * overlap + 1e-9 * (sum(s) % 1000))\n            for s, new, overlap in candidates\n            if new > 0\n        ),\n        key=lambda x: x[1],\n        reverse=True,\n    )\n\n    if not scored:\n        return None\n\n    best_subset, best_score = scored[0]\n    best_score = np.clip(best_score, -1e18, 1e18)\n\n    if best_score <= 0:\n        return None\n    return best_subset\n\n",
  "diversity_bonus_jaccard_aug_30": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Greedy selection with vectorised Jaccard distance and deterministic tie\u2011break.\"\"\"\n    rem_set = set(remaining_elements)\n    selected_np = [np.array(s) for s in selected_subsets]\n    best_subset = None\n    best_score = -np.inf\n\n    for idx, s in enumerate(remaining_subsets):\n        s_np = np.array(s)\n        new_cov = len(rem_set.intersection(set(s_np)))\n        if new_cov <= 0:\n            continue\n\n        if not selected_np:\n            diversity = 1.0\n        else:\n            dists = []\n            for t_np in selected_np:\n                inter = len(np.intersect1d(s_np, t_np, assume_unique=True))\n                uni   = len(np.union1d(s_np, t_np, assume_unique=True))\n                jac   = inter / (uni + 1e-12)\n                jac   = np.clip(jac, 0.0, 1.0)\n                dists.append(1.0 - jac)\n            diversity = np.clip(np.mean(dists), 0.0, 1.0)\n\n        # deterministic noise for stable tie\u2011breaking\n        score = new_cov * (1.0 + 0.5 * diversity) + 1e-6 * idx\n        if score > best_score:\n            best_score = score\n            best_subset = s\n\n    return best_subset\n\n",
  "diversity_bonus_jaccard_aug_31": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"List\u2011comprehension based greedy selector with soft top\u2011k choice.\"\"\"\n    rem_set = set(remaining_elements)\n    selected_np = [np.array(s) for s in selected_subsets]\n    scores = []\n    subsets = []\n\n    for idx, s in enumerate(remaining_subsets):\n        s_np = np.array(s)\n        new_cov = len(rem_set.intersection(set(s_np)))\n        if new_cov == 0:\n            continue\n\n        if not selected_np:\n            diversity = 1.0\n        else:\n            dists = [\n                1.0 - np.clip(\n                    len(np.intersect1d(s_np, t, assume_unique=True)) /\n                    (len(np.union1d(s_np, t, assume_unique=True)) + 1e-12),\n                    0.0, 1.0)\n                for t in selected_np\n            ]\n            diversity = np.clip(np.mean(dists), 0.0, 1.0)\n\n        score = new_cov * (1.0 + 0.3 * diversity) + 1e-6 * idx\n        scores.append(score)\n        subsets.append(s)\n\n    if not scores:\n        return None\n\n    top_k = 5\n    top_indices = np.argsort(scores)[-top_k:]\n    rng = np.random.default_rng(42)\n    chosen_idx = rng.choice(top_indices)\n    return subsets[chosen_idx]\n\n",
  "diversity_bonus_jaccard_aug_32": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"While\u2011loop selector using maximum Jaccard distance and deterministic noise.\"\"\"\n    rem_set = set(remaining_elements)\n    selected_np = [np.array(s) for s in selected_subsets]\n    best_subset = None\n    best_score = -np.inf\n    i = 0\n\n    while i < len(remaining_subsets):\n        s = remaining_subsets[i]\n        s_np = np.array(s)\n        new_cov = len(rem_set.intersection(set(s_np)))\n        if new_cov > 0:\n            if not selected_np:\n                diversity = 1.0\n            else:\n                dists = [\n                    1.0 - np.clip(\n                        len(np.intersect1d(s_np, t, assume_unique=True)) /\n                        (len(np.union1d(s_np, t, assume_unique=True)) + 1e-12),\n                        0.0, 1.0)\n                    for t in selected_np\n                ]\n                diversity = np.clip(np.max(dists), 0.0, 1.0)\n\n            score = new_cov * (1.0 + 0.5 * diversity) + 1e-6 * i\n            if score > best_score:\n                best_score = score\n                best_subset = s\n        i += 1\n\n    return best_subset\n\n",
  "diversity_bonus_jaccard_aug_33": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Soft\u2011max weighted selector with exponential score and clipped values.\"\"\"\n    rem_set = set(remaining_elements)\n    selected_np = [np.array(s) for s in selected_subsets]\n    best_subset = None\n    best_exp = -np.inf\n    i = 0\n\n    while i < len(remaining_subsets):\n        s = remaining_subsets[i]\n        s_np = np.array(s)\n        new_cov = len(rem_set.intersection(set(s_np)))\n        if new_cov == 0:\n            i += 1\n            continue\n\n        if not selected_np:\n            diversity = 1.0\n        else:\n            dists = [\n                1.0 - np.clip(\n                    len(np.intersect1d(s_np, t, assume_unique=True)) /\n                    (len(np.union1d(s_np, t, assume_unique=True)) + 1e-12),\n                    0.0, 1.0)\n                for t in selected_np\n            ]\n            diversity = np.clip(np.max(dists), 0.0, 1.0)\n\n        score = new_cov * (1.0 + 0.5 * diversity)\n        # soft\u2011max style: exponentiate and add tiny epsilon for stability\n        exp_score = np.exp(np.clip(score, -50, 50)) + 1e-12 * i\n        if exp_score > best_exp:\n            best_exp = exp_score\n            best_subset = s\n        i += 1\n\n    return best_subset\n\n",
  "diversity_bonus_jaccard_aug_34": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Functional style selector using median diversity and deterministic tie\u2011break.\"\"\"\n    rem_set = set(remaining_elements)\n    selected_np = [np.array(s) for s in selected_subsets]\n\n    def score_subset(s):\n        s_np = np.array(s)\n        new_cov = len(rem_set.intersection(set(s_np)))\n        if new_cov == 0:\n            return None\n        if not selected_np:\n            diversity = 1.0\n        else:\n            dists = [\n                1.0 - np.clip(\n                    len(np.intersect1d(s_np, t, assume_unique=True)) /\n                    (len(np.union1d(s_np, t, assume_unique=True)) + 1e-12),\n                    0.0, 1.0)\n                for t in selected_np\n            ]\n            diversity = np.clip(np.median(dists), 0.0, 1.0)\n        return (new_cov * (1.0 + 0.5 * diversity), s)\n\n    scored = list(map(score_subset, remaining_subsets))\n    scored = [item for item in scored if item is not None]\n\n    if not scored:\n        return None\n\n    # deterministic tie\u2011breaking by adding a tiny index\u2011based offset\n    best = max(scored, key=lambda x: x[0] + 1e-6 * remaining_subsets.index(x[1]))\n    return best[1]\n\n",
  "two_step_lookahead_aug_35": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Greedy selection with deterministic noise and vectorised gains.\"\"\"\n    # Safety constants\n    epsilon = 1e-12\n    gamma = np.clip(0.7, 0, 1)          # tuned weight\n    noise = np.arange(len(remaining_subsets), dtype=float) * 1e-6\n\n    # Pre\u2011compute set objects\n    sets = [set(s) for s in remaining_subsets]\n    rem0   = set(remaining_elements)\n\n    # Compute immediate gains\n    gains1 = np.array([len(rem0 & s) for s in sets], dtype=float)\n\n    # Filter out subsets that bring no new element\n    valid_idx = np.where(gains1 > 0)[0]\n    if valid_idx.size == 0:\n        return None\n\n    # Prepare arrays for best second\u2011step gains\n    best_gain2 = np.zeros(len(remaining_subsets), dtype=float)\n    for i in valid_idx:\n        rem1 = rem0 - sets[i]\n        # second\u2011step gains against all other remaining subsets\n        second_gains = np.array([len(rem1 & t) for j, t in enumerate(sets) if j != i], dtype=float)\n        best_gain2[i] = np.max(second_gains, initial=0)\n\n    # Final scores with deterministic noise\n    scores = gains1 + gamma * (best_gain2 / (1 + epsilon)) + noise\n    # Clip to avoid overflow\n    scores = np.clip(scores, -1e18, 1e18)\n\n    best_idx = int(np.argmax(scores[valid_idx]))\n    return remaining_subsets[valid_idx[best_idx]]\n\n",
  "two_step_lookahead_aug_36": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Uses a soft\u2011max style selection and mean\u2011adjusted scoring.\"\"\"\n    epsilon = 1e-12\n    gamma = np.clip(0.5, 0, 1)          # tuned weight\n    sets = [set(s) for s in remaining_subsets]\n    rem0 = set(remaining_elements)\n\n    # Compute all gains in a single pass\n    gains1 = np.array([len(rem0 & s) for s in sets], dtype=float)\n    valid = gains1 > 0\n    if not np.any(valid):\n        return None\n\n    # Mean of all gains for baseline adjustment\n    mean_gain = np.mean(gains1[valid])\n\n    # Second\u2011step gains\n    best_gain2 = np.zeros(len(remaining_subsets), dtype=float)\n    for i in range(len(remaining_subsets)):\n        if not valid[i]:\n            continue\n        rem1 = rem0 - sets[i]\n        second = np.array([len(rem1 & t) for j, t in enumerate(sets) if j != i], dtype=float)\n        best_gain2[i] = np.max(second, initial=0)\n\n    # Soft\u2011max style scores\n    raw_scores = gains1 + gamma * (best_gain2 - mean_gain)\n    # Avoid division by zero in exponentiation\n    exp_vals = np.exp(-raw_scores / (1 + epsilon))\n    probs = exp_vals / (np.sum(exp_vals) + epsilon)\n    chosen_idx = int(np.argmax(probs[valid]))\n    return remaining_subsets[valid.nonzero()[0][chosen_idx]]\n\n",
  "two_step_lookahead_aug_37": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Randomly selects among the top\u2011k scoring subsets with deterministic seed.\"\"\"\n    np.random.seed(0)                    # deterministic randomness\n    epsilon = 1e-12\n    gamma = np.clip(0.6, 0, 1)\n    k = 3                                 # top\u2011k parameter\n\n    sets = [set(s) for s in remaining_subsets]\n    rem0 = set(remaining_elements)\n\n    gains1 = np.array([len(rem0 & s) for s in sets], dtype=float)\n    valid = gains1 > 0\n    if not np.any(valid):\n        return None\n\n    # Compute best second\u2011step gains\n    best_gain2 = np.zeros(len(remaining_subsets), dtype=float)\n    for i in range(len(remaining_subsets)):\n        if not valid[i]:\n            continue\n        rem1 = rem0 - sets[i]\n        second = np.array([len(rem1 & t) for j, t in enumerate(sets) if j != i], dtype=float)\n        best_gain2[i] = np.max(second, initial=0)\n\n    # Final scores with a tiny noise for deterministic tie\u2011breaking\n    scores = gains1 + gamma * best_gain2 + 1e-8 * np.arange(len(remaining_subsets))\n    # Select top\u2011k indices\n    topk_idx = np.argpartition(-scores, k-1)[:k]\n    # Randomly pick among them\n    chosen = int(np.random.choice(topk_idx))\n    return remaining_subsets[chosen]\n\n",
  "two_step_lookahead_aug_38": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Uses array\u2011based intersection proxy and clipped gamma.\"\"\"\n    epsilon = 1e-12\n    gamma = np.clip(0.55, 0, 1)\n\n    # Convert everything to numpy arrays for vectorised operations\n    rem0_arr = np.array(remaining_elements, dtype=int)\n    sets_arr = [np.array(s, dtype=int) for s in remaining_subsets]\n\n    # Helper to compute intersection size using np.isin\n    def inter_size(a, b):\n        return np.sum(np.isin(a, b))\n\n    gains1 = np.array([inter_size(rem0_arr, s) for s in sets_arr], dtype=float)\n    valid = gains1 > 0\n    if not np.any(valid):\n        return None\n\n    best_gain2 = np.zeros(len(remaining_subsets), dtype=float)\n    for i in range(len(remaining_subsets)):\n        if not valid[i]:\n            continue\n        rem1_arr = np.setdiff1d(rem0_arr, sets_arr[i], assume_unique=True)\n        second = np.array([inter_size(rem1_arr, t) for j, t in enumerate(sets_arr) if j != i], dtype=float)\n        best_gain2[i] = np.max(second, initial=0)\n\n    scores = gains1 + gamma * (best_gain2 / (1 + epsilon))\n    best_idx = int(np.argmax(scores))\n    return remaining_subsets[best_idx]\n\n",
  "two_step_lookahead_aug_39": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Lambda\u2011based scoring with max aggregation and noise for ties.\"\"\"\n    epsilon = 1e-12\n    gamma = np.clip(0.65, 0, 1)\n\n    rem0 = set(remaining_elements)\n    sets = [set(s) for s in remaining_subsets]\n\n    # Immediate gains\n    gains1 = np.array([len(rem0 & s) for s in sets], dtype=float)\n\n    # Lambda to compute score for each index\n    score_fn = lambda i: (\n        gains1[i] +\n        gamma * np.max(\n            [len((rem0 - sets[i]) & t) for j, t in enumerate(sets) if j != i],\n            default=0\n        )\n    )\n\n    # Add deterministic noise for tie\u2011breaking\n    noise = np.arange(len(remaining_subsets), dtype=float) * 1e-7\n    scores = np.array([score_fn(i) + noise[i] for i in range(len(remaining_subsets))], dtype=float)\n\n    # Filter out subsets that contribute nothing\n    valid = gains1 > 0\n    if not np.any(valid):\n        return None\n\n    best_idx = int(np.argmax(scores[valid]))\n    return remaining_subsets[valid.nonzero()[0][best_idx]]\n\n",
  "unique_element_bonus_aug_40": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Greedy selector that prefers subsets covering unique remaining elements.\"\"\"\n    rem_set = set(remaining_elements)\n    freq_dict: Dict[int, int] = {e: 0 for e in rem_set}\n    for subset in remaining_subsets:\n        for e in set(subset):\n            if e in rem_set:\n                freq_dict[e] += 1\n\n    best_set = None\n    best_score = -np.inf\n    for subset in remaining_subsets:\n        new_elems = rem_set.intersection(subset)\n        if not new_elems:\n            continue\n        unique_cnt = sum(1 for e in new_elems if freq_dict[e] == 1)\n        score = len(new_elems) + 2.0 * unique_cnt\n        score = np.clip(score, 0, 1e6)          # bound the score\n        if score > best_score:\n            best_score = score\n            best_set = subset\n    return best_set\n\n",
  "unique_element_bonus_aug_41": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Same logic but uses a while\u2011loop and a different weighting scheme.\"\"\"\n    rem_set = set(remaining_elements)\n    freq_dict: Dict[int, int] = {}\n    for subset in remaining_subsets:\n        for e in set(subset):\n            if e in rem_set:\n                freq_dict[e] = freq_dict.get(e, 0) + 1\n\n    idx = 0\n    best_set = None\n    best_score = -np.inf\n    while idx < len(remaining_subsets):\n        subset = remaining_subsets[idx]\n        new_elems = rem_set.intersection(subset)\n        if new_elems:\n            unique_cnt = sum(1 for e in new_elems if freq_dict[e] == 1)\n            score = 0.5 * len(new_elems) + 1.5 * unique_cnt  # altered weights\n            score = np.clip(score, -1e6, 1e6)\n            if score > best_score:\n                best_score = score\n                best_set = subset\n        idx += 1\n    return best_set\n\n",
  "unique_element_bonus_aug_42": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Selects a subset from the top\u2011k highest scoring ones using deterministic randomness.\"\"\"\n    rng = np.random.default_rng(42)  # deterministic seed\n    rem_set = set(remaining_elements)\n    freq: Dict[int, int] = {}\n    for subset in remaining_subsets:\n        for e in set(subset):\n            if e in rem_set:\n                freq[e] = freq.get(e, 0) + 1\n\n    scores = []\n    for subset in remaining_subsets:\n        new_elems = rem_set.intersection(subset)\n        if not new_elems:\n            scores.append(-np.inf)\n            continue\n        unique_cnt = sum(1 for e in new_elems if freq[e] == 1)\n        score = len(new_elems) + 2.0 * unique_cnt\n        scores.append(score)\n\n    scores = np.array(scores, dtype=float)\n    # Softmax probabilities for selection\n    exp_scores = np.exp(scores - np.max(scores))\n    probs = exp_scores / (np.sum(exp_scores) + 1e-12)  # epsilon to avoid div\u2011zero\n\n    top_k = min(3, len(remaining_subsets))\n    top_indices = np.argpartition(-scores, top_k - 1)[:top_k]\n    chosen = rng.choice(top_indices, p=probs[top_indices] / (np.sum(probs[top_indices]) + 1e-12))\n    return remaining_subsets[chosen]\n\n",
  "unique_element_bonus_aug_43": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorised frequency counting with numpy arrays and mean\u2011based scoring.\"\"\"\n    rem_arr = np.array(remaining_elements, dtype=int)\n    max_elem = rem_arr.max()\n    freq_arr = np.zeros(max_elem + 1, dtype=int)\n\n    for subset in remaining_subsets:\n        uniq = np.unique(subset)\n        mask = uniq <= max_elem\n        freq_arr[uniq[mask]] += 1\n\n    best_set = None\n    best_score = -np.inf\n    for subset in remaining_subsets:\n        new_arr = np.intersect1d(rem_arr, subset, assume_unique=True)\n        if new_arr.size == 0:\n            continue\n        unique_mask = freq_arr[new_arr] == 1\n        unique_cnt = np.sum(unique_mask)\n        score = np.mean(new_arr) + 0.5 * unique_cnt  # use mean for aggregation\n        score = np.clip(score, 0, 1e6)\n        if score > best_score:\n            best_score = score\n            best_set = subset\n    return best_set\n\n",
  "unique_element_bonus_aug_44": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Median\u2011based scoring with deterministic tie handling.\"\"\"\n    rem_set = set(remaining_elements)\n    freq: Dict[int, int] = {}\n    for subset in remaining_subsets:\n        for e in set(subset):\n            if e in rem_set:\n                freq[e] = freq.get(e, 0) + 1\n\n    best_set = None\n    best_score = -np.inf\n    for subset in remaining_subsets:\n        new_set = rem_set.intersection(subset)\n        if not new_set:\n            continue\n        unique_cnt = sum(1 for e in new_set if freq[e] == 1)\n        median_val = np.median(np.array(list(new_set)))\n        score = median_val + 1.0 * unique_cnt\n        score = np.clip(score, -1e6, 1e6)\n        if score > best_score:\n            best_score = score\n            best_set = subset\n    return best_set\n\n",
  "rarest_element_pivot_aug_45": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem = np.array(remaining_elements, dtype=int)\n    if rem.size == 0:\n        return None\n\n    # Frequency array\n    freq = np.zeros(rem.size, dtype=int)\n    elem_to_idx = {e: i for i, e in enumerate(rem)}\n    for s in remaining_subsets:\n        for e in set(s):\n            idx = elem_to_idx.get(e)\n            if idx is not None:\n                freq[idx] += 1\n\n    # Choose pivot: element with minimal positive frequency\n    pos_mask = freq > 0\n    if not pos_mask.any():\n        return None\n    min_idx = np.argmin(freq + (~pos_mask).astype(int) * 1e9)  # large number for zeros\n    pivot = rem[min_idx]\n\n    rem_set = set(rem)\n    best = None\n    best_score = -1.0\n    for s in remaining_subsets:\n        if pivot not in s:\n            continue\n        new_cov = len(rem_set.intersection(s))\n        score = np.clip(new_cov / (len(s) + 1e-12), 0, 1)  # weighted by size with epsilon\n        if score > best_score + 1e-12:  # epsilon tie\u2011breaking\n            best_score = score\n            best = s\n    return best if best_score > 0 else None\n\n",
  "rarest_element_pivot_aug_46": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Frequency via list comprehension\n    freq = {e: sum(1 for s in remaining_subsets if e in s) for e in rem_set}\n    pivot = min((e for e in freq if freq[e] > 0), key=freq.get, default=None)\n    if pivot is None:\n        return None\n\n    # Weighted score\n    scores = []\n    for idx, s in enumerate(remaining_subsets):\n        if pivot not in s:\n            scores.append((idx, -1.0))\n            continue\n        gain = len(rem_set.intersection(s))\n        score = gain / (np.sqrt(len(s)) + 1e-12)  # epsilon\n        score += idx * 1e-6  # deterministic noise\n        scores.append((idx, score))\n\n    # Filter positive scores\n    scores = [(i, sc) for i, sc in scores if sc > 0]\n    if not scores:\n        return None\n\n    # Sort by score descending\n    scores.sort(key=lambda x: x[1], reverse=True)\n\n    # Top\u20113 selection\n    top_k = min(3, len(scores))\n    top_indices = [i for i, _ in scores[:top_k]]\n    chosen_idx = np.random.choice(top_indices)\n    return remaining_subsets[chosen_idx]\n\n",
  "rarest_element_pivot_aug_47": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem = np.array(remaining_elements, dtype=int)\n    if rem.size == 0:\n        return None\n\n    # Frequency using numpy\n    freq = np.zeros(rem.size, dtype=int)\n    elem_to_idx = {e: i for i, e in enumerate(rem)}\n    for s in remaining_subsets:\n        for e in set(s):\n            idx = elem_to_idx.get(e)\n            if idx is not None:\n                freq[idx] += 1\n\n    # Pivot: element with frequency <= median\n    median_freq = np.median(freq)\n    candidates = [rem[i] for i, f in enumerate(freq) if f > 0 and f <= median_freq]\n    if not candidates:\n        return None\n    pivot = candidates[0]  # deterministic first\n\n    rem_set = set(rem)\n    # Compute scores\n    scores = []\n    for s in remaining_subsets:\n        if pivot not in s:\n            continue\n        gain = len(rem_set.intersection(s))\n        score = gain / (len(s) + 1e-12)  # epsilon\n        scores.append(score)\n\n    if not scores:\n        return None\n\n    # Softmin selection\n    neg_scores = -np.array(scores)\n    exp_vals = np.exp(neg_scores / (1.0 + 1e-12))\n    probs = exp_vals / (np.sum(exp_vals) + 1e-12)  # epsilon\n    probs = np.clip(probs, 0, 1)\n    idx = np.random.choice(len(scores), p=probs)\n    return remaining_subsets[idx]\n\n",
  "rarest_element_pivot_aug_48": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Approximate frequencies by sampling\n    sample_size = min(50, len(remaining_subsets))\n    sample_indices = np.random.choice(len(remaining_subsets), size=sample_size, replace=False)\n    sample_subsets = [remaining_subsets[i] for i in sample_indices]\n    freq = {e: 0 for e in rem_set}\n    for s in sample_subsets:\n        for e in set(s):\n            if e in freq:\n                freq[e] += 1\n\n    pivot = min((e for e in freq if freq[e] > 0), key=freq.get, default=None)\n    if pivot is None:\n        return None\n\n    # Compute gains for all subsets containing pivot\n    best = None\n    best_gain = -1\n    for idx, s in enumerate(remaining_subsets):\n        if pivot not in s:\n            continue\n        gain = len(rem_set.intersection(s))\n        gain = np.clip(gain, 0, len(rem_set))  # bound with np.clip\n        gain += idx * 1e-6  # deterministic noise\n        if gain > best_gain:\n            best_gain = gain\n            best = s\n\n    return best if best_gain > 0 else None\n\n",
  "rarest_element_pivot_aug_49": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    if not remaining_elements:\n        return None\n\n    rem = np.array(remaining_elements, dtype=int)\n    n_sub = len(remaining_subsets)\n    max_len = max(len(s) for s in remaining_subsets)\n\n    # Build padded array\n    arr = -np.ones((n_sub, max_len), dtype=int)\n    for i, s in enumerate(remaining_subsets):\n        arr[i, :len(s)] = s\n\n    valid_mask = arr != -1\n    # Compute intersection counts\n    in_rem = np.isin(arr, rem)\n    gains = np.sum(in_rem & valid_mask, axis=1)\n\n    # Pivot selection: element with minimum positive frequency\n    freq = np.sum(in_rem & valid_mask, axis=0)[:len(rem)]\n    pos_mask = freq > 0\n    if not np.any(pos_mask):\n        return None\n    pivot_idx = np.argmin(freq + (~pos_mask).astype(int) * 1e9)\n    pivot = rem[pivot_idx]\n\n    # Filter subsets containing pivot\n    contains_pivot = np.isin(arr, pivot) & valid_mask\n    contains_any = np.any(contains_pivot, axis=1)\n    if not np.any(contains_any):\n        return None\n\n    # Scores\n    lengths = np.array([len(s) for s in remaining_subsets], dtype=float)\n    scores = gains / (np.sqrt(lengths) + 1e-12)\n    scores = np.clip(scores, 0, 1)\n    scores += np.arange(n_sub) * 1e-9  # epsilon tie\u2011breaking\n    best_idx = np.argmax(scores * contains_any)\n    return remaining_subsets[best_idx]\n\n",
  "maximin_rarity_aug_50": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Choose set that best improves the worst\u2011case rarity among elements it newly covers.\"\"\"\n    rem_set = set(remaining_elements)\n    freq: dict[int, int] = {}\n\n    # Count how many remaining subsets cover each element (while loop)\n    i = 0\n    while i < len(remaining_subsets):\n        s = remaining_subsets[i]\n        for e in set(s):\n            if e in rem_set:\n                freq[e] = freq.get(e, 0) + 1\n        i += 1\n\n    best_subset = None\n    best_tuple = None  # (min_rarity, new_count, -size, -idx)\n    i = 0\n    while i < len(remaining_subsets):\n        s = remaining_subsets[i]\n        new_elems = [e for e in set(s) if e in rem_set]\n        if not new_elems:\n            i += 1\n            continue\n\n        # compute rarities and clip to avoid overflow\n        rarities = np.array([1.0 / (freq.get(e, 1) + 1e-12) for e in new_elems])\n        rarities = np.clip(rarities, 0, 1e9)\n\n        t = (np.min(rarities), len(new_elems), -len(s), -i)\n        if best_tuple is None or t > best_tuple:\n            best_tuple = t\n            best_subset = s\n        i += 1\n\n    return best_subset\n\n",
  "maximin_rarity_aug_51": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    freq: dict[int, int] = {}\n\n    # Frequency counting using a simple for loop\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem_set:\n                freq[e] = freq.get(e, 0) + 1\n\n    candidates = []\n    for s in remaining_subsets:\n        new = [e for e in set(s) if e in rem_set]\n        if not new:\n            continue\n\n        rarities = np.array([1.0 / (freq[e] + 1e-12) for e in new])\n        rarities = np.clip(rarities, 0, 1e9)\n        # softmin weighting\n        scores = np.exp(-rarities)\n        total_score = np.sum(scores)\n        candidates.append((s, total_score, len(new), len(s)))\n\n    if not candidates:\n        return None\n\n    # Keep the top\u2011k candidates and choose one at random\n    top_k = 5\n    candidates.sort(key=lambda x: x[1], reverse=True)\n    top_candidates = candidates[:top_k]\n    rng = np.random.default_rng()\n    chosen = rng.choice([c[0] for c in top_candidates])\n    return chosen\n\n",
  "maximin_rarity_aug_52": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    freq: dict[int, int] = {}\n\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem_set:\n                freq[e] = freq.get(e, 0) + 1\n\n    def score_subset(s: List[int]):\n        new = [e for e in set(s) if e in rem_set]\n        if not new:\n            return None\n        rarities = np.array([1.0 / (freq[e] + 1e-12) for e in new])\n        rarities = np.clip(rarities, 0, 1e9)\n        median_rarity = np.median(rarities)\n        # softmax over negative rarities (higher rarity \u2192 lower weight)\n        weights = np.exp(-rarities - median_rarity)\n        return (median_rarity, np.sum(weights), len(new), -len(s))\n\n    scored = [(s, score_subset(s)) for s in remaining_subsets]\n    scored = [(s, sc) for s, sc in scored if sc is not None]\n    if not scored:\n        return None\n\n    # Softmax over median rarity to obtain selection probabilities\n    medians = np.array([sc[0] for _, sc in scored])\n    probs = np.exp(-medians)\n    probs /= np.sum(probs)\n\n    rng = np.random.default_rng()\n    idx = rng.choice(len(scored), p=probs)\n    return scored[idx][0]\n\n",
  "maximin_rarity_aug_53": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    freq: dict[int, int] = {}\n\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem_set:\n                freq[e] = freq.get(e, 0) + 1\n\n    # Compute a score for each subset (max rarity of newly covered elements)\n    scores = []\n    for s in remaining_subsets:\n        new = [e for e in set(s) if e in rem_set]\n        if not new:\n            scores.append(-np.inf)\n            continue\n        rarities = np.array([1.0 / (freq[e] + 1e-12) for e in new])\n        rarities = np.clip(rarities, 0, 1e9)\n        scores.append(np.max(rarities))\n\n    scores = np.array(scores)\n    # Add tiny deterministic noise to break ties\n    noise = np.arange(len(scores)) * 1e-9\n    scores += noise\n\n    idx = np.argmax(scores)\n    return remaining_subsets[idx] if np.isfinite(scores[idx]) else None\n\n",
  "random_among_topk_gain_aug_54": "import numpy as np\nimport random\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Select a subset using a while\u2011loop, soft tie\u2011breaking noise, and a 12% top\u2011k rule.\"\"\"\n    rem_set = set(remaining_elements)\n    scored = []\n    idx = 0\n    while idx < len(remaining_subsets):\n        subset = remaining_subsets[idx]\n        gain = len(rem_set.intersection(subset))\n        if gain > 0:\n            # Clip to avoid any accidental overflow and add tiny noise for deterministic tie\u2011breaks\n            noisy_gain = np.clip(gain, 0, len(rem_set)) + np.random.default_rng().random() * 1e-6\n            scored.append((noisy_gain, subset))\n        idx += 1\n\n    if not scored:\n        return None\n\n    scored.sort(key=lambda x: x[0], reverse=True)\n    top_k = max(1, int(np.ceil(0.12 * len(scored))))  # 12% of the candidates\n    top_candidates = [s for _, s in scored[:top_k]]\n    return random.choice(top_candidates)\n\n",
  "random_among_topk_gain_aug_55": "import numpy as np\nimport random\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorised selection using softmax weighting and a fixed top\u2011k of 7.\"\"\"\n    rem_set = set(remaining_elements)\n    gains = np.array([len(rem_set.intersection(sub)) for sub in remaining_subsets], dtype=float)\n\n    # Remove zero\u2011gain subsets\n    positive_idx = np.where(gains > 0)[0]\n    if positive_idx.size == 0:\n        return None\n\n    gains = gains[positive_idx]\n    subsets = [remaining_subsets[i] for i in positive_idx]\n\n    # Softmax weights with epsilon to avoid division by zero\n    exp_gains = np.exp(gains - np.max(gains))  # stability trick\n    softmax = exp_gains / (np.sum(exp_gains) + 1e-12)\n    softmax = np.clip(softmax, 0, 1)\n\n    # Pick the subset with the highest softmax probability\n    best_idx = int(np.argmax(softmax))\n    return subsets[best_idx]\n\n",
  "random_among_topk_gain_aug_56": "import numpy as np\nimport random\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Recursive helper with weighted scoring favouring smaller sets.\"\"\"\n    rem_set = set(remaining_elements)\n\n    def _score(subsets):\n        scored = []\n        for sub in subsets:\n            gain = len(rem_set.intersection(sub))\n            if gain > 0:\n                # Weighted score: 70% gain, 30% inverse size\n                weight = 0.7 * gain + 0.3 * (len(sub) + 1e-12)  # +epsilon to avoid zero\n                scored.append((weight, sub))\n        return scored\n\n    scored = _score(remaining_subsets)\n    if not scored:\n        return None\n\n    # Invert conditional: return None only if scored is empty\n    scored.sort(key=lambda x: x[0], reverse=True)\n    top_k = max(1, int(np.ceil(0.15 * len(scored))))  # 15% top\n    top_candidates = [s for _, s in scored[:top_k]]\n    # Deterministic tie\u2011break by adding small noise\n    noisy_scores = [(s, np.random.default_rng().random() * 1e-6) for s in top_candidates]\n    chosen = max(noisy_scores, key=lambda x: x[1])[0]\n    return chosen\n\n",
  "random_among_topk_gain_aug_57": "import numpy as np\nimport random\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Softmin weighting with np.argsort and a fixed top\u2011k of 5.\"\"\"\n    rem_set = set(remaining_elements)\n    gains = np.array([len(rem_set.intersection(sub)) for sub in remaining_subsets], dtype=float)\n\n    # Softmin: 1/(gain+epsilon) to favour larger gains\n    eps = 1e-12\n    softmin = 1.0 / (gains + eps)\n    softmin = np.clip(softmin, 0, 1)  # bound between 0 and 1\n\n    # Get indices sorted by softmin (ascending -> best)\n    sorted_idx = np.argsort(softmin)\n    top_k = min(5, len(sorted_idx))\n    top_candidates = [remaining_subsets[i] for i in sorted_idx[:top_k]]\n    return random.choice(top_candidates)\n\n",
  "random_among_topk_gain_aug_58": "import numpy as np\nimport random\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Functional style with weighted score and deterministic noise.\"\"\"\n    rem_set = set(remaining_elements)\n\n    # Compute weighted scores: 60% gain, 40% subset size\n    def score(sub):\n        g = len(rem_set.intersection(sub))\n        return 0.6 * g + 0.4 * (len(sub) + 1e-12)  # epsilon to avoid zero division\n\n    scored = list(map(lambda s: (score(s), s), remaining_subsets))\n    scored = [(sc, sub) for sc, sub in scored if sc > 0]\n\n    if not scored:\n        return None\n\n    # Sort by score and pick top\u2011k (k=5)\n    scored.sort(key=lambda x: x[0], reverse=True)\n    top_k = min(5, len(scored))\n    top_candidates = [sub for _, sub in scored[:top_k]]\n\n    # Add deterministic noise for tie\u2011breaking\n    rng = np.random.default_rng()\n    noise = rng.random(len(top_candidates)) * 1e-6\n    noisy_scores = [(sub, noise[i]) for i, sub in enumerate(top_candidates)]\n    chosen = max(noisy_scores, key=lambda x: x[1])[0]\n    return chosen\n\n",
  "softmax_sampling_gain_aug_59": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_elems = set(remaining_elements)\n    cand_subsets = []\n    gains = []\n    for sub in remaining_subsets:\n        g = len(rem_elems.intersection(sub))\n        if g > 0:\n            cand_subsets.append(sub)\n            gains.append(g)\n    if not cand_subsets:\n        return None\n    temp = max(1e-6, np.median(gains))\n    weights = np.exp(np.array(gains, dtype=float) / temp)\n    probs = weights / (weights.sum() + 1e-12)\n    probs = np.clip(probs, 0, 1)\n    idx = int(np.random.choice(len(cand_subsets), p=probs))\n    return cand_subsets[idx]\n\n",
  "softmax_sampling_gain_aug_60": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_elems = set(remaining_elements)\n    cand_subsets = []\n    raw_gains = []\n    for sub in remaining_subsets:\n        g = len(rem_elems.intersection(sub))\n        if g > 0:\n            cand_subsets.append(sub)\n            raw_gains.append(g)\n    if not cand_subsets:\n        return None\n    noise = np.random.uniform(-1e-6, 1e-6, size=len(raw_gains))\n    gains = np.array(raw_gains, dtype=float) + noise\n    temp = max(1e-6, np.mean(gains))\n    weights = np.exp(gains / temp)\n    probs = weights / (weights.sum() + 1e-12)\n    probs = np.clip(probs, 0, 1)\n    max_gain = np.max(gains)\n    tie_indices = np.where(gains == max_gain)[0]\n    if len(tie_indices) > 1:\n        idx = int(np.random.choice(tie_indices))\n    else:\n        idx = int(np.random.choice(len(cand_subsets), p=probs))\n    return cand_subsets[idx]\n\n",
  "softmax_sampling_gain_aug_61": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_elems = set(remaining_elements)\n    cand_subsets = []\n    gains = []\n    for sub in remaining_subsets:\n        g = len(rem_elems.intersection(sub))\n        if g > 0:\n            cand_subsets.append(sub)\n            gains.append(g)\n    if not cand_subsets:\n        return None\n    k = min(3, len(gains))\n    top_indices = np.argpartition(-np.array(gains), k-1)[:k]\n    top_subsets = [cand_subsets[i] for i in top_indices]\n    top_gains = np.array([gains[i] for i in top_indices], dtype=float)\n    temp = max(1e-6, np.median(top_gains))\n    weights = np.exp(-top_gains / temp)\n    probs = weights / (weights.sum() + 1e-12)\n    probs = np.clip(probs, 0, 1)\n    idx = int(np.random.choice(len(top_subsets), p=probs))\n    return top_subsets[idx]\n\n",
  "softmax_sampling_gain_aug_62": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_elems = set(remaining_elements)\n    cand_subsets = []\n    gains = []\n    for sub in remaining_subsets:\n        g = len(rem_elems.intersection(sub))\n        if g > 0:\n            cand_subsets.append(sub)\n            gains.append(g)\n    if not cand_subsets:\n        return None\n    gains = np.array(gains, dtype=float)\n    temp = max(1e-6, np.max(gains))\n    weights = np.exp((gains ** 2) / temp)\n    probs = weights / (weights.sum() + 1e-12)\n    probs = np.clip(probs, 0, 1)\n    idx = int(np.random.choice(len(cand_subsets), p=probs))\n    return cand_subsets[idx]\n\n",
  "softmax_sampling_gain_aug_63": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_elems = set(remaining_elements)\n    cand_subsets = []\n    raw_gains = []\n    for sub in remaining_subsets:\n        g = len(rem_elems.intersection(sub))\n        if g > 0:\n            cand_subsets.append(sub)\n            raw_gains.append(g)\n    if not cand_subsets:\n        return None\n    gains = np.array(raw_gains, dtype=float) + 1e-12\n    probs = gains / (gains.sum() + 1e-12)\n    probs = np.clip(probs, 0, 1)\n    max_gain = np.max(gains)\n    tie_indices = np.where(gains == max_gain)[0]\n    if len(tie_indices) > 1:\n        lengths = np.array([len(cand_subsets[i]) for i in tie_indices])\n        idx = int(tie_indices[np.argmin(lengths)])\n    else:\n        idx = int(np.random.choice(len(cand_subsets), p=probs))\n    return cand_subsets[idx]\n\n",
  "annealed_softmax_gain_aug_64": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Softmax sampling with temperature decreasing as coverage progresses.\n    Uses list comprehensions, deterministic noise for tie\u2011breaking and\n    explicit epsilon handling.\"\"\"\n    rem_set = set(remaining_elements)\n    # compute gains and candidates via list comprehension\n    cand_subs, gains_list = zip(\n        *[(s, len(rem_set & set(s))) for s in remaining_subsets if len(rem_set & set(s)) > 0]\n    ) if remaining_subsets else ((), ())\n    if not cand_subs:\n        return None\n\n    # progress estimate\n    n_rem = max(1, len(rem_set))\n    n_total = n_rem + len({e for ss in selected_subsets for e in ss})\n    progress = 1.0 - (n_rem / max(1, n_total))\n\n    # deterministic noise to break ties\n    noise = np.arange(len(gains_list)) * 1e-8\n    gains_arr = np.array(gains_list, dtype=float) + noise\n\n    base = np.clip(np.mean(gains_arr), 1e-6, None)\n    temp = np.clip(base * (1.5 - 1.3 * progress), 1e-6, None)\n    # softmax probabilities\n    exp_vals = np.exp(gains_arr / temp)\n    probs = exp_vals / (np.sum(exp_vals) + 1e-12)\n\n    idx = int(np.random.choice(len(cand_subs), p=probs))\n    return cand_subs[idx]\n\n",
  "annealed_softmax_gain_aug_65": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Softmax sampling with a while loop, median\u2011based base, and top\u2011k selection.\"\"\"\n    rem_set = set(remaining_elements)\n    cand_subs = []\n    gains_arr = []\n\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        g = len(rem_set & set(s))\n        if g > 0:\n            cand_subs.append(s)\n            gains_arr.append(g)\n        idx += 1\n\n    if not cand_subs:\n        return None\n\n    gains_arr = np.array(gains_arr, dtype=float)\n    n_rem = max(1, len(rem_set))\n    n_total = n_rem + len({e for ss in selected_subsets for e in ss})\n    progress = 1.0 - (n_rem / max(1, n_total))\n\n    base = np.clip(np.median(gains_arr), 1e-6, None)\n    temp = np.clip(base * (1.2 + 0.5 * progress), 1e-6, None)\n\n    exp_vals = np.exp(gains_arr / temp)\n    probs = exp_vals / (np.sum(exp_vals) + 1e-12)\n\n    # choose top\u2011k (k=3) candidates deterministically\n    top_k = min(3, len(cand_subs))\n    top_indices = np.argsort(probs)[-top_k:]\n    chosen_idx = top_indices[0]\n    return cand_subs[chosen_idx]\n\n",
  "annealed_softmax_gain_aug_66": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorised softmin with top\u2011k random choice and deterministic noise.\"\"\"\n    rem_set = set(remaining_elements)\n    # pad subsets to equal length for vectorisation\n    max_len = max((len(s) for s in remaining_subsets), default=0)\n    padded = np.full((len(remaining_subsets), max_len), -1, dtype=int)\n    for i, s in enumerate(remaining_subsets):\n        padded[i, :len(s)] = s\n\n    # compute gains via broadcasting\n    rem_arr = np.array(list(rem_set), dtype=int)\n    # create mask of presence\n    mask = np.isin(padded, rem_arr[:, None])\n    gains_arr = mask.sum(axis=1).astype(float)\n\n    # filter out zero gains\n    valid_idx = gains_arr > 0\n    if not np.any(valid_idx):\n        return None\n    cand_subs = [remaining_subsets[i] for i in range(len(remaining_subsets)) if valid_idx[i]]\n    gains_arr = gains_arr[valid_idx]\n\n    # deterministic noise\n    noise = np.linspace(0, 1e-7, len(gains_arr))\n    gains_arr += noise\n\n    # softmin\n    base = np.clip(np.mean(gains_arr), 1e-6, None)\n    temp = np.clip(base * 1.4, 1e-6, None)\n    exp_vals = np.exp(-gains_arr / temp)\n    probs = exp_vals / (np.sum(exp_vals) + 1e-12)\n\n    # random among top\u2011k (k=4)\n    k = min(4, len(cand_subs))\n    top_indices = np.argsort(probs)[-k:]\n    chosen = np.random.choice(top_indices)\n    return cand_subs[chosen]\n\n",
  "annealed_softmax_gain_aug_67": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Functional style with map/filter, sum\u2011based base, and softmax.\"\"\"\n    rem_set = set(remaining_elements)\n\n    def gather(item):\n        s, g = item\n        return (s, g) if g > 0 else None\n\n    candidates = list(filter(None, map(\n        lambda s: gather((s, len(rem_set & set(s)))), remaining_subsets\n    )))\n\n    if not candidates:\n        return None\n\n    cand_subs, gains_arr = zip(*candidates)\n    gains_arr = np.array(gains_arr, dtype=float)\n\n    n_rem = max(1, len(rem_set))\n    n_total = n_rem + len({e for ss in selected_subsets for e in ss})\n    progress = 1.0 - (n_rem / max(1, n_total))\n\n    base = np.clip(np.sum(gains_arr) / (len(gains_arr) + 1e-12), 1e-6, None)\n    temp = np.clip(base * (1.0 + 0.6 * progress), 1e-6, None)\n\n    exp_vals = np.exp(gains_arr / temp)\n    probs = exp_vals / (np.sum(exp_vals) + 1e-12)\n\n    idx = int(np.random.choice(len(cand_subs), p=probs))\n    return cand_subs[idx]\n\n",
  "annealed_softmax_gain_aug_68": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Ratio\u2011based scoring, max\u2011based temperature, deterministic top\u2011k.\"\"\"\n    rem_set = set(remaining_elements)\n\n    # compute gain-to-size ratio\n    ratios = []\n    cand_subs = []\n    for s in remaining_subsets:\n        g = len(rem_set & set(s))\n        if g > 0:\n            ratios.append(g / (len(s) + 1e-12))\n            cand_subs.append(s)\n\n    if not cand_subs:\n        return None\n\n    ratios = np.array(ratios, dtype=float)\n\n    n_rem = max(1, len(rem_set))\n    n_total = n_rem + len({e for ss in selected_subsets for e in ss})\n    progress = 1.0 - (n_rem / max(1, n_total))\n\n    base = np.clip(np.sum(ratios) / (len(ratios) + 1e-12), 1e-6, None)\n    temp = np.clip(base * (1.8 - 1.0 * progress), 1e-6, None)\n\n    exp_vals = np.exp(ratios / temp)\n    probs = exp_vals / (np.sum(exp_vals) + 1e-12)\n\n    # deterministic top\u2011k (k=5)\n    k = min(5, len(cand_subs))\n    top_indices = np.argsort(probs)[-k:]\n    chosen = top_indices[0]  # deterministic pick\n    return cand_subs[chosen]\n\n",
  "progressive_size_penalty_aug_69": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selector that uses a while\u2011loop and adds tiny noise to break ties.\n    \"\"\"\n    # renamed variables for clarity\n    sel_sets = selected_subsets\n    rem_sets = remaining_subsets\n    rem_elems = remaining_elements\n\n    # build a set of remaining elements\n    rem = set(rem_elems)\n    n_rem = max(1, len(rem))\n    # total covered so far (unique elements)\n    total_cov = len({e for ss in sel_sets for e in ss})\n    n_tot = n_rem + total_cov\n\n    # progress (clipped to [0,1])\n    progress = np.clip(1.0 - (n_rem / max(1.0, n_tot)), 0.0, 1.0)\n\n    # penalty exponent that grows as we progress\n    alpha = 0.3 + 1.7 * progress\n\n    best = None\n    best_score = -np.inf\n\n    # iterate using a while loop\n    idx = 0\n    while idx < len(rem_sets):\n        s = rem_sets[idx]\n        new_cov = len(rem.intersection(s))\n        if new_cov > 0:\n            denom = (len(s) ** alpha) + 1e-12   # epsilon to avoid div by zero\n            score = new_cov / denom\n            # add a tiny deterministic noise to break ties\n            noise = (idx + 1) * 1e-6\n            if score + noise > best_score:\n                best_score = score + noise\n                best = s\n        idx += 1\n\n    return best\n\n",
  "progressive_size_penalty_aug_70": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Computes scores with a list comprehension, selects a random subset\n    among the top\u2011k candidates, and uses np.clip to keep progress bounded.\n    \"\"\"\n    rem = set(remaining_elements)\n    n_rem = max(1, len(rem))\n    total_cov = len({e for ss in selected_subsets for e in ss})\n    n_tot = n_rem + total_cov\n\n    progress = np.clip(1.0 - (n_rem / max(1.0, n_tot)), 0.0, 1.0)\n    alpha = 0.25 + 1.75 * progress\n\n    # compute scores for all subsets\n    scores = np.array([\n        (len(rem.intersection(s)) /\n         ((len(s) ** alpha) + 1e-12)) if len(rem.intersection(s)) > 0 else -np.inf\n        for s in remaining_subsets\n    ])\n\n    # if no candidate covers new elements\n    if np.all(scores == -np.inf):\n        return None\n\n    # select top\u2011k indices\n    top_k = 5\n    top_indices = np.argpartition(-scores, top_k - 1)[:top_k]\n    # choose randomly among the top\u2011k\n    chosen = np.random.choice(top_indices)\n    return remaining_subsets[chosen]\n\n",
  "progressive_size_penalty_aug_71": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Fully vectorised implementation that pads all subsets and\n    uses broadcasting to count intersections.\n    \"\"\"\n    rem = set(remaining_elements)\n    n_rem = max(1, len(rem))\n    total_cov = len({e for ss in selected_subsets for e in ss})\n    n_tot = n_rem + total_cov\n\n    progress = np.clip(1.0 - (n_rem / max(1.0, n_tot)), 0.0, 1.0)\n    alpha = 0.28 + 1.72 * progress\n\n    # prepare padded array of all remaining subsets\n    max_len = max(len(s) for s in remaining_subsets)\n    arr = np.full((len(remaining_subsets), max_len), -1, dtype=int)\n    for i, s in enumerate(remaining_subsets):\n        arr[i, :len(s)] = s\n\n    rem_arr = np.array(list(rem), dtype=int)\n\n    # broadcasting to compute intersection counts\n    mask = (arr[:, :, None] == rem_arr[None, None, :])\n    counts = np.sum(mask, axis=2)            # shape (n_subsets, max_len)\n    new_counts = np.sum(counts, axis=1)      # shape (n_subsets,)\n\n    valid = new_counts > 0\n    denom = (arr.shape[1] ** alpha) + 1e-12  # use padded length as penalty\n    scores = np.zeros_like(new_counts, dtype=float)\n    scores[valid] = new_counts[valid] / denom\n\n    # if no valid subset\n    if not np.any(valid):\n        return None\n\n    best_idx = np.argmax(scores)\n    return remaining_subsets[best_idx]\n\n",
  "progressive_size_penalty_aug_72": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Uses a logarithmic penalty on set size, deterministic tie\u2011breaking,\n    and np.clip to keep progress in [0,1].\n    \"\"\"\n    rem = set(remaining_elements)\n    n_rem = max(1, len(rem))\n    total_cov = len({e for ss in selected_subsets for e in ss})\n    n_tot = n_rem + total_cov\n\n    progress = np.clip(1.0 - (n_rem / max(1.0, n_tot)), 0.0, 1.0)\n    alpha = 0.15 + 1.85 * progress\n\n    best = None\n    best_score = -np.inf\n\n    for idx, s in enumerate(remaining_subsets):\n        new_cov = len(rem.intersection(s))\n        if new_cov == 0:\n            continue\n        denom = np.log1p(len(s)) + 1e-12   # logarithmic penalty\n        score = new_cov / (denom ** alpha)\n        # deterministic tie\u2011breaking: prefer smaller index\n        if score > best_score or (np.isclose(score, best_score) and idx < best_idx):\n            best_score = score\n            best = s\n            best_idx = idx\n\n    return best\n\n",
  "progressive_size_penalty_aug_73": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Dynamic weighting of alpha based on remaining elements, shuffles\n    candidate subsets before scoring, and uses the sum of new coverage\n    as the numerator.\n    \"\"\"\n    rem = set(remaining_elements)\n    n_rem = max(1, len(rem))\n    total_cov = len({e for ss in selected_subsets for e in ss})\n    n_tot = n_rem + total_cov\n\n    progress = np.clip(1.0 - (n_rem / max(1.0, n_tot)), 0.0, 1.0)\n    # alpha decreases as we cover more elements\n    alpha = 0.3 + 1.7 * (n_rem / (n_rem + total_cov + 1e-12))\n\n    # shuffle to avoid deterministic bias\n    shuffled_idx = np.random.permutation(len(remaining_subsets))\n    best = None\n    best_score = -np.inf\n\n    for idx in shuffled_idx:\n        s = remaining_subsets[idx]\n        new_cov = len(rem.intersection(s))\n        if new_cov == 0:\n            continue\n        denom = (len(s) ** alpha) + 1e-12\n        score = new_cov / denom\n        if score > best_score:\n            best_score = score\n            best = s\n\n    return best\n\n",
  "mean_size_balancing_aug_74": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    if not remaining_subsets:\n        return None\n\n    sizes = np.array([len(s) for s in remaining_subsets], dtype=np.float64)\n    median_size = float(np.median(sizes))\n    inter_counts = np.array([len(rem_set & set(s)) for s in remaining_subsets], dtype=np.float64)\n\n    valid_mask = inter_counts > 0\n    if not np.any(valid_mask):\n        return None\n\n    size_pen = np.abs(sizes - median_size) / (median_size + 1e-12)\n    size_pen = np.clip(size_pen, 0, 1)\n    scores = inter_counts - 0.7 * size_pen\n\n    # deterministic tie\u2011breaking noise\n    noise = np.arange(len(remaining_subsets), dtype=np.float64) * 1e-9\n    scores += noise\n\n    valid_indices = np.nonzero(valid_mask)[0]\n    best_idx = int(np.argmax(scores[valid_mask]))\n    return remaining_subsets[valid_indices[best_idx]]\n\n",
  "mean_size_balancing_aug_75": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    if not remaining_subsets:\n        return None\n\n    mean_size = float(np.mean([len(s) for s in remaining_subsets]))\n    best = None\n    best_score = -np.inf\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        new = len(rem_set & set(s))\n        if new > 0:\n            size = len(s)\n            size_pen = abs(size - mean_size) / (mean_size + 1e-12)\n            size_pen = np.clip(size_pen, 0, 1)\n            score = new - 0.4 * size_pen\n            if score > best_score:\n                best_score = score\n                best = s\n        idx += 1\n\n    if best is None:\n        return None\n\n    # Randomly pick among the top\u2011k best subsets\n    scores = []\n    for s in remaining_subsets:\n        new = len(rem_set & set(s))\n        if new > 0:\n            size = len(s)\n            size_pen = abs(size - mean_size) / (mean_size + 1e-12)\n            size_pen = np.clip(size_pen, 0, 1)\n            score = new - 0.4 * size_pen\n            scores.append((score, s))\n    scores.sort(key=lambda x: x[0], reverse=True)\n    top_k = min(5, len(scores))\n    if top_k == 0:\n        return None\n    rng = np.random.default_rng()\n    chosen = rng.choice([s for _, s in scores[:top_k]])\n    return chosen\n\n",
  "mean_size_balancing_aug_76": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    if not remaining_subsets:\n        return None\n\n    sizes = np.array([len(s) for s in remaining_subsets], dtype=np.float64)\n    mean_size = float(np.sum(sizes) / (len(sizes) + 1e-12))\n    inter_counts = np.array([len(rem_set & set(s)) for s in remaining_subsets], dtype=np.float64)\n\n    valid = inter_counts > 0\n    if not np.any(valid):\n        return None\n\n    size_pen = np.abs(sizes - mean_size) / (mean_size + 1e-12)\n    size_pen = np.clip(size_pen, 0, 1)\n    scores = inter_counts - 0.6 * size_pen\n\n    # softmin of negative scores\n    neg_scores = -scores\n    exp_vals = np.exp(neg_scores - np.max(neg_scores))\n    probs = exp_vals / (np.sum(exp_vals) + 1e-12)\n    probs[~valid] = 0\n    probs = probs / (np.sum(probs) + 1e-12)\n\n    rng = np.random.default_rng()\n    idx = rng.choice(len(remaining_subsets), p=probs)\n    return remaining_subsets[idx]\n\n",
  "mean_size_balancing_aug_77": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    if not remaining_subsets:\n        return None\n\n    mean_size = float(np.mean([len(s) for s in remaining_subsets]))\n    best = None\n    best_score = -np.inf\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        new = len(rem_set & set(s))\n        if new > 0:\n            size = len(s)\n            size_pen = abs(size - mean_size) / (mean_size + 1e-12)\n            size_pen = np.clip(size_pen, 0, 1)\n            score = new - 0.6 * size_pen\n            # deterministic tie\u2011breaking noise\n            score += idx * 1e-9\n            if score > best_score:\n                best_score = score\n                best = s\n        idx += 1\n    return best\n\n",
  "mean_size_balancing_aug_78": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    if not remaining_subsets:\n        return None\n\n    sizes = np.array([len(s) for s in remaining_subsets], dtype=np.float64)\n    # proxy for mean: use the maximum size as an upper bound\n    mean_size = float(np.max(sizes) + 1e-12)\n    inter_counts = np.array([len(rem_set & set(s)) for s in remaining_subsets], dtype=np.float64)\n\n    valid = inter_counts > 0\n    if not np.any(valid):\n        return None\n\n    size_pen = np.abs(sizes - mean_size) / (mean_size + 1e-12)\n    size_pen = np.clip(size_pen, 0, 1)\n    scores = inter_counts - 2.0 * size_pen\n\n    best_idx = int(np.argmax(scores))\n    return remaining_subsets[best_idx]\n\n",
  "harmonic_rarity_gain_aug_79": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Greedy selector with deterministic tie\u2011breaking and epsilon protection.\"\"\"\n    rem_set = set(remaining_elements)\n\n    # Frequency count of remaining elements\n    freq: Dict[int, int] = {}\n    for subset in remaining_subsets:\n        for elem in set(subset):\n            if elem in rem_set:\n                freq[elem] = freq.get(elem, 0) + 1\n\n    best, best_score = None, -np.inf\n    for idx, subset in enumerate(remaining_subsets):\n        # Score is the sum of 1/(1+ln(freq)) for new elements\n        score = sum(\n            1.0 / (1.0 + np.log(max(1, freq.get(e, 1))) + 1e-12)\n            for e in set(subset) if e in rem_set\n        )\n        # Deterministic noise for stable tie\u2011breaking\n        noise = 1e-6 * (hash(tuple(sorted(subset))) % 1000)\n        score += noise\n        if score > best_score and score > 0:\n            best_score, best = score, subset\n    return best\n\n",
  "harmonic_rarity_gain_aug_80": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Weighted selector with tuned constants and random top\u2011k choice.\"\"\"\n    rem_set = set(remaining_elements)\n\n    # Build frequency dictionary\n    freq: Dict[int, int] = {}\n    for subset in remaining_subsets:\n        for elem in set(subset):\n            if elem in rem_set:\n                freq[elem] = freq.get(elem, 0) + 1\n\n    # Compute weighted scores\n    scores = []\n    for subset in remaining_subsets:\n        sc = sum(\n            0.6 / (0.6 + 0.4 * np.log(max(1, freq.get(e, 1))) + 1e-12)\n            for e in set(subset) if e in rem_set\n        )\n        scores.append(sc)\n    scores = np.array(scores)\n\n    # Randomly pick among the top\u2011k scored subsets\n    top_k = 7\n    top_indices = np.argpartition(-scores, top_k - 1)[:top_k]\n    rng = np.random.default_rng(12345)\n    chosen_index = rng.choice(top_indices)\n    return remaining_subsets[chosen_index]\n\n",
  "harmonic_rarity_gain_aug_81": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Soft\u2011min based selector using mean aggregation and proxy log.\"\"\"\n    rem_set = set(remaining_elements)\n\n    # Frequency count\n    freq: Dict[int, int] = {}\n    for subset in remaining_subsets:\n        for elem in set(subset):\n            if elem in rem_set:\n                freq[elem] = freq.get(elem, 0) + 1\n\n    best, best_score = None, -np.inf\n    for subset in remaining_subsets:\n        elems = [e for e in set(subset) if e in rem_set]\n        if not elems:\n            continue\n        sc = np.mean(\n            1.0 / (1.0 + np.log1p(max(1, freq.get(e, 1))) + 1e-12)\n            for e in elems\n        )\n        # Soft\u2011min: lower mean gives higher score\n        score = -sc\n        # Deterministic noise to break ties\n        noise = (hash(tuple(sorted(subset))) % 1000) * 1e-7\n        score -= noise\n        if score > best_score:\n            best_score, best = score, subset\n    return best\n\n",
  "harmonic_rarity_gain_aug_82": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Inverted conditional logic with while loop and clipping.\"\"\"\n    rem_set = set(remaining_elements)\n\n    # Build frequency dictionary\n    freq: Dict[int, int] = {}\n    for subset in remaining_subsets:\n        for elem in set(subset):\n            if elem in rem_set:\n                freq[elem] = freq.get(elem, 0) + 1\n\n    best, best_score = None, -np.inf\n    idx = 0\n    while idx < len(remaining_subsets):\n        subset = remaining_subsets[idx]\n        score = 0.0\n        for elem in set(subset):\n            if elem in rem_set:\n                f = freq.get(elem, 1)\n                score += 1.0 / (1.0 + np.log(max(1, f)) + 1e-12)\n        # Invert sign to use a simple max comparison\n        neg_score = -score\n        if neg_score > best_score:\n            best_score, best = neg_score, subset\n        idx += 1\n\n    # Clip the final score to avoid any accidental overflow\n    best_score = np.clip(best_score, -np.inf, 0)\n    return best\n\n",
  "avoid_unique_leftovers_aug_83": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorised frequency counting and deterministic noise for tie\u2011breaking.\"\"\"\n    # Map each element to an index for fast array access\n    rem = set(remaining_elements)\n    elem_to_idx = {e: i for i, e in enumerate(sorted(rem))}\n    idx_array = np.fromiter((elem_to_idx[e] for e in rem), dtype=int)\n    freq_arr = np.zeros(len(rem), dtype=int)\n\n    # Build a 2\u2011D array of indices for each subset\n    subset_indices = []\n    for s in remaining_subsets:\n        indices = [elem_to_idx[e] for e in s if e in rem]\n        if indices:\n            subset_indices.append(indices)\n\n    # Accumulate frequencies\n    for indices in subset_indices:\n        freq_arr[indices] += 1\n\n    best = None\n    best_tuple = None  # (unique_covered, total_covered, -size, noise)\n    noise_eps = 1e-12\n    for s, indices in zip(remaining_subsets, subset_indices):\n        if not indices:\n            continue\n        unique_cov = int(np.sum(freq_arr[indices] == 1))\n        t = (unique_cov, len(indices), -len(s))\n        # deterministic noise to break ties\n        noise = np.random.uniform(-1e-6, 1e-6)\n        t_noise = t + (noise,)\n        if best_tuple is None or t_noise > best_tuple:\n            best_tuple = t_noise\n            best = s\n    return best\n\n",
  "avoid_unique_leftovers_aug_84": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Weighted scoring with random tie\u2011breaking among top\u2011k subsets.\"\"\"\n    rem = set(remaining_elements)\n    freq: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in freq:\n                freq[e] += 1\n\n    scores = []\n    for s in remaining_subsets:\n        new = [e for e in set(s) if e in rem]\n        if not new:\n            continue\n        unique_cov = sum(1 for e in new if freq.get(e, 0) == 1)\n        total_cov = len(new)\n        # Weighted score: 60% unique, 40% total\n        score = 0.6 * unique_cov + 0.4 * total_cov\n        scores.append((score, s))\n\n    if not scores:\n        return None\n\n    # Sort by score descending\n    scores.sort(key=lambda x: x[0], reverse=True)\n    top_k = min(3, len(scores))\n    top_choices = [s for _, s in scores[:top_k]]\n    # Randomly pick one among top\u2011k\n    return np.random.choice(top_choices, replace=False)\n\n",
  "avoid_unique_leftovers_aug_85": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Soft\u2011min selection with while loop and inverted condition.\"\"\"\n    rem = set(remaining_elements)\n    freq: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in freq:\n                freq[e] += 1\n\n    best = None\n    best_score = np.inf\n    idx = 0\n    epsilon = 1e-12\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        new = [e for e in set(s) if e in rem]\n        if new:  # invert logic: only process when there is coverage\n            unique_cov = sum(1 for e in new if freq.get(e, 0) == 1)\n            # Soft\u2011min score: lower is better\n            score = 1.0 / (unique_cov + len(new) + epsilon)\n            if score < best_score:\n                best_score = score\n                best = s\n        idx += 1\n    return best\n\n",
  "avoid_unique_leftovers_aug_86": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Lexicographic deterministic tie\u2011breaking and sum aggregation.\"\"\"\n    rem = set(remaining_elements)\n    freq: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in freq:\n                freq[e] += 1\n\n    best = None\n    best_tuple = None  # (unique, total, -size, lexicographic key)\n    for s in remaining_subsets:\n        new = [e for e in set(s) if e in rem]\n        if not new:\n            continue\n        unique_cov = sum(1 for e in new if freq.get(e, 0) == 1)\n        t = (unique_cov, len(new), -len(s))\n        # Lexicographic key: tuple of sorted elements\n        lex_key = tuple(sorted(s))\n        t_full = t + (lex_key,)\n        if best_tuple is None or t_full > best_tuple:\n            best_tuple = t_full\n            best = s\n    return best\n\n",
  "redundancy_elimination_probe_aug_87": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    chosen_sets = [set(s) for s in selected_subsets]\n    uncovered_elems = set(remaining_elements)\n    candidate_sets = remaining_subsets\n\n    best_set = None\n    best_score = -np.inf\n\n    for cand in candidate_sets:\n        cand_set = set(cand)\n        new_cov = len(uncovered_elems & cand_set)\n        if new_cov == 0:\n            continue\n\n        # Count how many chosen sets become redundant\n        red = sum(1 for t in chosen_sets if len(t) > 0 and t.issubset(cand_set))\n        score = new_cov + 1.5 * red\n\n        # Clip to avoid extreme values\n        score = np.clip(score, -1e6, 1e6)\n\n        if score > best_score:\n            best_score = score\n            best_set = cand\n\n    return best_set\n\n",
  "redundancy_elimination_probe_aug_88": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    uncovered = np.array(remaining_elements)\n    chosen = [set(s) for s in selected_subsets]\n    candidates = remaining_subsets\n\n    best_set = None\n    best_score = -np.inf\n\n    idx = 0\n    while idx < len(candidates):\n        cand = candidates[idx]\n        cand_set = set(cand)\n        new_cov = len(np.intersect1d(uncovered, np.array(list(cand_set)), assume_unique=True))\n        if new_cov > 0:\n            # Redundancy weighted differently\n            red = sum(1 for t in chosen if len(t) > 0 and t.issubset(cand_set))\n            score = new_cov * 1.2 + 0.8 * red\n\n            # Clip to a safe range\n            score = np.clip(score, -1e6, 1e6)\n\n            if score > best_score:\n                best_score = score\n                best_set = cand\n        idx += 1\n\n    return best_set\n\n",
  "redundancy_elimination_probe_aug_89": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    chosen = [set(s) for s in selected_subsets]\n    uncovered = set(remaining_elements)\n\n    scores = []\n    for cand in remaining_subsets:\n        cand_set = set(cand)\n        new_cov = len(uncovered & cand_set)\n        if new_cov == 0:\n            continue\n\n        red = sum(1 for t in chosen if len(t) > 0 and t.issubset(cand_set))\n        # Deterministic noise based on hash for tie\u2011breaking\n        noise = (hash(frozenset(cand)) % 1000) * 1e-6\n        score = new_cov + 1.5 * red + noise\n        scores.append((score, cand))\n\n    if not scores:\n        return None\n\n    # Pick top\u2011k candidates and choose deterministically\n    top_k = 3\n    scores.sort(key=lambda x: x[0], reverse=True)\n    top_candidates = [c for _, c in scores[:top_k]]\n\n    rng = np.random.default_rng(seed=42)\n    chosen_idx = rng.choice(len(top_candidates))\n    return top_candidates[chosen_idx]\n\n",
  "redundancy_elimination_probe_aug_90": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    chosen = [set(s) for s in selected_subsets]\n    uncovered = set(remaining_elements)\n\n    best_set = None\n    best_score = -np.inf\n\n    for cand in remaining_subsets:\n        cand_set = set(cand)\n        new_cov = len(uncovered & cand_set)\n        if new_cov == 0:\n            continue\n\n        red = sum(1 for t in chosen if len(t) > 0 and t.issubset(cand_set))\n        # Aggregate using median of new_cov and red\n        agg = np.median([new_cov, red])\n        score = agg * 2.0\n\n        # Clip to keep values bounded\n        score = np.clip(score, -1e6, 1e6)\n\n        if score > best_score:\n            best_score = score\n            best_set = cand\n\n    return best_set\n\n",
  "redundancy_elimination_probe_aug_91": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    chosen = [set(s) for s in selected_subsets]\n    uncovered = set(remaining_elements)\n\n    scores = []\n    for cand in remaining_subsets:\n        cand_set = set(cand)\n        new_cov = len(uncovered & cand_set)\n        if new_cov == 0:\n            continue\n\n        red = sum(1 for t in chosen if len(t) > 0 and t.issubset(cand_set))\n        score = new_cov + 1.5 * red\n        scores.append((score, cand))\n\n    if not scores:\n        return None\n\n    score_vals = np.array([s for s, _ in scores])\n    eps = 1e-12\n    # Softmin via negative softmax\n    exp_vals = np.exp(-score_vals / (1.0 + eps))\n    probs = exp_vals / (np.sum(exp_vals) + eps)\n\n    rng = np.random.default_rng(seed=123)\n    idx = rng.choice(len(scores), p=probs)\n    return scores[idx][1]\n\n",
  "low_overlap_with_remaining_aug_92": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Variation 1: Uses while loops and vectorised frequency calculation.\n    \"\"\"\n    rem_elems = set(remaining_elements)\n    cand_subsets = remaining_subsets\n\n    # Build frequency dictionary\n    freq: Dict[int, int] = {}\n    for s in cand_subsets:\n        for e in s:\n            if e in rem_elems:\n                freq[e] = freq.get(e, 0) + 1\n\n    best_subset = None\n    best_score = -np.inf\n    idx = 0\n    while idx < len(cand_subsets):\n        s = cand_subsets[idx]\n        s_set = set(s)\n        new_elems = [e for e in s_set if e in rem_elems]\n        if new_elems:\n            gain = len(new_elems)\n            overlap_pen = sum((freq[e] - 1) for e in new_elems)\n            score = gain - 0.15 * overlap_pen\n            # Clip to avoid extreme values\n            score = np.clip(score, -1e10, 1e10)\n            if score > best_score:\n                best_score = score\n                best_subset = s\n        idx += 1\n    return best_subset\n\n",
  "low_overlap_with_remaining_aug_93": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Variation 2: Parameter tuning with top_k random selection.\n    \"\"\"\n    rem = set(remaining_elements)\n    cand = remaining_subsets\n\n    # Frequency dictionary\n    freq: Dict[int, int] = {}\n    for s in cand:\n        for e in s:\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    scores = []\n    for s in cand:\n        s_set = set(s)\n        new_elems = [e for e in s_set if e in rem]\n        if new_elems:\n            gain = len(new_elems)\n            overlap_pen = sum((freq[e] - 1) for e in new_elems)\n            score = gain - 0.20 * overlap_pen   # tuned weight\n            scores.append((score, s))\n    if not scores:\n        return None\n\n    # Sort by score descending\n    scores.sort(key=lambda x: x[0], reverse=True)\n    top_k = 5\n    top_candidates = [s for _, s in scores[:top_k]]\n    # Randomly pick among top_k\n    chosen = np.random.choice(top_candidates)\n    return chosen\n\n",
  "low_overlap_with_remaining_aug_94": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Variation 3: Softmax-based selection with deterministic noise for tie-breaking.\n    \"\"\"\n    rem = set(remaining_elements)\n    cand = remaining_subsets\n\n    # Frequency dictionary\n    freq: Dict[int, int] = {}\n    for s in cand:\n        for e in s:\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    scores = []\n    for s in cand:\n        s_set = set(s)\n        new_elems = [e for e in s_set if e in rem]\n        if new_elems:\n            gain = len(new_elems)\n            overlap_pen = sum((freq[e] - 1) for e in new_elems)\n            score = gain - 0.15 * overlap_pen\n            scores.append(score)\n    if not scores:\n        return None\n\n    scores = np.array(scores, dtype=np.float64)\n    # Deterministic noise for tie\u2011breaking\n    noise = 1e-6 * np.arange(len(scores))\n    scores_noisy = scores + noise\n\n    # Softmax with numerical stability\n    max_val = np.max(scores_noisy)\n    exp_vals = np.exp(scores_noisy - max_val)\n    denom = np.sum(exp_vals) + 1e-12  # epsilon to avoid division by zero\n    probs = exp_vals / denom\n\n    idx = np.random.choice(len(cand), p=probs)\n    return cand[idx]\n\n",
  "low_overlap_with_remaining_aug_95": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Variation 4: Frequency via np.bincount and max overlap penalty.\n    \"\"\"\n    rem = set(remaining_elements)\n    cand = remaining_subsets\n\n    # Flatten all relevant elements for bincount\n    flat_elems = [e for s in cand for e in s if e in rem]\n    if not flat_elems:\n        return None\n    max_val = max(flat_elems)\n    freq_arr = np.bincount(np.array(flat_elems, dtype=np.int64), minlength=max_val + 1)\n    freq_arr = np.clip(freq_arr, 0, 1e6)  # avoid extreme values\n\n    best_subset = None\n    best_score = -np.inf\n    for s in cand:\n        s_set = set(s)\n        new_elems = [e for e in s_set if e in rem]\n        if not new_elems:\n            continue\n        gain = len(new_elems)\n        overlap_pen = np.max([freq_arr[e] - 1 for e in new_elems])\n        score = gain - 0.15 * overlap_pen\n        score = np.clip(score, -1e10, 1e10)\n        if score > best_score:\n            best_score = score\n            best_subset = s\n    return best_subset\n\n",
  "low_overlap_with_remaining_aug_96": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Variation 5: Median\u2011based gain adjustment and max overlap penalty.\n    \"\"\"\n    rem = set(remaining_elements)\n    cand = remaining_subsets\n\n    # Frequency dictionary\n    freq: Dict[int, int] = {}\n    for s in cand:\n        for e in s:\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    # Median of all gains\n    gains = []\n    for s in cand:\n        s_set = set(s)\n        new_elems = [e for e in s_set if e in rem]\n        if new_elems:\n            gains.append(len(new_elems))\n    if not gains:\n        return None\n    median_gain = np.median(gains)\n\n    best_subset = None\n    best_score = -np.inf\n    for s in cand:\n        s_set = set(s)\n        new_elems = [e for e in s_set if e in rem]\n        if not new_elems:\n            continue\n        gain = len(new_elems)\n        adjusted_gain = gain - median_gain  # promote diversity\n        overlap_pen = np.max([freq[e] - 1 for e in new_elems])\n        score = adjusted_gain - 0.15 * overlap_pen\n        score = np.clip(score, -1e10, 1e10)\n        if score > best_score:\n            best_score = score\n            best_subset = s\n    return best_subset\n\n",
  "conflict_then_rarity_switch_aug_97": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 1: vectorised frequency, high\u2011degree first, tie\u2011break with noise.\"\"\"\n    # rename variables for clarity\n    cover_set: set = set(remaining_elements)\n    freq: Dict[int, int] = {}\n    # build frequency table\n    idx = 0\n    while idx < len(remaining_subsets):\n        sub = remaining_subsets[idx]\n        for el in set(sub):\n            if el in cover_set:\n                freq[el] = freq.get(el, 0) + 1\n        idx += 1\n\n    n_rem = max(1, len(cover_set))\n    n_total = n_rem + len({e for ss in selected_subsets for e in ss})\n    progress = 1.0 - (n_rem / max(1, n_total))\n    progress = np.clip(progress, 0.0, 1.0)\n\n    best_subset: Optional[List[int]] = None\n    best_score = -np.inf\n\n    idx = 0\n    while idx < len(remaining_subsets):\n        sub = remaining_subsets[idx]\n        new_el = [e for e in set(sub) if e in cover_set]\n        if new_el:\n            if progress < 0.5:\n                # high\u2011degree focus\n                sc = np.sum([freq.get(e, 1) for e in new_el])\n            else:\n                # rarity focus\n                sc = np.sum([1.0 / (freq.get(e, 1) + 1e-12) for e in new_el])\n            # deterministic noise for tie\u2011breaking\n            sc += 1e-6 * idx\n            if sc > best_score:\n                best_score, best_subset = sc, sub\n        idx += 1\n    return best_subset\n\n",
  "conflict_then_rarity_switch_aug_98": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 2: weighted score, top\u2011k random choice, deterministic tie\u2011break.\"\"\"\n    cover_set = set(remaining_elements)\n    freq: Dict[int, int] = {}\n    for sub in remaining_subsets:\n        for el in set(sub):\n            if el in cover_set:\n                freq[el] = freq.get(el, 0) + 1\n\n    n_rem = max(1, len(cover_set))\n    n_total = n_rem + len({e for ss in selected_subsets for e in ss})\n    progress = 1.0 - (n_rem / max(1, n_total))\n    progress = np.clip(progress, 0.0, 1.0)\n\n    scores = []\n    for sub in remaining_subsets:\n        new_el = [e for e in set(sub) if e in cover_set]\n        if not new_el:\n            continue\n        if progress < 0.7:  # tuned threshold\n            sc = np.sum([freq.get(e, 1) ** 0.6 for e in new_el])  # weighted high\u2011degree\n        else:\n            sc = np.sum([1.0 / ((freq.get(e, 1) + 1e-12) ** 0.4) for e in new_el])  # weighted rarity\n        sc = np.clip(sc, -1e12, 1e12)  # bound to avoid overflow\n        scores.append((sub, sc))\n\n    if not scores:\n        return None\n\n    # pick top\u2011k\n    k = min(5, len(scores))\n    top_k = sorted(scores, key=lambda x: x[1], reverse=True)[:k]\n    # random choice among top\u2011k\n    idx = np.random.randint(0, k)\n    return top_k[idx][0]\n\n",
  "conflict_then_rarity_switch_aug_99": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 3: median\u2011based score, softmin selection, deterministic tie\u2011break.\"\"\"\n    cover_set = set(remaining_elements)\n    freq: Dict[int, int] = {}\n    for sub in remaining_subsets:\n        for el in set(sub):\n            if el in cover_set:\n                freq[el] = freq.get(el, 0) + 1\n\n    n_rem = max(1, len(cover_set))\n    n_total = n_rem + len({e for ss in selected_subsets for e in ss})\n    progress = 1.0 - (n_rem / max(1, n_total))\n    progress = np.clip(progress, 0.0, 1.0)\n\n    subs_scores = []\n    for sub in remaining_subsets:\n        new_el = [e for e in set(sub) if e in cover_set]\n        if not new_el:\n            continue\n        if progress < 0.5:\n            vals = np.array([freq.get(e, 1) for e in new_el], dtype=float)\n            sc = np.median(vals)\n        else:\n            vals = np.array([1.0 / (freq.get(e, 1) + 1e-12) for e in new_el], dtype=float)\n            sc = np.median(vals)\n        subs_scores.append((sub, sc))\n\n    if not subs_scores:\n        return None\n\n    # softmin: convert scores to probabilities inversely proportional\n    scores_arr = np.array([s for _, s in subs_scores], dtype=float)\n    probs = np.exp(-scores_arr)  # larger score -> smaller probability\n    probs = probs / np.clip(np.sum(probs), 1e-12, None)\n\n    idx = np.random.choice(len(subs_scores), p=probs)\n    return subs_scores[idx][0]\n\n",
  "conflict_then_rarity_switch_aug_100": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 4: while loops, np.square, inverted progress logic.\"\"\"\n    cover_set = set(remaining_elements)\n    freq: Dict[int, int] = {}\n    i = 0\n    while i < len(remaining_subsets):\n        sub = remaining_subsets[i]\n        for el in set(sub):\n            if el in cover_set:\n                freq[el] = freq.get(el, 0) + 1\n        i += 1\n\n    n_rem = max(1, len(cover_set))\n    n_total = n_rem + len({e for ss in selected_subsets for e in ss})\n    progress = 1.0 - (n_rem / max(1, n_total))\n    progress = np.clip(progress, 0.0, 1.0)\n\n    best_subset: Optional[List[int]] = None\n    best_score = -np.inf\n    idx = 0\n    while idx < len(remaining_subsets):\n        sub = remaining_subsets[idx]\n        new_el = [e for e in set(sub) if e in cover_set]\n        if new_el:\n            # inverted logic: use rarity early, high\u2011degree later\n            if progress < 0.5:\n                sc = np.sum([1.0 / (freq.get(e, 1) + 1e-12) for e in new_el])\n            else:\n                sc = np.sum([freq.get(e, 1) ** 2 for e in new_el])  # emphasize with square\n            sc = np.clip(sc, -1e12, 1e12)\n            if sc > best_score:\n                best_score, best_subset = sc, sub\n        idx += 1\n    return best_subset\n\n",
  "frequency_spread_bonus_aug_101": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 1: uses while loops, mean\u2011based spread, heavier weight, and deterministic noise.\"\"\"\n    rem_set = set(remaining_elements)\n\n    # Build frequency dictionary with a while loop\n    freq: Dict[int, int] = {}\n    idx = 0\n    while idx < len(remaining_subsets):\n        subset = remaining_subsets[idx]\n        for e in set(subset):\n            if e in rem_set:\n                freq[e] = freq.get(e, 0) + 1\n        idx += 1\n\n    best_subset = None\n    best_score = -np.inf\n    idx = 0\n    while idx < len(remaining_subsets):\n        subset = remaining_subsets[idx]\n        new_elems = [e for e in set(subset) if e in rem_set]\n        if new_elems:\n            fvals = np.array([freq.get(e, 1) for e in new_elems], dtype=float)\n            # crude spread: difference between mean and min\n            mean_spread = float(np.mean(fvals) - np.min(fvals))\n            mean_spread = np.clip(mean_spread, 0, 10)\n            score = len(new_elems) + 0.6 * mean_spread + 1e-6 * idx\n            if score > best_score:\n                best_score = score\n                best_subset = subset\n        idx += 1\n    return best_subset\n\n",
  "frequency_spread_bonus_aug_102": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 2: weighted spread with random top\u2011k tie\u2011breaking.\"\"\"\n    rem_set = set(remaining_elements)\n\n    # Frequency count\n    freq: Dict[int, int] = {e: 0 for e in rem_set}\n    for subset in remaining_subsets:\n        for e in set(subset):\n            if e in rem_set:\n                freq[e] += 1\n\n    scores = []\n    for subset in remaining_subsets:\n        new_elems = [e for e in set(subset) if e in rem_set]\n        if not new_elems:\n            continue\n        fvals = np.array([freq.get(e, 1) for e in new_elems], dtype=float)\n        spread = float(np.max(fvals) - np.min(fvals))\n        spread = np.clip(spread, 0, 5)\n        score = len(new_elems) + 0.3 * spread\n        scores.append((score, subset))\n\n    if not scores:\n        return None\n\n    # Pick the top\u2011k subsets (k=3) and randomly choose among the best\n    k = 3\n    top_k = sorted(scores, key=lambda x: x[0], reverse=True)[:k]\n    best_score = top_k[0][0]\n    candidates = [s for sc, s in top_k if abs(sc - best_score) < 1e-12]\n    return np.random.choice(candidates)\n\n",
  "frequency_spread_bonus_aug_103": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 3: median spread, soft\u2011inverse weighting, deterministic tie\u2011breaking.\"\"\"\n    rem = set(remaining_elements)\n\n    # Frequency dictionary\n    freq: Dict[int, int] = {}\n    for subset in remaining_subsets:\n        for e in set(subset):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    scores = []\n    for subset in remaining_subsets:\n        new = [e for e in set(subset) if e in rem]\n        if new:\n            fvals = np.array([freq.get(e, 1) for e in new], dtype=float)\n            med = np.median(fvals)\n            spread = float(np.max(fvals) - med)\n            spread = np.clip(spread, 0, 8)\n            inv = 1.0 / (spread + 1e-12)          # soft\u2011inverse to avoid division by zero\n            score = len(new) + 0.25 * inv\n            scores.append((score, subset))\n\n    if not scores:\n        return None\n\n    best_score = max(scores, key=lambda x: x[0])[0]\n    best_subsets = [s for sc, s in scores if abs(sc - best_score) < 1e-9]\n\n    if len(best_subsets) == 1:\n        return best_subsets[0]\n\n    # Deterministic tie\u2011breaker: smallest sum of element IDs\n    return min(best_subsets, key=lambda s: np.sum(s))\n\n",
  "frequency_spread_bonus_aug_104": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 4: vectorized frequency count, L2\u2011norm spread, weighted score.\"\"\"\n    rem = set(remaining_elements)\n\n    # Vectorized frequency calculation\n    if remaining_subsets:\n        all_elements = np.concatenate([np.array(sub, dtype=int) for sub in remaining_subsets])\n    else:\n        all_elements = np.array([], dtype=int)\n\n    mask = np.isin(all_elements, list(rem))\n    if all_elements.size > 0:\n        freq_arr = np.bincount(all_elements[mask], minlength=max(rem) + 1)\n        freq = {e: freq_arr[e] for e in rem}\n    else:\n        freq = {}\n\n    best = None\n    best_score = -np.inf\n    for subset in remaining_subsets:\n        new = [e for e in set(subset) if e in rem]\n        if not new:\n            continue\n        fvals = np.array([freq.get(e, 1) for e in new], dtype=float)\n        mean = np.mean(fvals)\n        spread = np.linalg.norm(fvals - mean)          # Euclidean spread\n        spread = np.clip(spread, 0, 12)\n        score = len(new) + 0.5 * spread\n        if score > best_score:\n            best_score = score\n            best = subset\n    return best\n\n",
  "rare_pair_bonus_aug_105": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selection with deterministic noise for tie\u2011breaking.\n    \"\"\"\n    unvisited = set(remaining_elements)\n    freq_map: Dict[int, int] = {e: 0 for e in unvisited}\n\n    # Count how many remaining subsets contain each uncovered element\n    for subset in remaining_subsets:\n        for elem in set(subset):\n            if elem in unvisited:\n                freq_map[elem] += 1\n\n    best_subset: Optional[List[int]] = None\n    best_score = -np.inf\n\n    for subset in remaining_subsets:\n        elems = [e for e in set(subset) if e in unvisited]\n        if not elems:\n            continue\n\n        base = len(elems)\n\n        # pick the fewest\u2011covered elements to compute a cheap pair bonus\n        rare_sorted = sorted(elems, key=lambda e: freq_map.get(e, 1))[:12]\n        bonus = 0.0\n        for i in range(len(rare_sorted)):\n            for j in range(i + 1, len(rare_sorted)):\n                a, b = rare_sorted[i], rare_sorted[j]\n                denom = (freq_map.get(a, 1) + 1e-12) * (freq_map.get(b, 1) + 1e-12)\n                bonus += 1.0 / denom\n\n        score = base + 3.0 * bonus\n\n        # deterministic tie\u2011breaker: small noise derived from the subset hash\n        noise = 1e-6 * (hash(tuple(subset)) % 1000)\n        score += noise\n\n        # clip to avoid extreme values\n        score = np.clip(score, -1e12, 1e12)\n\n        if score > best_score:\n            best_score = score\n            best_subset = subset\n\n    return best_subset\n\n",
  "rare_pair_bonus_aug_106": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Uses a weighted score and random choice among the top\u2011k candidates.\n    \"\"\"\n    unvisited = set(remaining_elements)\n    freq_map: Dict[int, int] = {}\n\n    for subset in remaining_subsets:\n        for elem in set(subset):\n            if elem in unvisited:\n                freq_map[elem] = freq_map.get(elem, 0) + 1\n\n    scores = []\n    for subset in remaining_subsets:\n        elems = [e for e in set(subset) if e in unvisited]\n        if not elems:\n            scores.append(-np.inf)\n            continue\n\n        base = len(elems)\n\n        rare_sorted = sorted(elems, key=lambda e: freq_map.get(e, 1))[:12]\n        bonus = 0.0\n        for i in range(len(rare_sorted)):\n            for j in range(i + 1, len(rare_sorted)):\n                a, b = rare_sorted[i], rare_sorted[j]\n                denom = (freq_map.get(a, 1) + 1e-12) * (freq_map.get(b, 1) + 1e-12)\n                bonus += 1.0 / denom\n\n        # weighted combination (tuned constants)\n        score = 0.5 * base + 2.0 * bonus\n        scores.append(score)\n\n    scores_np = np.array(scores, dtype=np.float64)\n\n    if np.all(np.isneginf(scores_np)):\n        return None\n\n    # clip to keep numbers stable\n    scores_np = np.clip(scores_np, -1e9, 1e9)\n\n    # select the top\u2011k candidates\n    k = 7\n    top_k_idx = np.argpartition(-scores_np, k - 1)[:k]\n\n    # random choice among the top\u2011k (ties resolved randomly)\n    chosen_idx = np.random.choice(top_k_idx)\n    return remaining_subsets[chosen_idx]\n\n",
  "rare_pair_bonus_aug_107": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Vectorised pair bonus calculation using numpy outer product.\n    \"\"\"\n    unvisited = set(remaining_elements)\n    freq_map: Dict[int, int] = {}\n\n    for subset in remaining_subsets:\n        for elem in set(subset):\n            if elem in unvisited:\n                freq_map[elem] = freq_map.get(elem, 0) + 1\n\n    best_subset: Optional[List[int]] = None\n    best_score = -np.inf\n\n    for subset in remaining_subsets:\n        elems = [e for e in set(subset) if e in unvisited]\n        if not elems:\n            continue\n\n        base = len(elems)\n\n        # vectorised pair bonus\n        rare_sorted = sorted(elems, key=lambda e: freq_map.get(e, 1))[:12]\n        arr = np.array([freq_map.get(e, 1) + 1e-12 for e in rare_sorted], dtype=np.float64)\n\n        if arr.size < 2:\n            bonus = 0.0\n        else:\n            prod = np.outer(arr, arr)          # pairwise product matrix\n            tri = np.triu(prod, k=1)           # upper triangle without diagonal\n            bonus = np.sum(1.0 / tri)\n\n        score = base + 4.0 * bonus\n        score = np.clip(score, -1e12, 1e12)\n\n        if score > best_score:\n            best_score = score\n            best_subset = subset\n\n    return best_subset\n\n",
  "maximize_remaining_support_aug_108": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selection with deterministic noise for tie\u2011breaking.\n    Uses list comprehensions and inverted conditionals.\n    \"\"\"\n    # Convert to sets for efficient set operations\n    rem_set: Set[int] = set(remaining_elements)\n    sets = [set(sub) for sub in remaining_subsets]\n\n    # Compute support counts for each remaining element\n    support: Dict[int, int] = {e: 0 for e in rem_set}\n    for s in sets:\n        for e in s & rem_set:\n            support[e] += 1\n\n    best: Optional[List[int]] = None\n    best_score: float = -np.inf\n\n    # Iterate with inverted condition (gain > 0)\n    for idx, s in enumerate(sets):\n        gain = len(rem_set & s)\n        if gain <= 0:\n            continue\n\n        rem1 = rem_set - s\n        if not rem1:                      # covers all remaining elements\n            return remaining_subsets[idx]\n\n        # Count fragile elements (support == 1) after removal\n        fragile = sum(1 for e in rem1 if support.get(e, 0) == 1)\n\n        # Scoring: weight 0.75 for both gain and fragile,\n        # add tiny deterministic noise to break ties\n        score = 0.75 * gain - 0.75 * fragile + 1e-6 * idx\n\n        if score > best_score:\n            best_score = score\n            best = remaining_subsets[idx]\n\n    return best\n\n",
  "maximize_remaining_support_aug_109": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Vectorised implementation using a binary incidence matrix.\n    Control flow is expressed with a while loop.\n    \"\"\"\n    n_subsets = len(remaining_subsets)\n    n_elem = len(remaining_elements)\n\n    # Map elements to column indices\n    elem_to_idx = {e: i for i, e in enumerate(remaining_elements)}\n\n    # Build binary incidence matrix (n_subsets x n_elem)\n    matrix = np.zeros((n_subsets, n_elem), dtype=int)\n    for i, subset in enumerate(remaining_subsets):\n        for e in subset:\n            idx = elem_to_idx.get(e)\n            if idx is not None:\n                matrix[i, idx] = 1\n\n    # Support counts for each element\n    support = matrix.sum(axis=0)          # shape (n_elem,)\n\n    best: Optional[List[int]] = None\n    best_score: float = -np.inf\n    i = 0\n    while i < n_subsets:\n        gain = matrix[i].sum()\n        if gain > 0:\n            rem1_mask = matrix[i] == 0\n            if not rem1_mask.any():\n                return remaining_subsets[i]      # covers all\n\n            fragile = int(np.sum((support == 1) & rem1_mask))\n            score = 0.6 * gain - 0.4 * fragile\n            score = np.clip(score, -1e6, 1e6)     # bound to avoid extremes\n\n            if score > best_score:\n                best_score = score\n                best = remaining_subsets[i]\n        i += 1\n\n    return best\n\n",
  "maximize_remaining_support_aug_110": "import numpy as np\nimport random\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Randomly selects among the top\u2011k (k=3) candidates.\n    Adds deterministic noise for stable tie\u2011breaking.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    sets = [set(sub) for sub in remaining_subsets]\n\n    support = {e: 0 for e in rem_set}\n    for s in sets:\n        for e in s & rem_set:\n            support[e] += 1\n\n    scores = []\n    for idx, s in enumerate(sets):\n        gain = len(rem_set & s)\n        if gain <= 0:\n            scores.append(-np.inf)          # ignore\n            continue\n\n        rem1 = rem_set - s\n        if not rem1:\n            return remaining_subsets[idx]\n\n        fragile = sum(1 for e in rem1 if support.get(e, 0) == 1)\n        score = 0.75 * gain - 0.75 * fragile + 1e-6 * idx\n        scores.append(score)\n\n    scores = np.array(scores)\n    if np.all(np.isneginf(scores)):\n        return None\n\n    top_k = min(3, len(scores))\n    top_indices = np.argpartition(-scores, top_k - 1)[:top_k]\n    chosen_idx = random.choice(top_indices.tolist())\n    return remaining_subsets[chosen_idx]\n\n",
  "maximize_remaining_support_aug_111": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Uses softmax (scaled by 10) to compute selection probabilities.\n    Adds tiny deterministic noise for deterministic behaviour.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    sets = [set(sub) for sub in remaining_subsets]\n\n    support = {e: 0 for e in rem_set}\n    for s in sets:\n        for e in s & rem_set:\n            support[e] += 1\n\n    scores = []\n    for idx, s in enumerate(sets):\n        gain = len(rem_set & s)\n        if gain <= 0:\n            scores.append(-np.inf)\n            continue\n\n        rem1 = rem_set - s\n        if not rem1:\n            return remaining_subsets[idx]\n\n        fragile = sum(1 for e in rem1 if support.get(e, 0) == 1)\n        score = 0.75 * gain - 0.75 * fragile + 1e-6 * idx\n        scores.append(score)\n\n    scores = np.array(scores)\n    if np.all(np.isneginf(scores)):\n        return None\n\n    # Softmax with scaling to sharpen distribution\n    scaled = 10.0 * scores\n    max_scaled = np.max(scaled)\n    exp_scores = np.exp(scaled - max_scaled)\n    denom = exp_scores.sum() + 1e-12          # epsilon to avoid division by zero\n    prob = exp_scores / denom\n    prob = np.clip(prob, 1e-12, 1.0 - 1e-12)  # bound probabilities\n\n    chosen_idx = int(np.argmax(prob))\n    return remaining_subsets[chosen_idx]\n\n",
  "adaptive_density_by_overlap_aug_112": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 1: uses a while loop, vectorized sampling, and epsilon safeguards.\"\"\"\n    # local aliases\n    sel_subs = selected_subsets\n    rem_subs = remaining_subsets\n    rem_elems = set(remaining_elements)\n\n    if not rem_subs:\n        return None\n\n    # sample a subset of remaining sets to estimate overlap\n    m = len(rem_subs)\n    sample_size = min(m, 25)\n    indices = np.random.choice(m, size=sample_size, replace=False)\n    samples = [set(rem_subs[i]) for i in indices]\n\n    # compute pairwise Jaccard similarities\n    jac_vals = []\n    for i in range(sample_size):\n        for j in range(i + 1, sample_size):\n            inter = len(samples[i] & samples[j])\n            uni = len(samples[i] | samples[j])\n            jac_vals.append(inter / (uni + 1e-12))  # avoid div by zero\n\n    overlap = np.mean(jac_vals) if jac_vals else 0.0\n    alpha = np.clip(0.6 + 1.6 * overlap, 0.5, 2.0)\n\n    best = None\n    best_score = -np.inf\n    idx = 0\n    while idx < len(rem_subs):\n        s = rem_subs[idx]\n        new = len(rem_elems & set(s))\n        if new > 0:\n            denom = (len(s) ** alpha) + 1e-12\n            score = new / denom\n            if score > best_score:\n                best_score = score\n                best = s\n        idx += 1\n\n    return best\n\n",
  "adaptive_density_by_overlap_aug_113": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 2: median overlap, tuned alpha, deterministic noise, top\u20113 random choice.\"\"\"\n    rem_elems = set(remaining_elements)\n\n    if not remaining_subsets:\n        return None\n\n    # estimate overlap using median of Jaccard similarities\n    m = len(remaining_subsets)\n    sample_size = min(m, 30)\n    idxs = np.random.choice(m, size=sample_size, replace=False)\n    samples = [set(remaining_subsets[i]) for i in idxs]\n\n    jaccard_vals = []\n    for i in range(sample_size):\n        for j in range(i + 1, sample_size):\n            inter = len(samples[i] & samples[j])\n            uni = len(samples[i] | samples[j])\n            jaccard_vals.append(inter / (uni + 1e-12))\n\n    overlap = np.median(jaccard_vals) if jaccard_vals else 0.0\n    alpha = np.clip(0.5 + 2.0 * overlap, 0.4, 2.5)\n\n    # compute scores with deterministic noise for tie\u2011breaking\n    scores = []\n    for s in remaining_subsets:\n        new = len(rem_elems & set(s))\n        if new <= 0:\n            scores.append(-np.inf)\n            continue\n        denom = (len(s) ** alpha) + 1e-12\n        score = new / denom\n        # deterministic noise\n        noise = (hash(tuple(sorted(s))) % 1000) * 1e-6\n        scores.append(score + noise)\n\n    scores = np.array(scores)\n\n    # select top\u20113 candidates and randomly choose among them\n    top_k = min(3, len(scores))\n    top_indices = np.argpartition(-scores, top_k - 1)[:top_k]\n    chosen_idx = np.random.choice(top_indices)\n    return remaining_subsets[chosen_idx]\n\n",
  "adaptive_density_by_overlap_aug_114": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 3: vectorised intersection counts, softmax selection, epsilon and clipping.\"\"\"\n    rem_elems = np.array(remaining_elements, dtype=int)\n    if not remaining_subsets:\n        return None\n\n    # estimate overlap using average Jaccard via vectorised operations\n    m = len(remaining_subsets)\n    sample_size = min(m, 20)\n    idxs = np.random.choice(m, size=sample_size, replace=False)\n    samples = [set(remaining_subsets[i]) for i in idxs]\n\n    jaccard_vals = []\n    for i in range(sample_size):\n        for j in range(i + 1, sample_size):\n            inter = len(samples[i] & samples[j])\n            uni = len(samples[i] | samples[j])\n            jaccard_vals.append(inter / (uni + 1e-12))\n    overlap = np.mean(jaccard_vals) if jaccard_vals else 0.0\n    alpha = np.clip(0.4 + 1.8 * overlap, 0.3, 2.0)\n\n    # compute scores\n    scores = []\n    for s in remaining_subsets:\n        s_arr = np.array(s, dtype=int)\n        new = np.sum(np.isin(s_arr, rem_elems))\n        if new <= 0:\n            scores.append(-np.inf)\n            continue\n        denom = (len(s) ** alpha) + 1e-12\n        scores.append(new / denom)\n\n    scores = np.array(scores)\n    # softmax probabilities\n    max_score = np.max(scores)\n    exp_scores = np.exp(scores - max_score)\n    probs = exp_scores / (np.sum(exp_scores) + 1e-12)\n    chosen_idx = np.random.choice(len(remaining_subsets), p=probs)\n    return remaining_subsets[chosen_idx]\n\n",
  "adaptive_density_by_overlap_aug_115": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 4: uses max Jaccard overlap, sum\u2011based denominator, epsilon, and np.clip.\"\"\"\n    rem_elems = set(remaining_elements)\n\n    if not remaining_subsets:\n        return None\n\n    # compute maximum Jaccard among a sample\n    m = len(remaining_subsets)\n    sample_size = min(m, 15)\n    idxs = np.random.choice(m, size=sample_size, replace=False)\n    samples = [set(remaining_subsets[i]) for i in idxs]\n\n    jaccard_vals = []\n    for i in range(sample_size):\n        for j in range(i + 1, sample_size):\n            inter = len(samples[i] & samples[j])\n            uni = len(samples[i] | samples[j])\n            jaccard_vals.append(inter / (uni + 1e-12))\n    overlap = np.max(jaccard_vals) if jaccard_vals else 0.0\n    alpha = np.clip(0.7 + 1.3 * overlap, 0.6, 2.0)\n\n    # compute scores using sum in denominator\n    best = None\n    best_score = -np.inf\n    for s in remaining_subsets:\n        new = len(rem_elems & set(s))\n        if new <= 0:\n            continue\n        denom = np.sum(np.array(s, dtype=int)) + 1e-12\n        score = new / (denom ** alpha)\n        if score > best_score:\n            best_score = score\n            best = s\n\n    return best\n\n",
  "highest_degree_weighted_gain_aug_116": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selection using a while\u2011loop and vectorised frequency counting.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not remaining_subsets or not rem_set:\n        return None\n\n    max_elem = max(rem_set)\n    freq_arr = np.zeros(max_elem + 1, dtype=int)\n\n    # Build frequency array\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem_set:\n                freq_arr[e] += 1\n\n    best, best_score = None, -1.0\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        new = [e for e in set(s) if e in rem_set]\n        if new:\n            score = float(np.sum(freq_arr[new])) + 1e-12  # safety epsilon\n            if score > best_score:\n                best_score = score\n                best = s\n        idx += 1\n    return best\n\n",
  "highest_degree_weighted_gain_aug_117": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selection with weighted frequencies, deterministic noise for tie\u2011breaking,\n    and a 0.7 scaling factor on frequencies.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not remaining_subsets or not rem_set:\n        return None\n\n    max_elem = max(rem_set)\n    freq_arr = np.zeros(max_elem + 1, dtype=float)\n\n    # Weighted frequency counting\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem_set:\n                freq_arr[e] += 0.7\n\n    # Deterministic noise generator\n    seed = hash(tuple(sorted(remaining_elements))) ^ hash(tuple(map(tuple, remaining_subsets)))\n    rng = np.random.default_rng(seed)\n    noise = rng.normal(loc=0.0, scale=1e-6, size=len(remaining_subsets))\n\n    best, best_score = None, -1.0\n    for idx, s in enumerate(remaining_subsets):\n        new = [e for e in set(s) if e in rem_set]\n        if new:\n            score = float(np.sum(freq_arr[new])) + noise[idx] + 1e-12\n            if score > best_score:\n                best_score = score\n                best = s\n    return best\n\n",
  "highest_degree_weighted_gain_aug_118": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selection that scores subsets by the maximum frequency of newly covered\n    elements, then picks randomly among the top\u2011k candidates.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not remaining_subsets or not rem_set:\n        return None\n\n    max_elem = max(rem_set)\n    freq_arr = np.zeros(max_elem + 1, dtype=int)\n\n    # Build frequency array\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem_set:\n                freq_arr[e] += 1\n\n    # Compute scores\n    scores = []\n    for idx, s in enumerate(remaining_subsets):\n        new = [e for e in set(s) if e in rem_set]\n        if new:\n            score = float(np.max(freq_arr[new])) + 1e-12\n        else:\n            score = -1.0\n        scores.append((idx, score))\n\n    # Sort and pick top\u2011k\n    scores.sort(key=lambda x: x[1], reverse=True)\n    top_k = min(7, len(scores))\n    top_indices = [idx for idx, _ in scores[:top_k]]\n\n    # Deterministic random choice among top\u2011k\n    seed = hash(tuple(sorted(remaining_elements))) ^ hash(tuple(map(tuple, remaining_subsets)))\n    rng = np.random.default_rng(seed)\n    chosen_idx = rng.choice(top_indices)\n\n    return remaining_subsets[chosen_idx]\n\n",
  "highest_degree_weighted_gain_aug_119": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selection that uses a logarithmic proxy for frequencies and averages\n    them over newly covered elements.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not remaining_subsets or not rem_set:\n        return None\n\n    max_elem = max(rem_set)\n    freq_arr = np.zeros(max_elem + 1, dtype=int)\n\n    # Count raw frequencies\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem_set:\n                freq_arr[e] += 1\n\n    # Apply logarithmic proxy and clip to avoid overflow\n    freq_arr = np.clip(freq_arr, 0, 1_000_000)\n    log_freq = np.log1p(freq_arr)  # log(1+freq)\n\n    best, best_score = None, -1.0\n    for s in remaining_subsets:\n        new = [e for e in set(s) if e in rem_set]\n        if new:\n            score = float(np.mean(log_freq[new])) + 1e-12\n            if score > best_score:\n                best_score = score\n                best = s\n    return best\n\n",
  "tfidf_blend_gain_aug_120": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Variant 1: Syntactic & structural rewrite.\n    Uses while loops and explicit indexing.\n    \"\"\"\n    remaining_set = set(remaining_elements)\n    m = max(1, len(remaining_subsets))\n\n    freq: Dict[int, int] = {}\n    idx = 0\n    while idx < len(remaining_subsets):\n        subset = remaining_subsets[idx]\n        for element in set(subset):\n            if element in remaining_set:\n                freq[element] = freq.get(element, 0) + 1\n        idx += 1\n\n    best_subset = None\n    best_score = -1e18\n    idx = 0\n    while idx < len(remaining_subsets):\n        subset = remaining_subsets[idx]\n        new_elements = [e for e in set(subset) if e in remaining_set]\n        if not new_elements:\n            idx += 1\n            continue\n\n        tf = len(new_elements)\n        idf = 0.0\n        for element in new_elements:\n            f = freq.get(element, 1) + 1e-12  # epsilon to avoid division by zero\n            idf += np.log((m + 1.0) / f)\n\n        score = tf + 0.35 * idf\n        # clip score to avoid extreme values\n        score = np.clip(score, -1e9, 1e9)\n\n        if score > best_score:\n            best_score = score\n            best_subset = subset\n        idx += 1\n\n    return best_subset\n\n",
  "tfidf_blend_gain_aug_121": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Variant 2: Parameter tuning with top\u2011k selection and deterministic noise.\n    \"\"\"\n    remaining_set = set(remaining_elements)\n    m = max(1, len(remaining_subsets))\n\n    freq: Dict[int, int] = {}\n    for subset in remaining_subsets:\n        for element in set(subset):\n            if element in remaining_set:\n                freq[element] = freq.get(element, 0) + 1\n\n    scores = []\n    for idx, subset in enumerate(remaining_subsets):\n        new_elements = [e for e in set(subset) if e in remaining_set]\n        if not new_elements:\n            scores.append((-np.inf, None))\n            continue\n\n        tf = len(new_elements)\n        idf = 0.0\n        for element in new_elements:\n            f = freq.get(element, 1) + 1e-12\n            idf += np.log1p((m + 1.0) / f - 1)  # log1p approximation of log\n\n        score = tf + 0.5 * idf\n        score = np.clip(score, -1e9, 1e9)\n        # deterministic noise to break ties\n        noise = idx * 1e-6\n        scores.append((score + noise, subset))\n\n    # select top_k subsets by score\n    top_k = 7\n    if len(scores) < top_k:\n        top_k = len(scores)\n\n    # sort scores descending\n    sorted_scores = sorted(scores, key=lambda x: x[0], reverse=True)\n    # pick the best among top_k\n    best_subset = sorted_scores[0][1] if sorted_scores else None\n    return best_subset\n\n",
  "minimize_fragile_support_after_pick_aug_122": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selection with vectorized support counting and deterministic tie\u2011breaking.\n    \"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    # --- support counts using numpy ------------------------------------\n    all_elems = [e for s in remaining_subsets for e in s]\n    uniq, counts = np.unique(all_elems, return_counts=True)\n    support = dict(zip(uniq, counts))\n\n    # --- evaluate each candidate ---------------------------------------\n    best_subset, best_score = None, -np.inf\n    for idx, s in enumerate(remaining_subsets):\n        sset = set(s)\n        gain = len(rem0 & sset)\n        if gain <= 0:\n            continue\n\n        rem1 = rem0 - sset\n        fragile = sum(1 for e in rem1 if support.get(e, 0) == 1)\n        score = gain - 0.8 * fragile\n\n        # deterministic tie\u2011breaking: tiny noise proportional to index\n        score += 1e-6 * (idx + 1)\n\n        # clip to avoid overflow\n        score = np.clip(score, -1e9, 1e9)\n\n        if score > best_score:\n            best_score, best_subset = score, s\n\n    return best_subset\n\n",
  "minimize_fragile_support_after_pick_aug_123": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selection using a while loop, random choice among tied top\u2011k,\n    and a relaxed fragile penalty.\n    \"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0 or not remaining_subsets:\n        return None\n\n    # --- support counts via numpy ---------------------------------------\n    all_elems = np.concatenate([np.array(s) for s in remaining_subsets])\n    uniq, counts = np.unique(all_elems, return_counts=True)\n    support = dict(zip(uniq, counts))\n\n    # --- evaluate candidates with a while loop -------------------------\n    scores = []\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        sset = set(s)\n        gain = len(rem0 & sset)\n        if gain > 0:\n            rem1 = rem0 - sset\n            fragile = sum(1 for e in rem1 if support.get(e, 0) == 1)\n            score = gain - 0.7 * fragile\n            score = np.clip(score, -1e9, 1e9)\n            scores.append((s, score))\n        idx += 1\n\n    if not scores:\n        return None\n\n    # --- pick among the best (within epsilon) --------------------------\n    score_vals = np.array([sc for _, sc in scores])\n    max_score = np.max(score_vals)\n    eps = 1e-8\n    top_indices = np.where(score_vals >= max_score - eps)[0]\n    chosen = np.random.choice(top_indices) if len(top_indices) > 1 else top_indices[0]\n    return scores[chosen][0]\n\n",
  "minimize_fragile_support_after_pick_aug_124": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Probabilistic selection via soft\u2011min sampling and deterministic tie\u2011breaking.\n    \"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0 or not remaining_subsets:\n        return None\n\n    # --- support counts ------------------------------------------------\n    all_elems = [e for s in remaining_subsets for e in s]\n    uniq, counts = np.unique(all_elems, return_counts=True)\n    support = dict(zip(uniq, counts))\n\n    # --- compute raw scores --------------------------------------------\n    scores = []\n    for s in remaining_subsets:\n        sset = set(s)\n        gain = len(rem0 & sset)\n        if gain <= 0:\n            scores.append(-np.inf)\n            continue\n        rem1 = rem0 - sset\n        frag_arr = np.array([support.get(e, 0) for e in rem1])\n        fragile = np.sum(frag_arr <= 1)\n        score = gain - 0.9 * fragile\n        scores.append(score)\n\n    scores_np = np.array(scores)\n    if np.all(np.isneginf(scores_np)):\n        return None\n\n    # --- soft\u2011min probability ------------------------------------------\n    T = 1.0\n    logits = -scores_np / (T + 1e-12)\n    exp_logits = np.exp(logits - np.max(logits))          # numerical stability\n    probs = exp_logits / (np.sum(exp_logits) + 1e-12)\n    probs = np.clip(probs, 0, 1)\n\n    # --- deterministic noise for reproducible tie\u2011breaking -------------\n    noise = np.arange(len(probs)) * 1e-6\n    probs = probs + noise\n    probs = probs / (np.sum(probs) + 1e-12)\n\n    idx = np.argmax(probs)\n    return remaining_subsets[idx]\n\n",
  "minimize_fragile_support_after_pick_aug_125": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    List\u2011comprehension based evaluation, max\u2011score selection,\n    and a stricter fragile penalty.\n    \"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    # --- support counts ------------------------------------------------\n    all_elems = [e for s in remaining_subsets for e in s]\n    uniq, counts = np.unique(all_elems, return_counts=True)\n    support = dict(zip(uniq, counts))\n\n    # --- candidates with positive gain ---------------------------------\n    candidates = [(s, len(rem0 & set(s))) for s in remaining_subsets\n                  if len(rem0 & set(s)) > 0]\n    if not candidates:\n        return None\n\n    # --- compute scores with deterministic noise ----------------------\n    scores = np.array([gain - 0.6 * sum(1 for e in (rem0 - set(s))\n                                       if support.get(e, 0) == 1)\n                       for s, gain in candidates])\n    scores += np.arange(len(scores)) * 1e-6          # noise for tie\u2011break\n    scores = np.clip(scores, -1e9, 1e9)\n\n    best_idx = np.argmax(scores)\n    return candidates[best_idx][0]\n\n",
  "minimax_support_leftover_aug_126": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Vectorised implementation using numpy padding.\n    Adds a tiny deterministic noise term to break ties.\n    \"\"\"\n    # Prepare padded array of remaining subsets\n    max_len = max(len(s) for s in remaining_subsets) if remaining_subsets else 0\n    if max_len == 0:\n        return None\n    arr = np.full((len(remaining_subsets), max_len), -1, dtype=int)\n    for i, s in enumerate(remaining_subsets):\n        arr[i, :len(s)] = s\n\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Compute support counts for each remaining element\n    support: Dict[int, int] = {}\n    for e in rem_set:\n        support[e] = int((arr == e).any(axis=1).sum())\n\n    rng = np.random.default_rng(0)  # deterministic seed\n    best_subset = None\n    best_tuple = None\n    for s in remaining_subsets:\n        sset = set(s)\n        gain = len(rem_set & sset)\n        if gain <= 0:\n            continue\n        rem1 = rem_set - sset\n        max_sup = max((support.get(e, 0) for e in rem1), default=0)\n        noise = rng.random() * 1e-6  # tiny deterministic noise\n        t = (gain, -max_sup, -len(s), noise)\n        if best_tuple is None or t > best_tuple:\n            best_tuple, best_subset = t, s\n    return best_subset\n\n",
  "minimax_support_leftover_aug_127": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    While\u2011loop control flow, list comprehensions for support,\n    weighted scoring with softmin style, and deterministic noise.\n    \"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    # Support via list comprehension\n    support = {e: sum(1 for s in remaining_subsets if e in s) for e in rem0}\n\n    rng = np.random.default_rng(1)  # deterministic\n    best_subset = None\n    best_tuple = None\n\n    i = 0\n    while i < len(remaining_subsets):\n        s = remaining_subsets[i]\n        sset = set(s)\n        gain = len(rem0 & sset)\n        if gain > 0:\n            rem1 = rem0 - sset\n            max_sup = max((support.get(e, 0) for e in rem1), default=0)\n            # Weighted score: higher gain, lower max_sup\n            weight = 0.6 * gain - 0.4 * max_sup\n            weight = np.clip(weight, -1e12, 1e12)  # safety clip\n            noise = rng.random() * 1e-6\n            t = (weight, -max_sup, -len(s), noise)\n            if best_tuple is None or t > best_tuple:\n                best_tuple, best_subset = t, s\n        i += 1\n    return best_subset\n\n",
  "minimax_support_leftover_aug_128": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Compute scores, keep top\u2011k candidates, then randomly select one\n    (deterministic randomness). Uses median for support aggregation.\n    \"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    support = {e: 0 for e in rem0}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    scores = []\n    for s in remaining_subsets:\n        sset = set(s)\n        gain = len(rem0 & sset)\n        if gain <= 0:\n            continue\n        rem1 = rem0 - sset\n        max_sup = max((support.get(e, 0) for e in rem1), default=0)\n        # Use median of remaining supports as alternative aggregation\n        med_sup = np.median([support.get(e, 0) for e in rem1]) if rem1 else 0\n        t = (gain, -max_sup, -len(s), med_sup)\n        scores.append((t, s))\n\n    if not scores:\n        return None\n\n    # Sort by primary tuple components\n    scores.sort(key=lambda x: x[0], reverse=True)\n    top_k = 5\n    top_candidates = [s for _, s in scores[:top_k]]\n    rng = np.random.default_rng(2)  # deterministic\n    chosen = rng.choice(top_candidates)\n    return chosen\n\n",
  "minimax_support_leftover_aug_129": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Approximate support using a random sample of subsets.\n    Uses softmin weighting (gain / (max_sup + eps)).\n    \"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    # Approximate support via sampling\n    rng = np.random.default_rng(3)\n    sample_size = min(10, len(remaining_subsets))\n    sampled = rng.choice(remaining_subsets, size=sample_size, replace=False)\n    support = {e: 0 for e in rem0}\n    for s in sampled:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    best_subset = None\n    best_score = -np.inf\n    eps = 1e-12\n    for s in remaining_subsets:\n        sset = set(s)\n        gain = len(rem0 & sset)\n        if gain <= 0:\n            continue\n        rem1 = rem0 - sset\n        max_sup = max((support.get(e, 0) for e in rem1), default=0)\n        # Softmin score\n        score = gain / (max_sup + eps)\n        score = np.clip(score, -1e12, 1e12)  # prevent overflow\n        # Add deterministic noise for tie\u2011breaking\n        noise = rng.random() * 1e-6\n        score += noise\n        if score > best_score:\n            best_score = score\n            best_subset = s\n    return best_subset\n\n",
  "gain_minus_log_size_aug_130": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Vectorised implementation that builds score arrays and picks the best set.\n    Adds a tiny deterministic noise to break ties.\n    \"\"\"\n    # Prepare data\n    rem_set = set(remaining_elements)\n    lam_factor = 0.8\n\n    # Compute gains for all remaining subsets\n    gains = np.array([len(rem_set & set(s)) for s in remaining_subsets])\n\n    # Keep only subsets that cover at least one remaining element\n    valid_mask = gains > 0\n    if not np.any(valid_mask):\n        return None\n\n    # Compute scores: gain - lam * log(1 + |S|)\n    subset_sizes = np.array([len(s) for s in remaining_subsets])[valid_mask]\n    scores = gains[valid_mask] - lam_factor * np.log1p(subset_sizes + 1e-12)\n\n    # Clip scores to avoid extreme values\n    scores = np.clip(scores, -1e9, 1e9)\n\n    # Add a tiny deterministic noise to break ties deterministically\n    scores += 1e-9 * np.arange(len(scores))\n\n    # Pick the index with the maximum score\n    best_idx = np.argmax(scores)\n    return remaining_subsets[valid_mask][best_idx]\n\n",
  "gain_minus_log_size_aug_131": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    While\u2011loop variant that collects valid subsets, then randomly selects\n    among the top\u2011k best candidates.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    lam = np.clip(0.8, 0.0, 1.0)          # ensure lam stays in [0,1]\n\n    # Gather all valid subsets with their gains and sizes\n    valid_data = []\n    while remaining_subsets:\n        s = remaining_subsets.pop()\n        gain = len(rem_set & set(s))\n        if gain > 0:\n            valid_data.append((s, gain, len(s)))\n\n    if not valid_data:\n        return None\n\n    # Compute scores\n    scores = np.array([g - lam * np.log1p(l + 1e-12) for _, g, l in valid_data])\n\n    # Select top\u2011k candidates (k = 5 or fewer if not enough sets)\n    top_k = min(5, len(scores))\n    top_indices = np.argpartition(-scores, top_k - 1)[:top_k]\n\n    # Randomly pick one of the top\u2011k candidates\n    chosen_idx = np.random.choice(top_indices)\n    return valid_data[chosen_idx][0]\n\n",
  "gain_minus_log_size_aug_132": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Soft\u2011min selection: computes a cost as 1/(score + epsilon) and picks\n    the subset with the smallest cost. Adds epsilon to avoid division by zero.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    lam = 0.8\n\n    best = None\n    best_cost = np.inf\n\n    for s in remaining_subsets:\n        gain = len(rem_set & set(s))\n        if gain <= 0:\n            continue\n\n        # Compute the score with a tiny epsilon in the log argument\n        score = gain - lam * np.log1p(len(s) + 1e-12)\n        # Clip to keep score strictly positive before inversion\n        score = np.clip(score, 1e-12, None)\n\n        # Soft\u2011min cost\n        cost = 1.0 / (score + 1e-12)\n        if cost < best_cost:\n            best_cost, best = cost, s\n\n    return best\n\n",
  "gain_minus_log_size_aug_133": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Softmax weighting: converts scores to probabilities and deterministically\n    selects the candidate with the highest probability. Uses epsilon to\n    stabilise the normalisation step.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    lam = np.clip(0.8, 0.0, 1.0)          # keep lam in a sane range\n\n    candidate_sets = []\n    candidate_scores = []\n\n    for s in remaining_subsets:\n        gain = len(rem_set & set(s))\n        if gain <= 0:\n            continue\n        score = gain - lam * np.log1p(len(s) + 1e-12)\n        candidate_sets.append(s)\n        candidate_scores.append(score)\n\n    if not candidate_scores:\n        return None\n\n    scores = np.array(candidate_scores)\n    # Clip to avoid numerical overflow in exp\n    scores = np.clip(scores, -1e9, 1e9)\n\n    # Softmax probabilities\n    exp_scores = np.exp(scores / 1.0)                    # temperature = 1.0\n    probs = exp_scores / (np.sum(exp_scores) + 1e-12)    # epsilon to avoid div0\n\n    # Deterministically pick the set with the highest probability\n    best_idx = np.argmax(probs)\n    return candidate_sets[best_idx]\n\n",
  "sqrt_diminishing_returns_aug_134": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    best_subset = None\n    best_score = -np.inf\n    rng = np.random.default_rng(seed=0)          # deterministic noise source\n\n    for subset in remaining_subsets:\n        gain = len(rem_set.intersection(subset))\n        if gain <= 0:\n            continue\n\n        # Denominator with epsilon to avoid division by zero\n        denom = 1.0 + np.sqrt(len(subset)) + 1e-12\n        score = np.sqrt(gain) / denom\n\n        # Deterministic tie\u2011breaking noise based on subset hash\n        noise = (hash(tuple(subset)) % 1000) * 1e-6\n        score += noise\n\n        # Clip to a valid range\n        score = np.clip(score, 0, 1)\n\n        if score > best_score:\n            best_score = score\n            best_subset = subset\n\n    return best_subset\n\n",
  "sqrt_diminishing_returns_aug_135": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    scores = []\n    subsets = []\n\n    idx = 0\n    while idx < len(remaining_subsets):\n        subset = remaining_subsets[idx]\n        gain = len(rem_set.intersection(subset))\n        if gain > 0:\n            # Hyper\u2011parameter tuning: reduced influence of size\n            denom = 1.0 + 0.5 * np.sqrt(len(subset)) + 1e-12\n            score = np.sqrt(gain) / denom\n            scores.append(score)\n            subsets.append(subset)\n        idx += 1\n\n    if not scores:\n        return None\n\n    scores = np.array(scores)\n    # Clip scores to avoid extreme values\n    scores = np.clip(scores, 0, 1)\n\n    best_idx = np.argmax(scores)\n    return subsets[best_idx]\n\n",
  "sqrt_diminishing_returns_aug_136": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    scores = []\n    subsets = []\n\n    for subset in remaining_subsets:\n        gain = len(rem_set.intersection(subset))\n        if gain <= 0:\n            continue\n\n        denom = 1.0 + np.sqrt(len(subset)) + 1e-12\n        score = np.sqrt(gain) / denom\n        scores.append(score)\n        subsets.append(subset)\n\n    if not scores:\n        return None\n\n    scores = np.array(scores)\n    # Softmax with temperature to smooth selection\n    temp = 0.5\n    logits = scores / temp\n    exp_logits = np.exp(logits - np.max(logits))  # numerical stability\n    probs = exp_logits / np.clip(np.sum(exp_logits), 1e-12, None)\n\n    chosen_idx = np.argmax(probs)  # deterministic tie\u2011break\n    return subsets[chosen_idx]\n\n",
  "sqrt_diminishing_returns_aug_137": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    scores = []\n    subsets = []\n\n    for subset in remaining_subsets:\n        gain = len(rem_set.intersection(subset))\n        if gain <= 0:\n            continue\n\n        denom = 1.0 + np.sqrt(len(subset)) + 1e-12\n        score = np.sqrt(gain) / denom\n        scores.append(score)\n        subsets.append(subset)\n\n    if not scores:\n        return None\n\n    scores = np.array(scores)\n    # Clip to keep scores in a safe range\n    scores = np.clip(scores, 0, 1)\n\n    # Randomly pick among the top\u2011k best subsets\n    top_k = min(3, len(scores))\n    top_indices = np.argpartition(-scores, top_k - 1)[:top_k]\n    rng = np.random.default_rng(seed=42)\n    chosen_idx = rng.choice(top_indices)\n    return subsets[chosen_idx]\n\n",
  "log_diminishing_returns_aug_138": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorized implementation with deterministic tie\u2011breaking.\"\"\"\n    uncovered = set(remaining_elements)\n\n    # Compute gains and subset sizes\n    gains = np.array([len(uncovered.intersection(s)) for s in remaining_subsets], dtype=int)\n    sizes = np.array([len(s) for s in remaining_subsets], dtype=int)\n\n    # Only keep subsets that cover at least one new element\n    valid = gains > 0\n    if not np.any(valid):\n        return None\n\n    # Scoring: log\u2011gain divided by a size\u2011dependent penalty\n    denom = 1.0 + np.log1p(sizes[valid]) + 1e-12  # epsilon to avoid division by zero\n    scores = np.log1p(gains[valid]) / denom\n    scores = np.clip(scores, 0.0, 1.0)  # prevent extreme values\n\n    # Add a tiny deterministic noise to break ties\n    noise = np.arange(scores.size) * 1e-6\n    scores += noise\n\n    # Pick the best candidate\n    best_idx = np.argmax(scores)\n    best_subset = [s for s, v in zip(remaining_subsets, valid) if v][best_idx]\n    return best_subset\n\n",
  "log_diminishing_returns_aug_139": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"While\u2011loop version with weighted scoring and a top\u2011k selection.\"\"\"\n    uncovered = set(remaining_elements)\n    n = len(remaining_subsets)\n\n    scores = []\n    subsets = []\n\n    idx = 0\n    while idx < n:\n        s = remaining_subsets[idx]\n        gain = len(uncovered.intersection(s))\n        if gain > 0:\n            size = len(s)\n            # Modified scoring: weighted combination of log\u2011gain and inverse size\n            score = 0.6 * np.log1p(gain) + 0.4 * (1.0 / (size + 1e-12))\n            score = np.clip(score, 0.0, 1.0)          # bound the score\n            score += 1e-6 * idx                      # deterministic tie\u2011breaking\n            scores.append(score)\n            subsets.append(s)\n        idx += 1\n\n    if not scores:\n        return None\n\n    # Keep only the top\u2011k candidates\n    top_k = min(7, len(scores))\n    top_indices = np.argsort(-np.array(scores))[:top_k]\n    top_scores = [scores[i] for i in top_indices]\n    top_subsets = [subsets[i] for i in top_indices]\n\n    # Return the best among the top\u2011k\n    best_idx = np.argmax(top_scores)\n    return top_subsets[best_idx]\n\n",
  "log_diminishing_returns_aug_140": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Median\u2011threshold selection with deterministic tie\u2011breaking.\"\"\"\n    uncovered = set(remaining_elements)\n\n    gains = [len(uncovered.intersection(s)) for s in remaining_subsets]\n    if not any(gains):\n        return None\n\n    scores = [\n        np.log1p(g) / (1.0 + np.log1p(len(s)) + 1e-12)  # epsilon for safety\n        for g, s in zip(gains, remaining_subsets)\n    ]\n    scores = [np.clip(sc, 0.0, 1.0) for sc in scores]  # clip to avoid outliers\n\n    median_score = np.median(scores)\n\n    # Candidates that meet or exceed the median\n    candidates = [(s, sc) for s, sc in zip(remaining_subsets, scores) if sc >= median_score]\n    if not candidates:\n        return None\n\n    # Deterministic tie\u2011breaking using a small noise term\n    best_subset = max(\n        candidates,\n        key=lambda pair: pair[1] + 1e-6 * remaining_subsets.index(pair[0])\n    )[0]\n    return best_subset\n\n",
  "log_diminishing_returns_aug_141": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Proxy\u2011log scoring with a deterministic random choice among the top\u2011k.\"\"\"\n    uncovered = set(remaining_elements)\n    n = len(remaining_subsets)\n\n    scores = []\n    idx = 0\n    while idx < n:\n        s = remaining_subsets[idx]\n        gain = len(uncovered.intersection(s))\n        if gain > 0:\n            size = len(s)\n            # Proxy for log1p(gain): gain/(1+gain)\n            proxy_log = gain / (1.0 + gain + 1e-12)\n            proxy_den = 1.0 + np.log1p(size) + 1e-12\n            score = proxy_log / proxy_den\n            score = np.clip(score, 0.0, 1.0)  # bound the score\n            scores.append((s, score))\n        idx += 1\n\n    if not scores:\n        return None\n\n    # Keep the top\u2011k candidates (k = 5 or fewer)\n    top_k = min(5, len(scores))\n    scores.sort(key=lambda pair: pair[1], reverse=True)\n    top_candidates = [pair[0] for pair in scores[:top_k]]\n\n    # Deterministic \u201crandom\u201d choice using a fixed seed\n    rng = np.random.default_rng(seed=42)\n    chosen = rng.choice(top_candidates)\n    return chosen\n\n",
  "future_topq_mean_gain_aug_142": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 1: Uses median aggregation and deterministic tie\u2011breaking noise.\"\"\"\n    uncovered = set(remaining_elements)\n    if not uncovered:\n        return None\n\n    gamma = 0.6\n    q = 0.2\n\n    subs = [set(s) for s in remaining_subsets]\n    best_idx, best_score = None, -1e18\n\n    i = 0\n    while i < len(subs):\n        cur = subs[i]\n        g1 = len(uncovered.intersection(cur))\n        if g1 <= 0:\n            i += 1\n            continue\n        rem_after = uncovered - cur\n        if not rem_after:\n            return remaining_subsets[i]\n\n        future_gains = []\n        for j, nxt in enumerate(subs):\n            if j == i:\n                continue\n            g2 = len(rem_after.intersection(nxt))\n            if g2 > 0:\n                future_gains.append(g2)\n\n        if not future_gains:\n            score = g1\n        else:\n            future_gains.sort(reverse=True)\n            k = max(1, int(np.ceil(q * len(future_gains) + 1e-12)))\n            k = np.clip(k, 1, len(future_gains))\n            score = g1 + gamma * float(np.median(future_gains[:k]))\n\n        # deterministic noise to break ties\n        score += 1e-6 * i\n        if score > best_score:\n            best_score, best_idx = score, i\n        i += 1\n\n    return remaining_subsets[best_idx] if best_idx is not None else None\n\n",
  "future_best3_lookahead_aug_143": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorised set\u2011cover heuristic using NumPy arrays.\"\"\"\n    rem_elems = np.array(remaining_elements, dtype=int)\n    if rem_elems.size == 0:\n        return None\n\n    a, b = 0.55, 0.25\n    sets_arr = [np.array(s, dtype=int) for s in remaining_subsets]\n\n    best_idx = None\n    best_score = -1e18\n    noise = 1e-6  # deterministic tie\u2011breaker\n\n    for idx, s in enumerate(sets_arr):\n        gain1 = np.isin(s, rem_elems).sum()\n        if gain1 == 0:\n            continue\n\n        rem1 = np.setdiff1d(rem_elems, s, assume_unique=True)\n        if rem1.size == 0:\n            return remaining_subsets[idx]\n\n        # gains for the other subsets\n        g2_list = []\n        for j, t in enumerate(sets_arr):\n            if j == idx:\n                continue\n            g2 = np.isin(t, rem1).sum()\n            if g2 > 0:\n                g2_list.append(g2)\n\n        if g2_list:\n            g2_arr = np.array(g2_list, dtype=int)\n            best2 = g2_arr.max()\n            best3 = np.partition(g2_arr, -2)[-2] if g2_arr.size > 1 else 0\n        else:\n            best2 = best3 = 0\n\n        score = gain1 + a * best2 + b * best3 + noise * idx\n        if score > best_score:\n            best_score = score\n            best_idx = idx\n\n    return remaining_subsets[best_idx] if best_idx is not None else None\n\n",
  "future_best3_lookahead_aug_144": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Set\u2011cover heuristic with tuned weights and top\u2011k aggregation.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    a, b = 0.40, 0.20          # tuned hyper\u2011parameters\n    top_k = 4                  # number of secondary gains considered\n\n    best_idx = None\n    best_score = -1e18\n\n    for idx, subset in enumerate(remaining_subsets):\n        sset = set(subset)\n        gain1 = len(rem_set & sset)\n        if gain1 == 0:\n            continue\n\n        rem1 = rem_set - sset\n        if not rem1:\n            return remaining_subsets[idx]\n\n        g2_list = []\n        for other in remaining_subsets:\n            if other is subset:\n                continue\n            g2 = len(rem1 & set(other))\n            if g2 > 0:\n                g2_list.append(g2)\n\n        g2_list.sort(reverse=True)\n        best2 = g2_list[0] if len(g2_list) >= 1 else 0\n        best3 = g2_list[1] if len(g2_list) >= 2 else 0\n\n        score = gain1 + a * best2 + b * best3 + 1e-8 * idx\n        score = np.clip(score, -1e18, 1e18)  # bound to avoid overflow\n\n        if score > best_score:\n            best_score = score\n            best_idx = idx\n\n    return remaining_subsets[best_idx] if best_idx is not None else None\n\n",
  "future_best3_lookahead_aug_145": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Soft\u2011min probabilistic selection with deterministic noise.\"\"\"\n    rem_elems = np.array(remaining_elements, dtype=int)\n    if rem_elems.size == 0:\n        return None\n\n    a, b = 0.55, 0.25\n    temperature = 1.0\n\n    scores = []\n    for subset in remaining_subsets:\n        s = np.array(subset, dtype=int)\n        gain1 = np.isin(s, rem_elems).sum()\n        if gain1 == 0:\n            scores.append(-np.inf)\n            continue\n\n        rem1 = np.setdiff1d(rem_elems, s, assume_unique=True)\n        if rem1.size == 0:\n            return subset\n\n        g2_list = []\n        for other in remaining_subsets:\n            if other is subset:\n                continue\n            g2 = np.isin(np.array(other, dtype=int), rem1).sum()\n            if g2 > 0:\n                g2_list.append(g2)\n\n        if g2_list:\n            g2_arr = np.array(g2_list, dtype=int)\n            best2 = np.median(g2_arr)\n            best3 = np.partition(g2_arr, -2)[-2] if g2_arr.size > 1 else 0\n        else:\n            best2 = best3 = 0\n\n        scores.append(gain1 + a * best2 + b * best3)\n\n    scores_np = np.array(scores, dtype=float)\n    exp_vals = np.exp(-scores_np / (temperature + 1e-12))\n    probs = exp_vals / (exp_vals.sum() + 1e-12)\n    chosen_idx = np.random.choice(len(remaining_subsets), p=probs)\n    return remaining_subsets[chosen_idx]\n\n",
  "future_best3_lookahead_aug_146": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"While\u2011loop implementation with deterministic noise and clipping.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    a, b = 0.60, 0.30\n    noise = 1e-5\n\n    best_idx = None\n    best_score = -1e18\n\n    i = 0\n    while i < len(remaining_subsets):\n        subset = remaining_subsets[i]\n        sset = set(subset)\n        gain1 = len(rem_set & sset)\n        if gain1 == 0:\n            i += 1\n            continue\n\n        rem1 = rem_set - sset\n        if not rem1:\n            return remaining_subsets[i]\n\n        # compute secondary gains\n        g2_vals = []\n        j = 0\n        while j < len(remaining_subsets):\n            if j == i:\n                j += 1\n                continue\n            other = remaining_subsets[j]\n            g2 = len(rem1 & set(other))\n            if g2 > 0:\n                g2_vals.append(g2)\n            j += 1\n\n        if g2_vals:\n            g2_arr = np.array(g2_vals, dtype=int)\n            best2 = g2_arr.max()\n            best3 = np.partition(g2_arr, -2)[-2] if g2_arr.size > 1 else 0\n        else:\n            best2 = best3 = 0\n\n        score = gain1 + a * best2 + b * best3 + noise * i\n        score = np.clip(score, -1e18, 1e18)\n\n        if score > best_score:\n            best_score = score\n            best_idx = i\n\n        i += 1\n\n    return remaining_subsets[best_idx] if best_idx is not None else None\n\n",
  "maximize_support_entropy_leftover_aug_147": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef _entropy(counts: np.ndarray) -> float:\n    eps = 1e-12\n    total = float(counts.sum())\n    if total <= 0:\n        return 0.0\n    p = counts / (total + eps)\n    p = np.clip(p, eps, None)\n    return float(-(p * np.log(p)).sum())\n\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    remaining_set = set(remaining_elements)\n    if remaining_set:\n        elem_cover: Dict[int, int] = {e: 0 for e in remaining_set}\n        for subset in remaining_subsets:\n            for elem in set(subset):\n                if elem in elem_cover:\n                    elem_cover[elem] += 1\n\n        best_subset, best_score = None, -np.inf\n        for subset in remaining_subsets:\n            subset_set = set(subset)\n            gain = len(remaining_set & subset_set)\n            if gain <= 0:\n                continue\n            after_set = remaining_set - subset_set\n            if not after_set:\n                return subset\n            counts = np.array([elem_cover.get(e, 0) for e in after_set], dtype=float)\n            ent = _entropy(counts)\n            ent = np.clip(ent, 0, 10)\n            score = gain + 0.35 * ent\n            if score > best_score:\n                best_score, best_subset = score, subset\n        return best_subset\n    return None\n\n",
  "maximize_support_entropy_leftover_aug_148": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef _entropy(counts: np.ndarray) -> float:\n    eps = 1e-12\n    total = float(counts.sum())\n    if total <= 0:\n        return 0.0\n    p = counts / (total + eps)\n    p = np.clip(p, eps, None)\n    return float(-(p * np.log(p)).sum())\n\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    remaining_set = set(remaining_elements)\n    if not remaining_set:\n        return None\n\n    elem_cover: Dict[int, int] = {e: 0 for e in remaining_set}\n    for subset in remaining_subsets:\n        for elem in set(subset):\n            if elem in elem_cover:\n                elem_cover[elem] += 1\n\n    scores = []\n    for subset in remaining_subsets:\n        subset_set = set(subset)\n        gain = len(remaining_set & subset_set)\n        if gain <= 0:\n            continue\n        after_set = remaining_set - subset_set\n        if not after_set:\n            return subset\n        counts = np.array([elem_cover.get(e, 0) for e in after_set], dtype=float)\n        ent = _entropy(counts)\n        ent = np.clip(ent, 0, 10)\n        score = gain + 0.45 * ent\n        score += 1e-6 * sum(subset)  # deterministic tie\u2011breaking noise\n        scores.append((score, subset))\n\n    if not scores:\n        return None\n\n    # choose from the top\u2011k candidates\n    top_k = 5\n    top_scores = sorted(scores, key=lambda x: x[0], reverse=True)[:top_k]\n    best_subset = max(top_scores, key=lambda x: x[0])[1]\n    return best_subset\n\n",
  "maximize_support_entropy_leftover_aug_149": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef _entropy(counts: np.ndarray) -> float:\n    eps = 1e-12\n    total = float(counts.sum())\n    if total <= 0:\n        return 0.0\n    p = counts / (total + eps)\n    p = np.clip(p, eps, None)\n    return float(-(p * np.log(p)).sum())\n\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    remaining_set = set(remaining_elements)\n    if not remaining_set:\n        return None\n\n    elem_cover: Dict[int, int] = {e: 0 for e in remaining_set}\n    for subset in remaining_subsets:\n        for elem in set(subset):\n            if elem in elem_cover:\n                elem_cover[elem] += 1\n\n    scores = []\n    for subset in remaining_subsets:\n        subset_set = set(subset)\n        gain = len(remaining_set & subset_set)\n        if gain <= 0:\n            continue\n        after_set = remaining_set - subset_set\n        if not after_set:\n            return subset\n        counts = np.array([elem_cover.get(e, 0) for e in after_set], dtype=float)\n        ent = _entropy(counts)\n        ent = np.clip(ent, 0, 10)\n        score = gain + 0.35 * ent\n        score += 1e-6 * len(subset)  # deterministic noise\n        scores.append((score, subset))\n\n    if not scores:\n        return None\n\n    # soft\u2011min over the scores\n    scores_arr = np.array([s[0] for s in scores], dtype=float)\n    tau = 0.5\n    probs = np.exp(-scores_arr / tau)\n    probs = probs / probs.sum()\n    chosen_idx = np.random.choice(len(scores), p=probs)\n    return scores[chosen_idx][1]\n\n",
  "maximize_support_entropy_leftover_aug_150": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef _entropy(counts: np.ndarray) -> float:\n    eps = 1e-12\n    total = float(counts.sum())\n    if total <= 0:\n        return 0.0\n    p = counts / (total + eps)\n    p = np.clip(p, eps, None)\n    return float(-(p * np.log(p)).sum())\n\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    remaining_set = set(remaining_elements)\n    if not remaining_set:\n        return None\n\n    # Build support counts\n    elem_cover: Dict[int, int] = {e: 0 for e in remaining_set}\n    for subset in remaining_subsets:\n        for elem in set(subset):\n            if elem in elem_cover:\n                elem_cover[elem] += 1\n\n    # Compute a score for every candidate\n    scores = []\n    for subset in remaining_subsets:\n        subset_set = set(subset)\n        gain = len(remaining_set & subset_set)\n        if gain <= 0:\n            continue\n        after_set = remaining_set - subset_set\n        if not after_set:\n            return subset\n        counts = np.array([elem_cover.get(e, 0) for e in after_set], dtype=float)\n        ent = _entropy(counts)\n        ent = np.clip(ent, 0, 10)\n        mean_cover = np.mean(counts)\n        score = gain + 0.45 * ent + 0.05 * mean_cover\n        score += 1e-6 * sum(subset)  # deterministic tie\u2011breaking\n        scores.append((score, subset))\n\n    if not scores:\n        return None\n\n    # Random choice among the top\u2011k with weighted probabilities\n    top_k = 7\n    top_scores = sorted(scores, key=lambda x: x[0], reverse=True)[:top_k]\n    probs = np.exp(np.array([s[0] for s in top_scores]) / 1.0)\n    probs = probs / probs.sum()\n    chosen = np.random.choice(len(top_scores), p=probs)\n    return top_scores[chosen][1]\n\n",
  "lexicographic_cover_rarest_first_aug_151": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Variant 1: Purely structural rewrite.\n    Uses a while\u2011loop, numpy arrays for frequency counts, and a\n    lexicographic vector that starts with the coverage ratio.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # map element to index for array access\n    elem_to_idx = {e: i for i, e in enumerate(rem_set)}\n    freq_arr = np.zeros(len(rem_set), dtype=int)\n\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in elem_to_idx:\n                freq_arr[elem_to_idx[e]] += 1\n\n    uniq_freqs = sorted(set(freq_arr[freq_arr > 0]))\n    if not uniq_freqs:\n        return None\n\n    best_subset = None\n    best_vec = None\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        sset = set(s)\n        cov = [e for e in sset if e in rem_set]\n        if cov:\n            cov_ratio = len(cov) / (len(rem_set) + 1e-12)   # epsilon\n            vec = [cov_ratio]\n            for f in uniq_freqs:\n                vec.append(np.sum([1 for e in cov if freq_arr[elem_to_idx[e]] == f]))\n            vec.append(-len(s))\n            t = tuple(vec)\n            if best_vec is None or t > best_vec:\n                best_vec = t\n                best_subset = s\n        idx += 1\n\n    return best_subset\n\n",
  "lexicographic_cover_rarest_first_aug_152": "import numpy as np\nfrom typing import List, Optional\nimport random\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Variant 2: Parameterized scoring with random choice among top\u2011k.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    elem_to_idx = {e: i for i, e in enumerate(rem_set)}\n    freq_arr = np.zeros(len(rem_set), dtype=int)\n\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in elem_to_idx:\n                freq_arr[elem_to_idx[e]] += 1\n\n    # hyper\u2011parameters\n    min_cov_frac = 0.1          # minimum coverage fraction\n    cov_weight = 0.7\n    size_weight = 0.3\n    top_k = 5\n\n    scores = []\n    for s in remaining_subsets:\n        sset = set(s)\n        cov = [e for e in sset if e in rem_set]\n        if len(cov) < min_cov_frac * (len(rem_set) + 1e-12):   # epsilon\n            continue\n        # weighted score\n        cov_counts = np.array([np.sum([1 for e in cov if freq_arr[elem_to_idx[e]] == f])\n                               for f in set(freq_arr[freq_arr > 0])])\n        score = cov_counts.sum() * cov_weight - len(s) * size_weight\n        scores.append((score, s))\n\n    if not scores:\n        return None\n\n    # sort descending and pick random among top\u2011k\n    scores.sort(key=lambda x: x[0], reverse=True)\n    top_k = min(top_k, len(scores))\n    chosen = random.choice([s for _, s in scores[:top_k]])\n    return chosen\n\n",
  "lexicographic_cover_rarest_first_aug_153": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Variant 3: Semantic\u2011preserving with deterministic noise, median aggregation\n    and softmin probabilistic selection.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    elem_to_idx = {e: i for i, e in enumerate(rem_set)}\n    freq_arr = np.zeros(len(rem_set), dtype=int)\n\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in elem_to_idx:\n                freq_arr[elem_to_idx[e]] += 1\n\n    # collect scores with deterministic noise\n    scores = []\n    for s in remaining_subsets:\n        sset = set(s)\n        cov = [e for e in sset if e in rem_set]\n        if not cov:\n            continue\n        counts = [np.sum([1 for e in cov if freq_arr[elem_to_idx[e]] == f])\n                  for f in set(freq_arr[freq_arr > 0])]\n        median_val = np.median(counts) if counts else 0.0\n        # deterministic noise from hash\n        noise = 1e-6 * (hash(tuple(sorted(s))) % 1000)\n        score = median_val - len(s) * 0.5 + noise\n        scores.append((score, s))\n\n    if not scores:\n        return None\n\n    # softmin probabilities\n    scores_arr = np.array([sc for sc, _ in scores])\n    exp_vals = np.exp(-scores_arr)  # softmin\n    probs = exp_vals / (exp_vals.sum() + 1e-12)  # epsilon\n    probs = np.clip(probs, 0.0, 1.0)  # ensure bounds\n\n    rng = np.random.default_rng()\n    idx = rng.choice(len(scores), p=probs)\n    return scores[idx][1]\n\n",
  "lexicographic_cover_rarest_first_aug_154": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Variant 4: Structural rewrite with while loop, vectorized counts,\n    and random choice from top\u20115 highest weighted scores.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    elem_to_idx = {e: i for i, e in enumerate(rem_set)}\n    freq_arr = np.zeros(len(rem_set), dtype=int)\n\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in elem_to_idx:\n                freq_arr[elem_to_idx[e]] += 1\n\n    # weights\n    cov_weight = 0.6\n    size_weight = 0.4\n    top_k = 5\n\n    scores = []\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        sset = set(s)\n        cov = [e for e in sset if e in rem_set]\n        if cov:\n            freq_vals = np.array([freq_arr[elem_to_idx[e]] for e in cov])\n            # weighted score: sum of freq counts times cov_weight minus size\n            score = freq_vals.sum() * cov_weight - len(s) * size_weight\n            scores.append((score, s))\n        idx += 1\n\n    if not scores:\n        return None\n\n    scores.sort(key=lambda x: x[0], reverse=True)\n    top_k = min(top_k, len(scores))\n    chosen = np.random.choice([s for _, s in scores[:top_k]])\n    return chosen\n\n",
  "rare_to_common_ratio_aug_155": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy heuristic that prefers rare elements while adding a tiny deterministic\n    noise for tie\u2011breaking.  Uses list comprehensions and a median frequency\n    threshold.  Epsilon is added to denominators to avoid division by zero.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Build frequency dictionary for remaining elements\n    elem_freq: Dict[int, int] = {}\n    for subset in remaining_subsets:\n        for e in set(subset):\n            if e in rem_set:\n                elem_freq[e] = elem_freq.get(e, 0) + 1\n\n    fvals = np.array(list(elem_freq.values()), dtype=float)\n    med = float(np.median(fvals)) if fvals.size else 1.0\n\n    best_subset = None\n    best_score = -np.inf\n\n    for subset in remaining_subsets:\n        cov = [e for e in set(subset) if e in rem_set]\n        if len(cov) < 2:                     # skip tiny subsets\n            continue\n\n        rare = sum(1 for e in cov if elem_freq.get(e, 0) <= med)\n        common = len(cov) - rare\n        denom = 1.0 + common + 1e-12          # epsilon\n\n        score = rare / denom + 0.05 * len(cov)\n        # deterministic noise: proportional to the sum of covered elements\n        noise = 1e-6 * sum(cov)\n        score += noise\n\n        # clip to a reasonable range\n        score = np.clip(score, 0.0, 10.0)\n\n        if score > best_score:\n            best_score, best_subset = score, subset\n\n    return best_subset\n\n",
  "rare_to_common_ratio_aug_156": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Uses a while\u2011loop and vectorised frequency calculation via np.unique.\n    Selects among the top\u2011k (k=5) candidates using a weighted score.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Flatten all elements and compute frequencies in one go\n    all_elems = np.concatenate([np.array(sub) for sub in remaining_subsets])\n    mask = np.isin(all_elems, list(rem_set))\n    filtered = all_elems[mask]\n    unique, counts = np.unique(filtered, return_counts=True)\n    elem_freq = dict(zip(unique.tolist(), counts.tolist()))\n\n    # Pre\u2011compute scores for all subsets\n    scores = []\n    idx = 0\n    while idx < len(remaining_subsets):\n        subset = remaining_subsets[idx]\n        cov = [e for e in set(subset) if e in rem_set]\n        if len(cov) < 2:\n            idx += 1\n            continue\n        rare = sum(1 for e in cov if elem_freq.get(e, 0) <= np.median(list(elem_freq.values())))\n        common = len(cov) - rare\n        denom = 1.0 + common + 1e-12\n        score = 0.7 * (rare / denom) + 0.3 * (common / denom)\n        scores.append((score, subset))\n        idx += 1\n\n    if not scores:\n        return None\n\n    # Pick top\u2011k and randomly choose one among them\n    scores.sort(key=lambda x: x[0], reverse=True)\n    top_k = min(5, len(scores))\n    top_subsets = [s for _, s in scores[:top_k]]\n    chosen = np.random.choice(top_subsets)\n    return chosen\n\n",
  "rare_to_common_ratio_aug_157": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Uses np.mean for the frequency threshold, np.max for aggregation,\n    and selects randomly among the best candidates.  Adds a small\n    deterministic noise for tie\u2011breaking.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Frequency dictionary\n    elem_freq: Dict[int, int] = {}\n    for subset in remaining_subsets:\n        for e in set(subset):\n            if e in rem_set:\n                elem_freq[e] = elem_freq.get(e, 0) + 1\n\n    fvals = np.array(list(elem_freq.values()), dtype=float)\n    thresh = float(np.mean(fvals)) if fvals.size else 1.0\n\n    candidates = []\n    for subset in remaining_subsets:\n        cov = [e for e in set(subset) if e in rem_set]\n        if len(cov) < 2:\n            continue\n        rare = sum(1 for e in cov if elem_freq.get(e, 0) <= thresh)\n        common = len(cov) - rare\n        denom = 1.0 + common + 1e-12\n        score = rare / denom + 0.05 * len(cov)\n        # deterministic noise based on subset id\n        noise = 1e-6 * (hash(tuple(subset)) % 1000)\n        score += noise\n        score = np.clip(score, 0.0, 1e3)\n        candidates.append((score, subset))\n\n    if not candidates:\n        return None\n\n    # Choose randomly among the top 3 scores\n    candidates.sort(key=lambda x: x[0], reverse=True)\n    top = min(3, len(candidates))\n    top_subsets = [s for _, s in candidates[:top]]\n    return np.random.choice(top_subsets)\n\n",
  "rare_to_common_ratio_aug_158": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Inverts the usual flow by skipping subsets with insufficient coverage\n    early.  Uses np.max for the rare/common ratio and a fixed weight\n    combination.  Epsilon is added to denominators, and values are clipped.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Build frequency dictionary\n    elem_freq: Dict[int, int] = {}\n    for subset in remaining_subsets:\n        for e in set(subset):\n            if e in rem_set:\n                elem_freq[e] = elem_freq.get(e, 0) + 1\n\n    fvals = np.array(list(elem_freq.values()), dtype=float)\n    med = float(np.median(fvals)) if fvals.size else 1.0\n\n    best_subset = None\n    best_score = -np.inf\n\n    for subset in remaining_subsets:\n        cov = [e for e in set(subset) if e in rem_set]\n        if len(cov) < 2:                     # skip too small subsets\n            continue\n\n        rare = sum(1 for e in cov if elem_freq.get(e, 0) <= med)\n        common = len(cov) - rare\n        denom = 1.0 + common + 1e-12\n        # use np.max to ensure score does not drop below 0\n        ratio = np.max([rare / denom, 0.0])\n        score = 0.6 * ratio + 0.4 * (common / denom)\n        # deterministic noise\n        noise = 1e-6 * sum(cov)\n        score += noise\n        score = np.clip(score, 0.0, 5.0)\n\n        if score > best_score:\n            best_score, best_subset = score, subset\n\n    return best_subset\n\n",
  "intersection_graph_centrality_aug_159": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Prefer sets that touch many other sets via remaining elements (central in the intersection graph).\n    This variation rewrites the control flow using a while-loop and vectorized operations.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    sets = [set(s) for s in remaining_subsets]\n\n    # element -> how many sets contain it (restricted to remaining elements)\n    elem_freq: Dict[int, int] = {}\n    for sset in sets:\n        for e in sset:\n            if e in rem_set:\n                elem_freq[e] = elem_freq.get(e, 0) + 1\n\n    best_subset: Optional[List[int]] = None\n    best_score = -np.inf\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        sset = sets[idx]\n        new_elems = [e for e in sset if e in rem_set]\n        if new_elems:\n            gain = len(new_elems)\n            touch = np.sum([max(0, elem_freq[e] - 1) for e in new_elems])\n            score = gain + 0.15 * touch\n            score = np.clip(score, -1e9, 1e9)  # prevent overflow\n            if score > best_score:\n                best_score = score\n                best_subset = s\n        idx += 1\n    return best_subset\n\n",
  "intersection_graph_centrality_aug_160": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    This version tunes the scoring coefficients and adds deterministic noise for tie\u2011breaking.\n    It also replaces the sum with a mean to change the aggregation behaviour.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    sets = [set(s) for s in remaining_subsets]\n\n    elem_freq: Dict[int, int] = {}\n    for sset in sets:\n        for e in sset:\n            if e in rem_set:\n                elem_freq[e] = elem_freq.get(e, 0) + 1\n\n    best_subset: Optional[List[int]] = None\n    best_score = -np.inf\n    for s, sset in zip(remaining_subsets, sets):\n        new_elems = [e for e in sset if e in rem_set]\n        if not new_elems:\n            continue\n        gain = len(new_elems)\n        touch_vals = [elem_freq[e] - 1 for e in new_elems]\n        touch = np.mean(touch_vals)  # use mean instead of sum\n        # deterministic noise based on the subset content\n        noise = (hash(tuple(s)) % 1000) * 1e-6\n        score = gain + 0.25 * touch + noise\n        score = np.clip(score, -1e9, 1e9)\n        if score > best_score:\n            best_score = score\n            best_subset = s\n    return best_subset\n\n",
  "intersection_graph_centrality_aug_161": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Replaces the greedy argmax with a softmax\u2011based stochastic selection among the top\u2011k candidates.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    sets = [set(s) for s in remaining_subsets]\n\n    elem_freq: Dict[int, int] = {}\n    for sset in sets:\n        for e in sset:\n            if e in rem_set:\n                elem_freq[e] = elem_freq.get(e, 0) + 1\n\n    scores: list[float] = []\n    candidates: list[List[int]] = []\n    for s, sset in zip(remaining_subsets, sets):\n        new_elems = [e for e in sset if e in rem_set]\n        if not new_elems:\n            continue\n        gain = len(new_elems)\n        touch = sum(max(0, elem_freq[e] - 1) for e in new_elems)\n        score = gain + 0.15 * touch\n        scores.append(score)\n        candidates.append(s)\n\n    if not candidates:\n        return None\n\n    # Softmax with temperature and epsilon to avoid division by zero\n    temp = 0.5\n    exp_scores = np.exp(np.array(scores) / temp + 1e-12)\n    probs = exp_scores / (np.sum(exp_scores) + 1e-12)\n\n    # Randomly choose among the top\u2011k candidates\n    top_k = min(3, len(candidates))\n    idxs = np.argpartition(-np.array(scores), top_k - 1)[:top_k]\n    top_candidates = [candidates[i] for i in idxs]\n    top_probs = [probs[i] for i in idxs]\n    top_probs_sum = np.sum(top_probs) + 1e-12\n    top_probs = [p / top_probs_sum for p in top_probs]\n\n    chosen = np.random.choice(top_candidates, p=top_probs)\n    return chosen\n\n",
  "intersection_graph_centrality_aug_162": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Uses a proxy for centrality: the degree of each set in the intersection graph.\n    The adjacency matrix is constructed with NumPy, and degree centrality is used\n    to approximate the touch score.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    sets = [set(s) for s in remaining_subsets]\n    m = len(sets)\n\n    # Build adjacency matrix: 1 if two sets intersect\n    adj = np.zeros((m, m), dtype=int)\n    for i in range(m):\n        for j in range(i + 1, m):\n            if sets[i] & sets[j]:\n                adj[i, j] = adj[j, i] = 1\n    degree = np.sum(adj, axis=1)\n\n    best_subset: Optional[List[int]] = None\n    best_score = -np.inf\n    for idx, (s, sset) in enumerate(zip(remaining_subsets, sets)):\n        new_elems = [e for e in sset if e in rem_set]\n        if not new_elems:\n            continue\n        gain = len(new_elems)\n        touch = degree[idx]  # approximate centrality\n        score = gain + 0.12 * touch\n        score = np.clip(score, -1e9, 1e9)\n        if score > best_score:\n            best_score = score\n            best_subset = s\n    return best_subset\n\n",
  "intersection_graph_sparsity_aug_163": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Vectorised implementation using NumPy.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not remaining_subsets:\n        return None\n\n    # Build a binary matrix: rows = subsets, columns = remaining elements\n    all_elems = list(rem_set)\n    elem_to_idx = {e: i for i, e in enumerate(all_elems)}\n    mat = np.zeros((len(remaining_subsets), len(all_elems)), dtype=int)\n\n    for i, subset in enumerate(remaining_subsets):\n        for e in subset:\n            if e in rem_set:\n                mat[i, elem_to_idx[e]] = 1\n\n    # Count how many remaining subsets contain each element\n    ef_counts = mat.sum(axis=0)                       # shape (num_elements,)\n\n    # Gain = number of new elements each subset would cover\n    gains = mat.sum(axis=1)                           # shape (num_subsets,)\n\n    # Intersect pressure: for each subset sum over elements where ef_counts > 1\n    pressure = np.where(ef_counts > 1, ef_counts - 1, 0)   # shape (num_elements,)\n    intersect_pressure = mat @ pressure                 # shape (num_subsets,)\n\n    # Score = gain - 0.20 * intersect_pressure\n    scores = gains - 0.20 * intersect_pressure\n\n    # Deterministic tie\u2011breaking: tiny noise proportional to subset index\n    noise = np.arange(len(remaining_subsets)) * 1e-9\n    scores += noise\n\n    best_idx = int(np.argmax(scores))\n    if gains[best_idx] == 0:          # no new elements can be covered\n        return None\n    return remaining_subsets[best_idx]\n\n",
  "intersection_graph_sparsity_aug_164": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Uses median pressure, top\u2011k random selection, and deterministic RNG.\n    \"\"\"\n    rem = set(remaining_elements)\n    if not remaining_subsets:\n        return None\n\n    sets = [set(s) for s in remaining_subsets]\n    ef = {}\n    for sset in sets:\n        for e in sset:\n            if e in rem:\n                ef[e] = ef.get(e, 0) + 1\n\n    scores = []\n    for s, sset in zip(remaining_subsets, sets):\n        new = [e for e in sset if e in rem]\n        if not new:\n            continue\n        gain = len(new)\n        pressure = np.median([max(0, ef[e] - 1) for e in new])   # median instead of mean\n        scores.append(gain - 0.25 * pressure)                   # tuned weight\n\n    if not scores:\n        return None\n\n    scores_arr = np.array(scores, dtype=float)\n    sorted_idx = np.argsort(-scores_arr)                       # descending order\n    top_k = min(3, len(scores_arr))\n\n    rng = np.random.default_rng(42)                            # deterministic RNG\n    chosen_idx = sorted_idx[rng.integers(0, top_k)]\n    return remaining_subsets[chosen_idx]\n\n",
  "intersection_graph_sparsity_aug_165": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Inverts the scoring: higher gain and lower pressure are favoured via a reciprocal.\n    \"\"\"\n    rem = set(remaining_elements)\n    if not remaining_subsets:\n        return None\n\n    sets = [set(s) for s in remaining_subsets]\n    ef = {}\n    for sset in sets:\n        for e in sset:\n            if e in rem:\n                ef[e] = ef.get(e, 0) + 1\n\n    best = None\n    best_score = -np.inf\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        sset = sets[idx]\n        new = [e for e in sset if e in rem]\n        if new:\n            gain = len(new)\n            pressure = np.max([max(0, ef[e] - 1) for e in new])   # max instead of sum\n            # Reciprocal penalty with epsilon to avoid division by zero\n            score = gain * (1.0 / (pressure + 1e-12))\n            if score > best_score:\n                best_score, best = score, s\n        idx += 1\n\n    return best\n\n",
  "two_rarest_pivot_pair_aug_166": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorised frequency counting with numpy.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Flatten all remaining subsets and count occurrences of each element\n    flat = np.concatenate([np.array(sub, dtype=int) for sub in remaining_subsets])\n    uniq, counts = np.unique(flat, return_counts=True)\n    freq = {int(u): int(c) for u, c in zip(uniq, counts) if int(u) in rem_set}\n\n    if not freq:\n        return None\n\n    # Sort by increasing frequency (rarest first)\n    sorted_cands = sorted(freq.items(), key=lambda x: x[1])\n    e1 = sorted_cands[0][0]\n    e2 = sorted_cands[1][0] if len(sorted_cands) > 1 else None\n\n    best_subset = None\n    best_gain = -1\n\n    def _gain(sub):\n        return len(rem_set.intersection(sub))\n\n    # Prefer a set that covers both rare elements\n    if e2 is not None:\n        for sub in remaining_subsets:\n            if e1 in sub and e2 in sub:\n                g = _gain(sub)\n                if g > best_gain:\n                    best_gain = g\n                    best_subset = sub\n        if best_subset is not None:\n            return best_subset\n\n    # Fallback: choose a set that contains the single rare element\n    for sub in remaining_subsets:\n        if e1 in sub:\n            g = _gain(sub)\n            if g > best_gain:\n                best_gain = g\n                best_subset = sub\n\n    return best_subset if best_gain > 0 else None\n\n",
  "two_rarest_pivot_pair_aug_167": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"While\u2011loop implementation with deterministic noise for tie\u2011breaking.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    freq: Dict[int, int] = {e: 0 for e in rem}\n    i = 0\n    while i < len(remaining_subsets):\n        sub = remaining_subsets[i]\n        j = 0\n        sub_set = set(sub)\n        while j < len(sub_set):\n            e = sub_set.pop()\n            if e in freq:\n                freq[e] += 1\n            j += 1\n        i += 1\n\n    candidates = [(f, e) for e, f in freq.items() if f > 0]\n    if not candidates:\n        return None\n    candidates.sort(key=lambda x: x[0])\n\n    e1 = candidates[0][1]\n    e2 = candidates[1][1] if len(candidates) > 1 else None\n\n    best = None\n    best_gain = -1\n\n    # Deterministic noise term for tie\u2011breaking\n    def _noise(e: int) -> float:\n        return 1e-9 * e\n\n    if e2 is not None:\n        i = 0\n        while i < len(remaining_subsets):\n            sub = remaining_subsets[i]\n            if e1 in sub and e2 in sub:\n                g = len(rem.intersection(sub))\n                g_adj = g + _noise(e1)\n                if g_adj > best_gain:\n                    best_gain = g_adj\n                    best = sub\n            i += 1\n        if best is not None:\n            return best\n\n    i = 0\n    while i < len(remaining_subsets):\n        sub = remaining_subsets[i]\n        if e1 in sub:\n            g = len(rem.intersection(sub))\n            g_adj = g + _noise(e1)\n            if g_adj > best_gain:\n                best_gain = g_adj\n                best = sub\n        i += 1\n\n    return best if best_gain > 0 else None\n\n",
  "two_rarest_pivot_pair_aug_168": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Soft\u2011max probability selection based on coverage gain.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    freq: Dict[int, int] = {e: 0 for e in rem}\n    for sub in remaining_subsets:\n        for e in set(sub):\n            if e in freq:\n                freq[e] += 1\n\n    candidates = [(f, e) for e, f in freq.items() if f > 0]\n    if not candidates:\n        return None\n    candidates.sort(key=lambda x: x[0])\n\n    e1 = candidates[0][1]\n    e2 = candidates[1][1] if len(candidates) > 1 else None\n\n    # Compute coverage gains for subsets that contain e1\n    gains = np.array([len(rem.intersection(sub)) if e1 in sub else 0\n                      for sub in remaining_subsets], dtype=np.float64)\n\n    # Soft\u2011max probabilities (add epsilon to avoid division by zero)\n    exp_vals = np.exp(gains - np.max(gains))\n    probs = exp_vals / (np.sum(exp_vals) + 1e-12)\n\n    # Randomly pick a subset according to the probabilities\n    idx = np.random.choice(len(remaining_subsets), p=probs)\n    chosen = remaining_subsets[idx]\n    return chosen if len(rem.intersection(chosen)) > 0 else None\n\n",
  "two_rarest_pivot_pair_aug_169": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Select based on coverage ratio with epsilon\u2011regularised denominator.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    freq: Dict[int, int] = {e: 0 for e in rem}\n    for sub in remaining_subsets:\n        for e in set(sub):\n            if e in freq:\n                freq[e] += 1\n\n    candidates = [(f, e) for e, f in freq.items() if f > 0]\n    if not candidates:\n        return None\n    candidates.sort(key=lambda x: x[0])\n\n    e1 = candidates[0][0]\n    e2 = candidates[1][0] if len(candidates) > 1 else None\n\n    best = None\n    best_ratio = -1.0\n\n    def _ratio(sub: List[int]) -> float:\n        return np.clip(len(rem.intersection(sub)) / (len(sub) + 1e-12), 0, 1)\n\n    if e2 is not None:\n        for sub in remaining_subsets:\n            if e1 in sub and e2 in sub:\n                r = _ratio(sub)\n                if r > best_ratio or (np.isclose(r, best_ratio) and len(sub) < len(best)):\n                    best_ratio = r\n                    best = sub\n        if best is not None:\n            return best\n\n    for sub in remaining_subsets:\n        if e1 in sub:\n            r = _ratio(sub)\n            if r > best_ratio or (np.isclose(r, best_ratio) and len(sub) < len(best)):\n                best_ratio = r\n                best = sub\n\n    return best if best_ratio > 0 else None\n\n",
  "pairwise_synergy_low_cooccurrence_aug_170": "import numpy as np\nimport itertools\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Original heuristic with renamed variables, added noise for tie\u2011breaking,\n    and a slightly reduced synergy weight.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    sets = [set(s) for s in remaining_subsets]\n\n    # element frequency over the remaining subsets\n    elem_freq = {}\n    for sset in sets:\n        for e in sset:\n            if e in rem_set:\n                elem_freq[e] = elem_freq.get(e, 0) + 1\n\n    # focus on the rarest elements\n    K = 40\n    rare_elems = sorted(elem_freq.keys(), key=lambda e: elem_freq[e])[:K]\n    rare_set = set(rare_elems)\n\n    # pair frequencies among the rare elements\n    pair_freq = {}\n    for sset in sets:\n        elems = sorted([e for e in sset if e in rare_set])\n        for a, b in itertools.combinations(elems, 2):\n            pair_freq[(a, b)] = pair_freq.get((a, b), 0) + 1\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        elems = [e for e in s if e in rem_set]\n        if not elems:\n            continue\n\n        base = len(elems)\n\n        # synergy bonus: sum 1/(1+pair_freq) with a small epsilon\n        bonus = 0.0\n        elems2 = [e for e in elems if e in rare_set][:18]\n        for a, b in itertools.combinations(sorted(elems2), 2):\n            freq = pair_freq.get((a, b), 0) + 1e-12\n            bonus += 1.0 / (1.0 + freq)\n\n        score = base + 0.8 * bonus\n        # deterministic tie\u2011breaking with tiny random noise\n        score += np.random.uniform(0, 1e-6)\n\n        if score > best_score:\n            best_score, best = score, s\n\n    return best\n\n",
  "pairwise_synergy_low_cooccurrence_aug_171": "import numpy as np\nimport itertools\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Vectorised variant: uses numpy for frequency counting,\n    replaces the greedy choice with a soft\u2011min over the top\u2011k candidates,\n    and averages pair frequencies instead of summing.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    sets = [set(s) for s in remaining_subsets]\n\n    # element frequency using a numpy array for speed\n    max_elem = max(remaining_elements) if remaining_elements else 0\n    elem_freq = np.zeros(max_elem + 1, dtype=int)\n    for sset in sets:\n        for e in sset:\n            if e in rem_set:\n                elem_freq[e] += 1\n\n    K = 50\n    rare_elems = np.argsort(elem_freq)[:K]\n    rare_set = set(rare_elems)\n\n    # pair frequencies among rare elements\n    pair_freq = {}\n    for sset in sets:\n        elems = sorted([e for e in sset if e in rare_set])\n        for a, b in itertools.combinations(elems, 2):\n            pair_freq[(a, b)] = pair_freq.get((a, b), 0) + 1\n\n    # compute a score for each candidate\n    scores = []\n    for s in remaining_subsets:\n        elems = [e for e in s if e in rem_set]\n        if not elems:\n            scores.append(-np.inf)\n            continue\n\n        base = len(elems)\n        elems2 = [e for e in elems if e in rare_set][:20]\n        freq_list = [pair_freq.get((a, b), 0) + 1e-12\n                     for a, b in itertools.combinations(sorted(elems2), 2)]\n        bonus = np.mean(freq_list) if freq_list else 0.0\n        scores.append(base + 0.85 * bonus)\n\n    scores = np.array(scores)\n    # soft\u2011min over the top\u2011k candidates\n    top_k = 3\n    top_idx = np.argsort(-scores)[:top_k]\n    probs = np.exp(-scores[top_idx])\n    probs /= probs.sum()\n    chosen = np.random.choice(top_idx, p=probs)\n    return remaining_subsets[chosen]\n\n",
  "pairwise_synergy_low_cooccurrence_aug_172": "import numpy as np\nimport itertools\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    While\u2011loop implementation with a proxy approximation for pair frequencies.\n    The pair frequencies are estimated from a small random sample of each set.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    sets = [set(s) for s in remaining_subsets]\n\n    # element frequency\n    elem_freq = {}\n    for sset in sets:\n        for e in sset:\n            if e in rem_set:\n                elem_freq[e] = elem_freq.get(e, 0) + 1\n\n    K = 30\n    rare_elems = sorted(elem_freq.keys(), key=lambda e: elem_freq[e])[:K]\n    rare_set = set(rare_elems)\n\n    # proxy pair frequencies via random sampling\n    pair_freq = {}\n    for sset in sets:\n        elems = [e for e in sset if e in rare_set]\n        np.random.shuffle(elems)\n        sample = elems[:min(10, len(elems))]\n        for a, b in itertools.combinations(sample, 2):\n            pair_freq[(a, b)] = pair_freq.get((a, b), 0) + 1\n\n    best, best_score = None, -np.inf\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        elems = [e for e in s if e in rem_set]\n        if elems:\n            base = len(elems)\n            elems2 = [e for e in elems if e in rare_set][:15]\n            bonus = 0.0\n            for a, b in itertools.combinations(sorted(elems2), 2):\n                freq = pair_freq.get((a, b), 0) + 1e-12\n                bonus += 1.0 / (1.0 + freq)\n            score = base + 0.75 * bonus\n            if score > best_score:\n                best_score, best = score, s\n        idx += 1\n\n    return best\n\n",
  "pairwise_synergy_low_cooccurrence_aug_173": "import numpy as np\nimport itertools\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Generator\u2011expression based rewrite with deterministic tie\u2011breaking.\n    The synergy bonus is clipped to avoid overflow, and the best\n    subset is chosen by index when scores are equal.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    sets = [set(s) for s in remaining_subsets]\n\n    # element frequency via generator expression\n    elem_freq = {e: sum(e in s for s in sets) for e in rem_set}\n\n    K = 35\n    rare_elems = sorted(elem_freq, key=elem_freq.get)[:K]\n    rare_set = set(rare_elems)\n\n    # pair frequencies\n    pair_freq = {}\n    for sset in sets:\n        elems = [e for e in sset if e in rare_set]\n        for a, b in itertools.combinations(sorted(elems), 2):\n            pair_freq[(a, b)] = pair_freq.get((a, b), 0) + 1\n\n    best, best_score, best_idx = None, -np.inf, -1\n    for idx, s in enumerate(remaining_subsets):\n        elems = [e for e in s if e in rem_set]\n        if not elems:\n            continue\n\n        base = len(elems)\n        elems2 = [e for e in elems if e in rare_set][:17]\n        bonus = sum(1.0 / (1.0 + pair_freq.get((a, b), 0) + 1e-12)\n                    for a, b in itertools.combinations(sorted(elems2), 2))\n        score = np.clip(base + 0.85 * bonus, 0, None)\n\n        # deterministic tie\u2011breaking by smallest index\n        if score > best_score or (score == best_score and idx < best_idx):\n            best_score, best, best_idx = score, s, idx\n\n    return best\n\n",
  "top_gain_smallest_size_percentile_aug_174": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Choose a subset with the highest uncovered element gain.\n    Randomly picks one of the top\u2011p candidates, adding deterministic noise\n    for tie\u2011breaking.\"\"\"\n    rem_set = set(remaining_elements)\n\n    # Build a list of (gain, size, subset) tuples\n    scored = [(len(rem_set & set(s)), len(s), s) for s in remaining_subsets]\n    scored = [t for t in scored if t[0] > 0]\n    if not scored:\n        return None\n\n    # Sort by gain (descending) then by size (ascending)\n    scored.sort(key=lambda x: x[0], reverse=True)\n\n    # Hyper\u2011parameter: take the top 20\u202f% of candidates\n    p = 0.20\n    top_k = max(1, int(np.ceil(p * len(scored))))\n    top = scored[:top_k]\n\n    # Add deterministic noise (based on the index in the top list)\n    noisy = [(g, sz, s, 1e-6 * idx) for idx, (g, sz, s) in enumerate(top)]\n\n    # Randomly pick one of the top candidates\n    chosen = np.random.choice(len(noisy))\n    return noisy[chosen][2]\n\n",
  "top_gain_smallest_size_percentile_aug_175": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Select subset by softmin weighting on the uncovered element gain.\"\"\"\n    rem_arr = np.array(remaining_elements, dtype=int)\n\n    gains = []\n    sizes = []\n    indices = []\n    i = 0\n    while i < len(remaining_subsets):\n        s = remaining_subsets[i]\n        s_arr = np.array(s, dtype=int)\n        inter_len = len(np.intersect1d(rem_arr, s_arr, assume_unique=False))\n        if inter_len > 0:\n            gains.append(inter_len)\n            sizes.append(len(s))\n            indices.append(i)\n        i += 1\n\n    if not gains:\n        return None\n\n    gains = np.array(gains, dtype=float)\n\n    # Softmin weighting (larger gain \u2192 smaller weight)\n    exp_vals = np.exp(-gains + 1e-12)          # epsilon to avoid overflow\n    weights = exp_vals / (np.sum(exp_vals) + 1e-12)  # epsilon for division\n    weights = np.clip(weights, 0, 1)\n\n    chosen = np.argmin(weights)\n    return remaining_subsets[indices[chosen]]\n\n",
  "top_gain_smallest_size_percentile_aug_176": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Median\u2011based tie\u2011breaking with deterministic noise.\"\"\"\n    rem = set(remaining_elements)\n\n    # Score each subset: (gain, size, subset)\n    scored = [(len(rem & set(s)), len(s), s) for s in remaining_subsets]\n    scored = [t for t in scored if t[0] > 0]\n    if not scored:\n        return None\n\n    # Sort by descending gain\n    scored.sort(key=lambda x: x[0], reverse=True)\n\n    # Hyper\u2011parameter: take a fixed top\u2011k slice\n    p = 0.15\n    top_k = max(1, int(np.ceil(p * len(scored))) + 2)  # roughly 7 for typical sizes\n    top = scored[:top_k]\n\n    # Deterministic noise based on subset content\n    noisy = []\n    for g, sz, s in top:\n        noise = 1e-8 * (hash(tuple(s)) % 1000)  # deterministic, small\n        noisy.append((g + noise, sz, s))\n\n    # Use the median of the noisy scores to rank\n    median_val = np.median([ns[0] for ns in noisy])\n    idx = np.argmin([abs(ns[0] - median_val) for ns in noisy])\n    return noisy[idx][2]\n\n",
  "top_gain_smallest_size_percentile_aug_177": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorised gain\u2011to\u2011size ratio ranking with random top\u2011k selection.\"\"\"\n    rem_arr = np.array(remaining_elements, dtype=int)\n\n    gains = np.empty(len(remaining_subsets), dtype=float)\n    sizes = np.empty(len(remaining_subsets), dtype=int)\n\n    for idx, s in enumerate(remaining_subsets):\n        s_arr = np.array(s, dtype=int)\n        gains[idx] = len(np.intersect1d(rem_arr, s_arr, assume_unique=False))\n        sizes[idx] = len(s)\n\n    mask = gains > 0\n    if not np.any(mask):\n        return None\n\n    gains = gains[mask]\n    sizes = sizes[mask]\n    # Ratio of gain to size, with epsilon to avoid division by zero\n    scores = gains / (sizes + 1e-12)\n    scores = np.clip(scores, 0, 1)\n\n    # Hyper\u2011parameter: top\u201125\u202f% of candidates\n    p = 0.25\n    top_k = max(1, int(np.ceil(p * len(scores))))\n    top_indices = np.argpartition(-scores, top_k - 1)[:top_k]\n\n    # Randomly choose among the top candidates\n    chosen_local = np.random.choice(top_indices)\n\n    # Map back to the original subset list\n    original_indices = np.where(mask)[0]\n    return remaining_subsets[original_indices[chosen_local]]\n\n",
  "smallest_percentile_then_max_gain_aug_178": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Select the next subset using a vectorized candidate filtering approach.\"\"\"\n    rem_set = set(remaining_elements)\n    # Candidate list: (subset size, uncovered gain, subset)\n    candidates = [(len(s), len(rem_set & set(s)), s)\n                  for s in remaining_subsets\n                  if len(rem_set & set(s)) > 0]\n    if not candidates:\n        return None\n\n    # Sort by subset size (ascending)\n    candidates.sort(key=lambda x: x[0])\n\n    # Compute k with epsilon and clip to valid range\n    p = 0.20\n    k = int(np.ceil(p * len(candidates) + 1e-12))\n    k = np.clip(k, 1, len(candidates))\n\n    small = candidates[:k]\n    # Sort by uncovered gain descending, then by size ascending\n    small.sort(key=lambda x: (-x[1], x[0]))\n    return small[0][2]\n\n",
  "smallest_percentile_then_max_gain_aug_179": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Weighted scoring with adjustable top_k threshold.\"\"\"\n    rem_set = set(remaining_elements)\n    weight_gain = 0.6\n    weight_size = 0.4\n    total_elem = len(remaining_elements) + 1e-12\n\n    candidates = []\n    for s in remaining_subsets:\n        g = len(rem_set & set(s))\n        if g == 0:\n            continue\n        size = len(s) + 1e-12\n        # Scoring combines uncovered gain and subset size\n        score = (weight_gain * g) / size + (weight_size * size) / total_elem\n        candidates.append((score, s))\n\n    if not candidates:\n        return None\n\n    # Sort by score descending\n    candidates.sort(key=lambda x: -x[0])\n\n    top_k = 7\n    top_k = np.clip(top_k, 1, len(candidates))\n    top_candidates = candidates[:top_k]\n\n    # Deterministic tie\u2011breaking using hash\u2011based noise\n    best = top_candidates[0]\n    for cand in top_candidates[1:]:\n        if cand[0] > best[0] + 1e-12:\n            best = cand\n    return best[1]\n\n",
  "smallest_percentile_then_max_gain_aug_180": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Softmin selection with deterministic random choice among top\u2011k.\"\"\"\n    rem_set = set(remaining_elements)\n    candidates = []\n    for s in remaining_subsets:\n        g = len(rem_set & set(s))\n        if g == 0:\n            continue\n        size = len(s) + 1e-12\n        score = g / size\n        # Deterministic noise for tie\u2011breaking\n        noise = (hash(tuple(s)) % 1000) * 1e-6\n        candidates.append((score + noise, s))\n\n    if not candidates:\n        return None\n\n    # Sort by score descending\n    candidates.sort(key=lambda x: -x[0])\n\n    top_k = 5\n    top_k = np.clip(top_k, 1, len(candidates))\n    top_candidates = candidates[:top_k]\n\n    # Softmin probabilities\n    temperature = 0.05\n    scores = np.array([c[0] for c in top_candidates])\n    exp_vals = np.exp(-scores / (temperature + 1e-12))\n    probs = exp_vals / (np.sum(exp_vals) + 1e-12)\n\n    # Deterministic RNG seed derived from the remaining subsets\n    seed = hash(tuple(tuple(sub) for sub in remaining_subsets)) & 0xffffffff\n    rng = np.random.default_rng(seed)\n    idx = rng.choice(len(top_candidates), p=probs)\n    return top_candidates[idx][1]\n\n",
  "smallest_percentile_then_max_gain_aug_181": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Proxy\u2011based scoring using mean and max, with random top\u2011k selection.\"\"\"\n    rem_set = set(remaining_elements)\n    total_elem = len(remaining_elements) + 1e-12\n\n    candidates = []\n    for s in remaining_subsets:\n        g = len(rem_set & set(s))\n        if g == 0:\n            continue\n        size = len(s) + 1e-12\n        # Proxy: mean of uncovered indices divided by max element value\n        mean_uncovered = np.mean(list(rem_set & set(s)) + [0])  # avoid empty list\n        max_elem = max(remaining_elements) + 1e-12\n        proxy = (mean_uncovered / max_elem) * (size / total_elem)\n        candidates.append((proxy, s))\n\n    if not candidates:\n        return None\n\n    # Randomly select among top_k candidates\n    top_k = 5\n    top_k = np.clip(top_k, 1, len(candidates))\n    candidates.sort(key=lambda x: -x[0])\n    top_candidates = candidates[:top_k]\n\n    rng = np.random.default_rng(42)  # fixed seed for reproducibility\n    idx = rng.integers(len(top_candidates))\n    return top_candidates[idx][1]\n\n",
  "adaptive_quantile_explore_exploit_aug_182": "import numpy as np\nimport random\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    sel_subs = selected_subsets\n    rem_subs = remaining_subsets\n    rem_elems = remaining_elements\n\n    rem_set = set(rem_elems)\n    scored = []\n    for sub in rem_subs:\n        cov = len(rem_set & set(sub))\n        if cov > 0:\n            scored.append((cov, sub))\n    if not scored:\n        return None\n\n    n_rem = max(1, len(rem_set))\n    # Compute total elements that have already been covered\n    all_covered = set()\n    for ss in sel_subs:\n        all_covered.update(ss)\n    n_total = n_rem + len(all_covered)\n\n    # Progress metric with epsilon to avoid division by zero\n    progress = 1.0 - (n_rem / (n_total + 1e-12))\n\n    # Deterministic noise for tie\u2011breaking\n    noise = np.arange(len(scored)) * 1e-6\n    scored = [(g + n, s) for (g, s), n in zip(scored, noise)]\n\n    scored.sort(key=lambda x: x[0], reverse=True)\n    # Quantile shrinks from 40\u202f% early to 10\u202f% late, clipped to safe bounds\n    q = np.clip(0.40 - 0.30 * progress, 0.05, 0.45)\n    k = max(1, int(np.ceil(q * len(scored))))\n    candidates = [s for _, s in scored[:k]]\n    return random.choice(candidates)\n\n",
  "adaptive_quantile_explore_exploit_aug_183": "import numpy as np\nimport random\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    sel_subs = selected_subsets\n    rem_subs = remaining_subsets\n    rem_elems = remaining_elements\n\n    rem_set = set(rem_elems)\n    scored = []\n    for sub in rem_subs:\n        cov = len(rem_set & set(sub))\n        if cov > 0:\n            # Weighted score: prioritize uncovered elements, penalize large subsets\n            score = cov * 0.7 - 0.3 * len(sub)\n            scored.append((score, sub))\n    if not scored:\n        return None\n\n    n_rem = max(1, len(rem_set))\n    all_covered = set()\n    for ss in sel_subs:\n        all_covered.update(ss)\n    n_total = n_rem + len(all_covered)\n\n    progress = 1.0 - (n_rem / (n_total + 1e-12))\n\n    # Small deterministic noise to break ties\n    noise = np.linspace(-1e-6, 1e-6, len(scored))\n    scored = [(sc + n, s) for (sc, s), n in zip(scored, noise)]\n\n    scored.sort(key=lambda x: x[0], reverse=True)\n    # Fixed top\u2011k selection\n    top_k = 7\n    k = min(top_k, len(scored))\n    candidates = [s for _, s in scored[:k]]\n    return random.choice(candidates)\n\n",
  "adaptive_quantile_explore_exploit_aug_184": "import numpy as np\nimport random\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    sel_subs = selected_subsets\n    rem_subs = remaining_subsets\n    rem_elems = remaining_elements\n\n    rem_set = set(rem_elems)\n    scores = []\n    for sub in rem_subs:\n        cov = len(rem_set & set(sub))\n        if cov > 0:\n            scores.append((cov, sub))\n    if not scores:\n        return None\n\n    n_rem = max(1, len(rem_set))\n    all_covered = set()\n    for ss in sel_subs:\n        all_covered.update(ss)\n    n_total = n_rem + len(all_covered)\n\n    progress = 1.0 - (n_rem / (n_total + 1e-12))\n\n    # Softmin selection using a temperature parameter\n    beta = 5.0\n    g_vals = np.array([g for g, _ in scores], dtype=np.float64)\n    shift = np.max(g_vals)  # numerical stability\n    exp_vals = np.exp(-(g_vals - shift) * beta)\n    probs = exp_vals / (np.sum(exp_vals) + 1e-12)\n\n    idx = np.random.choice(len(scores), p=probs)\n    return scores[idx][1]\n\n",
  "adaptive_quantile_explore_exploit_aug_185": "import numpy as np\nimport random\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    sel_subs = selected_subsets\n    rem_subs = remaining_subsets\n    rem_elems = remaining_elements\n\n    rem_set = set(rem_elems)\n    scored = []\n    for sub in rem_subs:\n        sub_set = set(sub)\n        inter = rem_set & sub_set\n        if inter:\n            union = rem_set | sub_set\n            # Jaccard similarity as a proxy score\n            jaccard = len(inter) / (len(union) + 1e-12)\n            scored.append((jaccard, sub))\n    if not scored:\n        return None\n\n    n_rem = max(1, len(rem_set))\n    all_covered = set()\n    for ss in sel_subs:\n        all_covered.update(ss)\n    n_total = n_rem + len(all_covered)\n\n    progress = 1.0 - (n_rem / (n_total + 1e-12))\n\n    # Fixed top\u2011k selection\n    top_k = 5\n    scored.sort(key=lambda x: x[0], reverse=True)\n    k = min(top_k, len(scored))\n    top_candidates = [s for _, s in scored[:k]]\n\n    # Probability proportional to similarity\n    sims = np.array([s[0] for s in scored[:k]], dtype=np.float64)\n    probs = sims / (np.sum(sims) + 1e-12)\n    idx = np.random.choice(len(top_candidates), p=probs)\n    return top_candidates[idx]\n\n",
  "three_phase_core_middle_fringe_aug_186": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Frequency counting using a while\u2011loop\n    freq: Dict[int, int] = {}\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n        idx += 1\n\n    n_rem = max(1, len(rem))\n    seen = {e for ss in selected_subsets for e in ss}\n    n_total = n_rem + len(seen)\n    progress = 1.0 - (n_rem / max(1, n_total + 1e-12))\n    progress = np.clip(progress, 0.0, 1.0)\n\n    fvals = np.array(list(freq.values()), dtype=float)\n    q1 = float(np.quantile(fvals, 0.33)) if fvals.size else 1.0\n    q2 = float(np.quantile(fvals, 0.66)) if fvals.size else 1.0\n\n    best, best_score = None, -1e18\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        new = [e for e in set(s) if e in rem]\n        if not new:\n            idx += 1\n            continue\n\n        if progress < 0.33:\n            sc = sum(1 for e in new if freq.get(e, 0) >= q2)\n            score = 3.0 * sc + 0.05 * len(new)\n        elif progress < 0.66:\n            score = 1.5 * len(new)\n        else:\n            sc = sum(1 for e in new if freq.get(e, 0) <= q1)\n            score = 3.0 * sc + 0.05 * len(new)\n\n        # Deterministic tie\u2011breaking noise\n        noise = 1e-6 * ((hash(tuple(s)) & 0xFFFF) / 0xFFFF)\n        score += noise\n\n        if score > best_score:\n            best_score, best = score, s\n        idx += 1\n    return best\n\n",
  "three_phase_core_middle_fringe_aug_187": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Frequency counting with list comprehensions\n    freq: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    n_rem = max(1, len(rem))\n    seen = {e for ss in selected_subsets for e in ss}\n    n_total = n_rem + len(seen)\n    progress = 1.0 - (n_rem / max(1, n_total + 1e-12))\n    progress = np.clip(progress, 0.0, 1.0)\n\n    fvals = np.array(list(freq.values()), dtype=float)\n    q1 = float(np.median(fvals)) if fvals.size else 1.0\n    q2 = float(np.percentile(fvals, 75)) if fvals.size else 1.0\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        new = list(set(s) & rem)\n        if not new:\n            continue\n\n        if progress >= 0.66:\n            sc = sum(1 for e in new if freq.get(e, 0) <= q1)\n            score = 2.5 * sc + 0.2 * len(new)\n        elif progress >= 0.33:\n            score = 1.2 * len(new) + 0.1 * np.mean([freq.get(e, 0) for e in new])\n        else:\n            sc = sum(1 for e in new if freq.get(e, 0) >= q2)\n            score = 2.8 * sc + 0.15 * len(new)\n\n        # Deterministic noise for tie\u2011breaking\n        noise = 1e-7 * ((hash(tuple(sorted(s))) & 0xFFFFF) / 0xFFFFF)\n        score += noise\n\n        score = np.clip(score, -1e12, 1e12)\n        if score > best_score:\n            best_score, best = score, s\n    return best\n\n",
  "three_phase_core_middle_fringe_aug_188": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    freq: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    n_rem = max(1, len(rem))\n    seen = {e for ss in selected_subsets for e in ss}\n    n_total = n_rem + len(seen)\n    progress = 1.0 - (n_rem / max(1, n_total + 1e-12))\n    progress = np.clip(progress, 0.0, 1.0)\n\n    fvals = np.array(list(freq.values()), dtype=float)\n    if fvals.size:\n        q1, q2 = np.percentile(fvals, [25, 75])\n    else:\n        q1 = q2 = 1.0\n\n    scores = []\n    for s in remaining_subsets:\n        new = list(set(s) & rem)\n        if not new:\n            continue\n\n        if progress < 0.33:\n            sc = sum(1 for e in new if freq.get(e, 0) >= q2)\n            score = 1.8 * sc + 0.12 * len(new)\n        elif progress < 0.66:\n            score = 1.0 * len(new)\n        else:\n            sc = sum(1 for e in new if freq.get(e, 0) <= q1)\n            score = 1.8 * sc + 0.12 * len(new)\n        scores.append((score, s))\n\n    if not scores:\n        return None\n\n    scores_arr = np.array([s[0] for s in scores])\n    weights = np.exp(-scores_arr)          # softmin weighting\n    weights = weights / (weights.sum() + 1e-12)\n\n    seed = abs(hash(tuple(tuple(ss) for ss in selected_subsets)))\n    rng = np.random.default_rng(seed)\n    chosen_idx = rng.choice(len(scores), p=weights)\n    return scores[chosen_idx][1]\n\n",
  "three_phase_core_middle_fringe_aug_189": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    freq: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    n_rem = max(1, len(rem))\n    seen = {e for ss in selected_subsets for e in ss}\n    n_total = n_rem + len(seen)\n    progress = 1.0 - (n_rem / max(1, n_total + 1e-12))\n    progress = np.clip(progress, 0.0, 1.0)\n\n    fvals = np.array(list(freq.values()), dtype=float)\n    if fvals.size:\n        q1 = np.percentile(fvals, 20)\n        q2 = np.percentile(fvals, 80)\n    else:\n        q1 = q2 = 1.0\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        new = list(set(s) & rem)\n        if not new:\n            continue\n\n        if progress < 0.33:\n            sc = sum(1 for e in new if freq.get(e, 0) >= q2)\n            score = 2.0 * sc + 0.08 * len(new)\n        elif progress < 0.66:\n            score = np.max([len(new), 1])            # proxy: use maximum of len and 1\n        else:\n            sc = sum(1 for e in new if freq.get(e, 0) <= q1)\n            score = 2.0 * sc + 0.08 * len(new)\n\n        score = np.clip(score, -1e12, 1e12)\n        if score > best_score:\n            best_score, best = score, s\n    return best\n\n",
  "criticality_by_min_set_size_aug_190": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorised implementation with deterministic noise for tie\u2011breaking.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Smallest subset size that can cover each remaining element\n    min_sz: Dict[int, float] = {e: np.inf for e in rem_set}\n    for s in remaining_subsets:\n        sz = len(s)\n        for e in s:\n            if e in rem_set and sz < min_sz[e]:\n                min_sz[e] = sz\n\n    best, best_score = None, -np.inf\n    noise = np.arange(len(remaining_subsets)) * 1e-6   # deterministic tie\u2011breaker\n\n    for idx, s in enumerate(remaining_subsets):\n        new = [e for e in s if e in rem_set]\n        if not new:\n            continue\n        # 1 / min_size for each new element, clipped to avoid div\u2011by\u2011zero\n        score = sum(1.0 / np.clip(min_sz[e], 1e-12, None) for e in new)\n        score += 0.05 * len(new) + noise[idx]          # small nudge + noise\n        if score > best_score:\n            best_score, best = score, s\n    return best\n\n",
  "criticality_by_min_set_size_aug_191": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Uses a while\u2011loop and adjusted weighting constants.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Compute minimal covering subset size for each remaining element\n    min_sz: Dict[int, float] = {e: np.inf for e in rem_set}\n    for s in remaining_subsets:\n        sz = len(s)\n        for e in s:\n            if e in rem_set and sz < min_sz[e]:\n                min_sz[e] = sz\n\n    best, best_score = None, -np.inf\n    i = 0\n    while i < len(remaining_subsets):\n        s = remaining_subsets[i]\n        new = [e for e in s if e in rem_set]\n        if new:\n            # 0.6 * inverse\u2011size sum + 0.4 * coverage count\n            score = 0.6 * sum(1.0 / np.clip(min_sz[e], 1e-12, None) for e in new) \\\n                    + 0.4 * len(new)\n            if score > best_score:\n                best_score, best = score, s\n        i += 1\n    return best\n\n",
  "criticality_by_min_set_size_aug_192": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Randomly selects among the top\u2011k subsets based on the maximum inverse\u2011size.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Minimal covering subset size per element\n    min_sz: Dict[int, float] = {e: np.inf for e in rem_set}\n    for s in remaining_subsets:\n        sz = len(s)\n        for e in s:\n            if e in rem_set and sz < min_sz[e]:\n                min_sz[e] = sz\n\n    scores = []\n    for s in remaining_subsets:\n        new = [e for e in s if e in rem_set]\n        if not new:\n            scores.append(-np.inf)\n            continue\n        # Use the maximum 1/min_size among new elements\n        max_inv = max(1.0 / np.clip(min_sz[e], 1e-12, None) for e in new)\n        scores.append(max_inv + 0.05 * len(new))\n\n    scores_np = np.array(scores)\n    top_k = min(5, len(remaining_subsets))\n    top_indices = np.argpartition(-scores_np, top_k-1)[:top_k]\n    chosen = np.random.choice(top_indices)\n    return remaining_subsets[chosen]\n\n",
  "criticality_by_min_set_size_aug_193": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Chooses a subset from the top\u2011k based on the sum of inverse sizes.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Smallest covering subset size for each element\n    min_sz: Dict[int, float] = {e: np.inf for e in rem_set}\n    for s in remaining_subsets:\n        sz = len(s)\n        for e in s:\n            if e in rem_set and sz < min_sz[e]:\n                min_sz[e] = sz\n\n    scores = []\n    for s in remaining_subsets:\n        new = [e for e in s if e in rem_set]\n        if new:\n            inv_sum = sum(1.0 / np.clip(min_sz[e], 1e-12, None) for e in new)\n            scores.append(inv_sum + 0.05 * len(new))\n        else:\n            scores.append(-np.inf)\n\n    scores_np = np.array(scores)\n    top_k = min(3, len(remaining_subsets))\n    top_indices = np.argpartition(-scores_np, top_k-1)[:top_k]\n    chosen = np.random.choice(top_indices)\n    return remaining_subsets[chosen]\n\n",
  "reserve_index_weighting_aug_194": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Variant 1 \u2013 vector\u2011friendly implementation with deterministic tie\u2011breaking\n    and a slightly tweaked scoring formula.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # ----- support & average size calculation -----\n    support: Dict[int, int] = {e: 0 for e in rem_set}\n    size_sum: Dict[int, int] = {e: 0 for e in rem_set}\n\n    for s in remaining_subsets:\n        ss = set(s)\n        sz = len(ss)\n        for e in ss:\n            if e in support:\n                support[e] += 1\n                size_sum[e] += sz\n\n    # ----- scoring -----\n    rng = np.random.default_rng(seed=42)          # deterministic noise generator\n    best, best_score = None, -np.inf\n    for s in remaining_subsets:\n        ss = set(s)\n        new_elems = [e for e in ss if e in rem_set]\n        if not new_elems:\n            continue\n\n        score = 0.0\n        for e in new_elems:\n            sup = support[e] + 1e-12                # epsilon to avoid div\u2011by\u2011zero\n            avg_sz = size_sum[e] / sup\n            score += (1.0 / sup) * (1.0 / (avg_sz + 1e-12))\n\n        score += 0.01 * len(new_elems)              # tweaked weight\n        score += rng.uniform(0, 1e-6)               # deterministic tie\u2011breaker\n\n        if score > best_score:\n            best_score, best = score, s\n\n    return best\n\n",
  "reserve_index_weighting_aug_195": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Variant 4 \u2013 proxy approximations, np.max aggregation, and tweaked hyper\u2011parameters.\n    \"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # ----- support & size sums using numpy arrays -----\n    support = {e: 0 for e in rem}\n    size_sum = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        ss = set(s)\n        sz = len(ss)\n        for e in ss:\n            if e in support:\n                support[e] += 1\n                size_sum[e] += sz\n\n    # ----- compute scores -----\n    rng = np.random.default_rng(seed=99)\n    scores = []\n    for s in remaining_subsets:\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            scores.append(-np.inf)\n            continue\n        sc = np.max([(1.0 / (support[e] + 1e-12)) * (1.0 / ((size_sum[e] / (support[e] + 1e-12)) + 1e-12))\n                     for e in new])\n        sc += 0.03 * len(new)          # heavier weight on new elements\n        sc += rng.uniform(0, 1e-6)     # deterministic tie\u2011breaker\n        scores.append(sc)\n\n    scores = np.array(scores)\n    best_idx = np.argmax(scores)\n    return remaining_subsets[best_idx]\n\n",
  "maximize_gain_to_remaining_ratio_aug_196": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Greedy selection with deterministic noise for tie\u2011breaking.\n    Uses a while loop and vectorised clipping.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    denom = float(max(1, len(rem_set))) + 1e-12          # epsilon to avoid div\u2011by\u2011zero\n\n    best_subset = None\n    best_score = -np.inf\n    idx = 0\n    while idx < len(remaining_subsets):\n        subset = remaining_subsets[idx]\n        gain = len(rem_set.intersection(subset))\n        if gain > 0:\n            score = np.clip(gain / denom, 0.0, 1.0)        # bound score\n            # deterministic noise: based on subset contents\n            noise = (sum(subset) % 1000) * 1e-6\n            score += noise\n            if score > best_score:\n                best_score = score\n                best_subset = subset\n        idx += 1\n    return best_subset\n\n",
  "maximize_gain_to_remaining_ratio_aug_197": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Selects a subset from the top\u2011k highest scores and picks one at random.\n    Utilises list comprehension and soft selection.\n    \"\"\"\n    rem = set(remaining_elements)\n    denom_eps = float(max(1, len(rem))) + 1e-12\n\n    # Compute scores for all eligible subsets\n    scores = [(np.clip(len(rem.intersection(s)) / denom_eps, 0.0, 1.0), s)\n              for s in remaining_subsets\n              if len(rem.intersection(s)) > 0]\n\n    if not scores:\n        return None\n\n    # Sort descending by score\n    scores.sort(key=lambda x: x[0], reverse=True)\n\n    top_k = min(5, len(scores))\n    top_subsets = [s for _, s in scores[:top_k]]\n\n    # Randomly pick one of the top\u2011k subsets\n    chosen = np.random.choice(top_subsets)\n    return chosen\n\n",
  "maximize_gain_to_remaining_ratio_aug_198": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Uses median gain as a threshold and selects the best ratio above it.\n    Demonstrates semantic\u2011preserving aggregation.\n    \"\"\"\n    rem = set(remaining_elements)\n    denom = float(max(1, len(rem))) + 1e-12\n\n    # Gather gains for all eligible subsets\n    gains = np.array([len(rem.intersection(s)) for s in remaining_subsets\n                      if len(rem.intersection(s)) > 0])\n\n    if gains.size == 0:\n        return None\n\n    median_gain = np.median(gains)\n\n    best_subset = None\n    best_score = -np.inf\n    for s in remaining_subsets:\n        gain = len(rem.intersection(s))\n        if gain <= 0 or gain < median_gain:\n            continue\n        score = np.clip(gain / denom, 0.0, 1.0)\n        if score > best_score:\n            best_score = score\n            best_subset = s\n\n    return best_subset\n\n",
  "maximize_gain_to_remaining_ratio_aug_199": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Selects the subset with the maximum clipped score, adding deterministic noise\n    for tie\u2011breaking. Uses max aggregation and inverted conditional logic.\n    \"\"\"\n    rem = set(remaining_elements)\n    denom = float(max(1, len(rem))) + 1e-12\n\n    # Build list of (score, subset) tuples\n    score_subset_pairs = []\n    for s in remaining_subsets:\n        gain = len(rem.intersection(s))\n        if gain <= 0:\n            continue\n        score = np.clip(gain / denom, 0.0, 1.0)\n        # deterministic noise: depends on subset length and sum\n        noise = (len(s) * 3 + sum(s)) % 7 * 1e-6\n        score_subset_pairs.append((score + noise, s))\n\n    if not score_subset_pairs:\n        return None\n\n    # Pick the subset with the highest noisy score\n    best_subset = max(score_subset_pairs, key=lambda x: x[0])[1]\n    return best_subset\n\n",
  "expected_steps_proxy_minimize_aug_200": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Greedy heuristic with deterministic tie\u2011breaking and safe denominators.\"\"\"\n    rem_elements = set(remaining_elements)\n    if not rem_elements:\n        return None\n\n    rem_sets = [set(sub) for sub in remaining_subsets]\n    best_idx = None\n    best_score = -np.inf\n\n    for idx, s in enumerate(rem_sets):\n        gain = len(rem_elements & s)\n        if gain == 0:\n            continue\n\n        # If this subset covers everything, we are done\n        if rem_elements <= s:\n            return remaining_subsets[idx]\n\n        rem_after = rem_elements - s\n        future_gains = [len(rem_after & t) for j, t in enumerate(rem_sets) if j != idx]\n        best_future = max(future_gains) if future_gains else 0\n\n        denom = max(1, best_future) + 1e-12\n        steps_est = 1.0 + np.ceil(len(rem_after) / denom)\n\n        # Small deterministic noise for tie\u2011breaking\n        noise = idx * 1e-6\n        score = -steps_est + 0.05 * gain + noise\n\n        if score > best_score:\n            best_score = score\n            best_idx = idx\n\n    return remaining_subsets[best_idx] if best_idx is not None else None\n\n",
  "expected_steps_proxy_minimize_aug_201": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorised scoring with clipped future gains.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    rem_sets = [set(sub) for sub in remaining_subsets]\n    n = len(rem_sets)\n\n    # Immediate gains\n    gains = np.fromiter((len(rem_set & s) for s in rem_sets), dtype=int, count=n)\n\n    # If any subset covers all remaining elements, return it\n    covers_all = [rem_set <= s for s in rem_sets]\n    if np.any(covers_all):\n        return remaining_subsets[int(np.argmax(covers_all))]\n\n    # Future gains for each candidate\n    future_gains = np.empty(n, dtype=int)\n    for i, s in enumerate(rem_sets):\n        rem_after = rem_set - s\n        future_gains[i] = np.max([len(rem_after & t) for j, t in enumerate(rem_sets) if j != i] or [0])\n\n    future_gains = np.clip(future_gains, 1, None) + 1e-12\n    remaining_sizes = np.array([len(rem_set - s) for s in rem_sets], dtype=int)\n\n    steps_est = 1.0 + np.ceil(remaining_sizes / future_gains)\n\n    scores = -steps_est + 0.1 * gains\n    best_idx = int(np.argmax(scores))\n    return remaining_subsets[best_idx]\n\n",
  "expected_steps_proxy_minimize_aug_202": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Randomised choice among the top\u2011k cheapest candidates.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    rem_sets = [set(sub) for sub in remaining_subsets]\n    n = len(rem_sets)\n\n    # Fast return if a subset covers everything\n    for i, s in enumerate(rem_sets):\n        if rem <= s:\n            return remaining_subsets[i]\n\n    # Immediate gains\n    gains = np.array([len(rem & s) for s in rem_sets], dtype=int)\n\n    # Approximate future gains\n    future = np.zeros(n, dtype=int)\n    for i, s in enumerate(rem_sets):\n        after = rem - s\n        future[i] = max([len(after & t) for j, t in enumerate(rem_sets) if j != i] or [0])\n\n    denom = np.clip(future, 1, None) + 1e-12\n    remaining_sizes = np.array([len(rem - s) for s in rem_sets], dtype=int)\n    steps_est = 1.0 + np.ceil(remaining_sizes / denom)\n\n    # Choose among the top\u2011k candidates with minimal estimated steps\n    top_k = min(3, n)\n    top_indices = np.argsort(steps_est)[:top_k]\n\n    rng = np.random.default_rng(12345)\n    chosen = rng.choice(top_indices)\n\n    return remaining_subsets[chosen]\n\n",
  "expected_steps_proxy_minimize_aug_203": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Proxy\u2011based heuristic using mean future gain and safe division.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    rem_sets = [set(sub) for sub in remaining_subsets]\n    n = len(rem_sets)\n\n    # Immediate full coverage\n    for i, s in enumerate(rem_sets):\n        if rem <= s:\n            return remaining_subsets[i]\n\n    gains = np.array([len(rem & s) for s in rem_sets], dtype=int)\n\n    # Mean future gain approximation\n    future_mean = np.empty(n, dtype=float)\n    for i, s in enumerate(rem_sets):\n        after = rem - s\n        future_gains = [len(after & t) for j, t in enumerate(rem_sets) if j != i]\n        future_mean[i] = np.mean(future_gains) if future_gains else 0.0\n\n    denom = np.clip(future_mean, 1e-12, None) + 1e-12\n    remaining_sizes = np.array([len(rem - s) for s in rem_sets], dtype=int)\n    steps_est = 1.0 + np.ceil(remaining_sizes / denom)\n\n    scores = -steps_est + 0.08 * gains\n    best_idx = int(np.argmax(scores))\n    return remaining_subsets[best_idx]\n\n",
  "gain_squared_soft_sampling_aug_204": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Select a subset from remaining_subsets using a stochastic weighted choice.\n    The weight is proportional to the square of the gain plus a tiny random\n    perturbation to break ties.\n    \"\"\"\n    # Build a set of elements that are still uncovered\n    remaining = set(remaining_elements)\n\n    # Gather candidates and their gains with a list comprehension\n    candidates = [s for s in remaining_subsets if len(remaining & s) > 0]\n    if not candidates:\n        return None\n\n    # Compute squared gains and add a small random noise for tie\u2011breaking\n    gains = np.array([float(len(remaining & s)) for s in candidates], dtype=float)\n    weights = np.square(gains) + 1e-8 * np.random.rand(len(candidates))\n\n    # Normalise the weights, guarding against division by zero\n    total = weights.sum() + 1e-12\n    probs = np.clip(weights / total, 0, 1)\n\n    # Sample one candidate according to the probabilities\n    idx = int(np.random.choice(len(candidates), p=probs))\n    return candidates[idx]\n\n",
  "gain_squared_soft_sampling_aug_205": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Uses a while loop to collect candidates, keeps only the top_k by gain,\n    then selects from them with a weighted probability.\n    \"\"\"\n    remaining = set(remaining_elements)\n    candidates = []\n    gains = []\n\n    i = 0\n    while i < len(remaining_subsets):\n        s = remaining_subsets[i]\n        g = len(remaining & s)\n        if g > 0:\n            candidates.append(s)\n            gains.append(float(g))\n        i += 1\n\n    if not candidates:\n        return None\n\n    # Keep only the best top_k candidates\n    top_k = 5\n    k = min(top_k, len(candidates))\n    idxs = np.argsort(gains)[-k:]\n    top_candidates = [candidates[j] for j in idxs]\n    top_gains = [gains[j] for j in idxs]\n\n    # Weighted probability with a 0.6 scaling factor\n    weights = np.array(top_gains, dtype=float) * 0.6\n    total = weights.sum() + 1e-12\n    probs = np.clip(weights / total, 0, 1)\n\n    chosen = int(np.random.choice(len(top_candidates), p=probs))\n    return top_candidates[chosen]\n\n",
  "gain_squared_soft_sampling_aug_206": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Vectorised implementation that uses a softmin-like selection.\n    \"\"\"\n    rem_arr = np.array(remaining_elements)\n\n    candidates = []\n    gains = []\n    for s in remaining_subsets:\n        s_arr = np.array(s)\n        g = np.intersect1d(rem_arr, s_arr, assume_unique=True).size\n        if g:\n            candidates.append(s)\n            gains.append(float(g))\n\n    if not candidates:\n        return None\n\n    gains_np = np.array(gains, dtype=float)\n    # Log transform to dampen large gains\n    log_gains = np.log1p(gains_np)\n\n    # Softmin: smaller log gains get larger probability\n    inv = 1.0 / (log_gains + 1e-12)\n    probs = np.clip(inv / inv.sum(), 0, 1)\n\n    idx = int(np.random.choice(len(candidates), p=probs))\n    return candidates[idx]\n\n",
  "gain_squared_soft_sampling_aug_207": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Deterministic selection of the subset with the maximum gain.\n    Tie\u2011breaking is handled by choosing the first occurring subset.\n    \"\"\"\n    remaining = set(remaining_elements)\n\n    # Compute gains for all subsets\n    gains = np.array([len(remaining & s) for s in remaining_subsets], dtype=float)\n    gains = np.clip(gains, 0, None)  # ensure non\u2011negative\n\n    max_gain = gains.max()\n    if max_gain <= 0:\n        return None\n\n    # Find indices with the maximum gain\n    idxs = np.where(gains == max_gain)[0]\n    # Return the first subset among those tied for the maximum\n    return remaining_subsets[int(idxs[0])]\n\n",
  "selected_overlap_dynamic_penalty_aug_208": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    covered: Set[int] = set()\n    for s in selected_subsets:\n        covered.update(s)\n\n    n_rem = max(1, len(rem_set))\n    n_total = n_rem + len(covered)\n    progress = 1.0 - (n_rem / (n_total + 1e-12))\n    progress = np.clip(progress, 0.0, 1.0)\n    lam = 0.2 + 0.8 * progress            # different weighting\n    lam = np.clip(lam, 0.0, 1.0)\n\n    best_subset = None\n    best_score = -np.inf\n\n    for s in remaining_subsets:\n        s_set = set(s)\n        gain = len(rem_set & s_set)\n        if gain <= 0:\n            continue\n        overlap = len(s_set & covered)\n        noise = (hash(tuple(s)) % 1000000) * 1e-9  # deterministic tiny noise\n        score = gain - lam * overlap + noise\n        if score > best_score:\n            best_score, best_subset = score, s\n\n    return best_subset\n\n",
  "selected_overlap_dynamic_penalty_aug_209": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    covered: Set[int] = set()\n    for s in selected_subsets:\n        covered.update(s)\n\n    n_rem = max(1, len(rem_set))\n    n_total = n_rem + len(covered)\n    progress = 1.0 - (n_rem / (n_total + 1e-12))\n    progress = np.clip(progress, 0.0, 1.0)\n    lam = 0.5 + 0.5 * progress            # alternate weighting\n    lam = np.clip(lam, 0.0, 1.0)\n\n    scores = []\n    for s in remaining_subsets:\n        s_set = set(s)\n        gain = len(rem_set & s_set)\n        if gain <= 0:\n            continue\n        overlap = len(s_set & covered)\n        score = gain - lam * overlap\n        scores.append((score, s))\n\n    if not scores:\n        return None\n\n    # pick among top\u2011k candidates deterministically\n    top_k = 3\n    scores.sort(key=lambda x: x[0], reverse=True)\n    top_candidates = scores[:top_k]\n    best_subset = min(top_candidates, key=lambda x: sum(x[1]))[1]\n    return best_subset\n\n",
  "selected_overlap_dynamic_penalty_aug_210": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    covered: Set[int] = set()\n    for s in selected_subsets:\n        covered.update(s)\n\n    n_rem = max(1, len(rem_set))\n    n_total = n_rem + len(covered)\n    progress = 1.0 - (n_rem / (n_total + 1e-12))\n    progress = np.clip(progress, 0.0, 1.0)\n    lam = 0.4 + 0.6 * progress            # different weighting\n    lam = np.clip(lam, 0.0, 1.0)\n\n    best_subset = None\n    best_score = -np.inf\n\n    for s in remaining_subsets:\n        s_arr = np.array(s, dtype=int)\n        # gain via mean of membership in the remaining set\n        gain = np.mean(np.isin(s_arr, list(rem_set))) * len(s_arr)\n        if gain <= 0:\n            continue\n        overlap_frac = np.mean(np.isin(s_arr, list(covered)))\n        overlap = overlap_frac * len(s_arr)\n        noise = (sum(s) % 100) * 1e-9          # deterministic noise\n        score = gain - lam * overlap + noise\n        if score > best_score:\n            best_score, best_subset = score, s\n\n    return best_subset\n\n",
  "selected_overlap_dynamic_penalty_aug_211": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    covered: Set[int] = set()\n    for s in selected_subsets:\n        covered.update(s)\n\n    n_rem = max(1, len(rem_set))\n    n_total = n_rem + len(covered)\n    progress = 1.0 - (n_rem / (n_total + 1e-12))\n    progress = np.clip(progress, 0.0, 1.0)\n    lam = 0.3 + 0.7 * np.minimum(progress, 0.8)   # capped progress\n    lam = np.clip(lam, 0.0, 1.0)\n\n    candidate_subsets = []\n    scores = []\n    for s in remaining_subsets:\n        s_set = set(s)\n        gain = len(rem_set & s_set)\n        if gain <= 0:\n            continue\n        overlap = len(s_set & covered)\n        score = gain - lam * overlap\n        noise = (sum(s) % 1000) * 1e-9          # deterministic noise\n        candidate_subsets.append(s)\n        scores.append(score + noise)\n\n    if not candidate_subsets:\n        return None\n\n    idx = int(np.argmax(scores))               # choose the max soft score\n    return candidate_subsets[idx]\n\n",
  "boundary_elements_first_aug_212": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorized support counting with weighted scoring.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    rem_arr = np.array(remaining_elements, dtype=int)\n    elem_to_idx = {e: i for i, e in enumerate(rem_arr)}\n    support = np.zeros(len(rem_arr), dtype=int)\n\n    # Count support for each element using numpy vectorisation\n    for s in remaining_subsets:\n        mask = np.isin(s, rem_arr)\n        if not mask.any():\n            continue\n        idxs = np.nonzero(mask)[0]\n        support[idxs] += 1\n\n    boundary_idxs = np.where((support > 0) & (support <= 2))[0]\n    boundary_set = set(rem_arr[boundary_idxs])\n\n    best_subset = None\n    best_score = -np.inf\n\n    for s in remaining_subsets:\n        mask = np.isin(s, rem_arr)\n        new_count = np.sum(mask)\n        if new_count == 0:\n            continue\n\n        boundary_count = np.sum(np.isin(s, boundary_set))\n        # Normalise counts to avoid division\u2011by\u2011zero and clip for safety\n        norm_boundary = boundary_count / (len(rem_arr) + 1e-12)\n        norm_new = new_count / (len(rem_arr) + 1e-12)\n        norm_boundary = np.clip(norm_boundary, 0, 1)\n        norm_new = np.clip(norm_new, 0, 1)\n\n        # Weighted scoring: boundary > total gain > size penalty\n        boundary_weight = 0.7\n        total_weight = 0.3\n        size_penalty = 0.05\n        score = (boundary_weight * norm_boundary +\n                 total_weight * norm_new -\n                 size_penalty * len(s))\n\n        if score > best_score:\n            best_score = score\n            best_subset = s\n\n    return best_subset\n\n",
  "boundary_elements_first_aug_213": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Deterministic noise added for tie\u2011breaking with modified weights.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    rem_arr = np.array(remaining_elements, dtype=int)\n    support = np.zeros(len(rem_arr), dtype=int)\n\n    for s in remaining_subsets:\n        mask = np.isin(s, rem_arr)\n        support[mask] += 1\n\n    boundary_idxs = np.where((support > 0) & (support <= 2))[0]\n    boundary_set = set(rem_arr[boundary_idxs])\n\n    best_subset = None\n    best_score = -np.inf\n\n    for s in remaining_subsets:\n        mask = np.isin(s, rem_arr)\n        new_count = np.sum(mask)\n        if new_count == 0:\n            continue\n\n        boundary_count = np.sum(np.isin(s, boundary_set))\n        # Normalise and clip\n        norm_boundary = boundary_count / (len(rem_arr) + 1e-12)\n        norm_new = new_count / (len(rem_arr) + 1e-12)\n        norm_boundary = np.clip(norm_boundary, 0, 1)\n        norm_new = np.clip(norm_new, 0, 1)\n\n        # Deterministic noise based on subset hash\n        noise = 1e-6 * (hash(tuple(s)) % 1000)\n\n        boundary_weight = 0.8\n        total_weight = 0.15\n        size_penalty = 0.05\n        score = (boundary_weight * norm_boundary +\n                 total_weight * norm_new -\n                 size_penalty * len(s) + noise)\n\n        if score > best_score:\n            best_score = score\n            best_subset = s\n\n    return best_subset\n\n",
  "boundary_elements_first_aug_214": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Softmax\u2011based random choice among the top\u2011k candidates.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    rem_arr = np.array(remaining_elements, dtype=int)\n    support = np.zeros(len(rem_arr), dtype=int)\n\n    for s in remaining_subsets:\n        mask = np.isin(s, rem_arr)\n        support[mask] += 1\n\n    boundary_idxs = np.where((support > 0) & (support <= 2))[0]\n    boundary_set = set(rem_arr[boundary_idxs])\n\n    scores = []\n    candidates = []\n\n    for s in remaining_subsets:\n        mask = np.isin(s, rem_arr)\n        new_count = np.sum(mask)\n        if new_count == 0:\n            continue\n\n        boundary_count = np.sum(np.isin(s, boundary_set))\n        norm_boundary = boundary_count / (len(rem_arr) + 1e-12)\n        norm_new = new_count / (len(rem_arr) + 1e-12)\n        norm_boundary = np.clip(norm_boundary, 0, 1)\n        norm_new = np.clip(norm_new, 0, 1)\n\n        boundary_weight = 0.75\n        total_weight = 0.2\n        size_penalty = 0.05\n        score = (boundary_weight * norm_boundary +\n                 total_weight * norm_new -\n                 size_penalty * len(s))\n        scores.append(score)\n        candidates.append(s)\n\n    if not scores:\n        return None\n\n    scores_np = np.array(scores, dtype=float)\n    max_score = np.max(scores_np)\n    exp_scores = np.exp(scores_np - max_score)  # avoid overflow\n    probs = exp_scores / (np.sum(exp_scores) + 1e-12)\n\n    top_k = min(7, len(scores_np))\n    top_indices = np.argsort(-scores_np)[:top_k]\n    top_probs = probs[top_indices]\n    top_probs = top_probs / (np.sum(top_probs) + 1e-12)\n\n    rng = np.random.default_rng(seed=42)\n    chosen_idx = rng.choice(top_indices, p=top_probs)\n    return candidates[chosen_idx]\n\n",
  "boundary_elements_first_aug_215": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Approximate support via random sampling and proxy scoring.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    rem_arr = np.array(remaining_elements, dtype=int)\n\n    # Approximate support by sampling a subset of subsets\n    rng = np.random.default_rng(seed=123)\n    sample_size = min(10, len(remaining_subsets))\n    if sample_size == 0:\n        sample_size = 1\n    sample_subsets = rng.choice(\n        np.array(remaining_subsets, dtype=object),\n        size=sample_size,\n        replace=False\n    )\n\n    support = np.zeros(len(rem_arr), dtype=int)\n    for s in sample_subsets:\n        mask = np.isin(s, rem_arr)\n        support[mask] += 1\n\n    boundary_idxs = np.where((support > 0) & (support <= 2))[0]\n    boundary_set = set(rem_arr[boundary_idxs])\n\n    best_subset = None\n    best_score = -np.inf\n\n    # Proxy for total gain: use mean subset size among all remaining subsets\n    mean_sub_size = np.mean([len(sub) for sub in remaining_subsets]) if remaining_subsets else 0\n\n    for s in remaining_subsets:\n        mask = np.isin(s, rem_arr)\n        new_count = np.sum(mask)\n        if new_count == 0:\n            continue\n\n        boundary_count = np.sum(np.isin(s, boundary_set))\n        norm_boundary = boundary_count / (len(rem_arr) + 1e-12)\n        norm_boundary = np.clip(norm_boundary, 0, 1)\n\n        # Proxy gain: smaller of new_count and mean subset size\n        proxy_gain = min(new_count, mean_sub_size)\n        norm_proxy = proxy_gain / (len(rem_arr) + 1e-12)\n        norm_proxy = np.clip(norm_proxy, 0, 1)\n\n        boundary_weight = 0.6\n        total_weight = 0.4\n        size_penalty = 0.05\n        score = (boundary_weight * norm_boundary +\n                 total_weight * norm_proxy -\n                 size_penalty * len(s))\n\n        if score > best_score:\n            best_score = score\n            best_subset = s\n\n    return best_subset\n\n",
  "boundary_ratio_aug_216": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 1: vectorized scoring with deterministic noise.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Count support for each element\n    support = {e: 0 for e in rem_set}\n    for subset in remaining_subsets:\n        for e in set(subset):\n            if e in support:\n                support[e] += 1\n\n    boundary = {e for e, c in support.items() if 0 < c <= 2}\n\n    best_subset = None\n    best_score = -np.inf\n    for subset in remaining_subsets:\n        subset_set = set(subset)\n        gain = len(rem_set & subset_set)\n        if gain == 0:\n            continue\n        b = len(boundary & subset_set)\n        denom = 1.0 + len(subset) + 1e-12\n        score = b / denom + 0.03 * gain\n        # deterministic noise for tie\u2011breaking\n        noise = (hash(frozenset(subset)) % 1000) * 1e-6\n        score = np.clip(score + noise, 0, 1e9)\n        if score > best_score:\n            best_score = score\n            best_subset = subset\n    return best_subset\n\n",
  "boundary_ratio_aug_217": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 4: inverted logic, proxy boundary, deterministic tie\u2011breaking.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support = {e: 0 for e in rem}\n    idx = 0\n    while idx < len(remaining_subsets):\n        sub = remaining_subsets[idx]\n        for e in set(sub):\n            if e in support:\n                support[e] += 1\n        idx += 1\n\n    boundary = {e for e, c in support.items() if 0 < c <= 3}  # proxy with <=3\n\n    best = None\n    best_score = -np.inf\n    idx = 0\n    while idx < len(remaining_subsets):\n        sub = remaining_subsets[idx]\n        sub_set = set(sub)\n        gain = len(rem & sub_set)\n        if gain == 0:\n            idx += 1\n            continue\n        b = len(boundary & sub_set)\n        denom = 1.0 + len(sub) + 1e-12\n        score = b / denom + 0.02 * gain\n        score = np.clip(score, 0, 1e9)\n        # deterministic noise\n        noise = (hash(tuple(sub)) % 1000) * 1e-7\n        score += noise\n        if score > best_score:\n            best_score = score\n            best = sub\n        idx += 1\n    return best\n\n",
  "progress_blend_gain_rarity_overlap_aug_218": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    covered = set()\n    for s in selected_subsets:\n        covered.update(s)\n\n    n_rem = max(1, len(rem_set))\n    n_total = n_rem + len(covered)\n    progress = np.clip(1.0 - n_rem / max(1, n_total), 0.0, 1.0)\n\n    all_elems = np.concatenate([np.array(sub) for sub in remaining_subsets])\n    mask = np.isin(all_elems, list(rem_set))\n    elems = all_elems[mask]\n    uniq, counts = np.unique(elems, return_counts=True)\n    freq = dict(zip(uniq.tolist(), counts.tolist()))\n\n    w_gain = 1.1 - 0.4 * progress\n    w_rare = 0.4 + 0.9 * progress\n    w_ov   = 0.2 + 0.8 * progress\n\n    best_subset = None\n    best_score = -1e18\n    for sub in remaining_subsets:\n        sub_set = set(sub)\n        new = [e for e in sub_set if e in rem_set]\n        if not new:\n            continue\n        gain = len(new)\n        rare = sum(1.0 / (freq.get(e, 1) + 1e-12) for e in new)\n        ov = len(sub_set & covered)\n        score = w_gain * gain + w_rare * rare - w_ov * ov\n        if score > best_score:\n            best_score, best_subset = score, sub\n\n    if best_subset is None:\n        return None\n\n    # Find all subsets with score close to best_score\n    eps = 1e-9\n    close_subs = []\n    for sub in remaining_subsets:\n        sub_set = set(sub)\n        new = [e for e in sub_set if e in rem_set]\n        if not new:\n            continue\n        gain = len(new)\n        rare = sum(1.0 / (freq.get(e, 1) + 1e-12) for e in new)\n        ov = len(sub_set & covered)\n        score = w_gain * gain + w_rare * rare - w_ov * ov\n        if abs(score - best_score) < eps:\n            close_subs.append(sub)\n\n    if len(close_subs) > 1:\n        idx = np.random.randint(len(close_subs))\n        return close_subs[idx]\n    return best_subset\n\n",
  "exp_rarity_weighted_gain_aug_219": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Vectorised implementation that scores each remaining subset by the sum of\n    exp(-freq/scale) over its uncovered elements.  A tiny deterministic\n    noise is added to break ties in a reproducible way.\n    \"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Build a mapping from element to its index for fast array lookup\n    elem_to_idx = {e: i for i, e in enumerate(rem)}\n    freq_arr = np.zeros(len(rem), dtype=np.int32)\n\n    # Count frequencies of uncovered elements across all remaining subsets\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                freq_arr[elem_to_idx[e]] += 1\n\n    if not freq_arr.any():\n        return None\n\n    # Scale is the median frequency, clipped to be at least 1\n    scale = np.clip(np.median(freq_arr.astype(float)), 1.0, None) + 1e-12\n\n    # Pre\u2011compute weight for each element\n    weights = np.exp(-freq_arr / scale)\n\n    best_subset, best_score = None, -np.inf\n    # Deterministic noise for tie\u2011breaking (based on subset index)\n    noise = 1e-6 * np.arange(len(remaining_subsets))\n\n    for idx, s in enumerate(remaining_subsets):\n        score = 0.0\n        for e in set(s):\n            if e in rem:\n                score += weights[elem_to_idx[e]]\n        # add a tiny noise to break ties deterministically\n        score += noise[idx]\n        if score > best_score and score > 0:\n            best_score, best_subset = score, s\n\n    return best_subset\n\n",
  "exp_rarity_weighted_gain_aug_220": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Uses the maximum weight of an uncovered element in a subset as the\n    score, then selects one of the top\u2011k best subsets (k=3) deterministically.\n    \"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Frequency dictionary\n    freq = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    if not freq:\n        return None\n\n    freq_vals = np.array(list(freq.values()), dtype=float)\n    # Scale: use mean instead of median, clipped to >=1\n    scale = np.clip(np.mean(freq_vals), 1.0, None) + 1e-12\n\n    # Compute scores: maximum of exp(-freq/scale) for each subset\n    scores = []\n    for s in remaining_subsets:\n        max_w = -np.inf\n        for e in set(s):\n            if e in rem:\n                w = np.exp(-freq.get(e, 1) / scale)\n                if w > max_w:\n                    max_w = w\n        scores.append(max_w if max_w > 0 else -np.inf)\n\n    # Identify top\u2011k indices (k=3)\n    scores_arr = np.array(scores)\n    top_k = 3\n    top_indices = np.argpartition(-scores_arr, top_k - 1)[:top_k]\n    # Sort the selected indices by score descending\n    top_indices = sorted(top_indices, key=lambda i: scores_arr[i], reverse=True)\n\n    # Deterministically pick the best among the top\u2011k\n    best_idx = top_indices[0]\n    return remaining_subsets[best_idx]\n\n",
  "exp_rarity_weighted_gain_aug_221": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Approximates element frequencies by sampling only the first half of the\n    remaining subsets.  Uses exp(-freq^2/scale) for rarity weighting.\n    \"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Sample half of the subsets for a proxy frequency count\n    sample = remaining_subsets[: max(1, len(remaining_subsets) // 2)]\n    freq = {}\n    for s in sample:\n        for e in set(s):\n            if e in rem:\n                freq[e] = freq.get(e, 0) + 1\n\n    if not freq:\n        return None\n\n    freq_vals = np.array(list(freq.values()), dtype=float)\n    # Scale: median of sampled frequencies, clipped to >=1\n    scale = np.clip(np.median(freq_vals), 1.0, None) + 1e-12\n\n    best_subset, best_score = None, -np.inf\n    for s in remaining_subsets:\n        score = 0.0\n        for e in set(s):\n            if e in rem:\n                f = freq.get(e, 1)\n                score += np.exp(-np.square(f) / scale)\n        if score > best_score and score > 0:\n            best_score, best_subset = score, s\n\n    return best_subset\n\n",
  "exp_rarity_weighted_gain_aug_222": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Classic greedy implementation rewritten with a while loop and\n    1/(freq+eps) weighting to emphasize rare elements.\n    \"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Frequency dictionary via list comprehension\n    freq = {e: sum(1 for s in remaining_subsets if e in s) for e in rem}\n\n    if not freq:\n        return None\n\n    freq_vals = np.array(list(freq.values()), dtype=float)\n    # Use mean as scale, ensure no division by zero\n    scale = np.clip(np.mean(freq_vals), 1.0, None) + 1e-12\n\n    best_subset, best_score = None, -np.inf\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        score = 0.0\n        for e in set(s):\n            if e in rem:\n                score += 1.0 / (freq.get(e, 0) + 1e-12)  # rarity weight\n        if score > best_score and score > 0:\n            best_score, best_subset = score, s\n        idx += 1\n\n    return best_subset\n\n",
  "element_cover_count_penalty_aug_223": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    sel_sets = selected_subsets\n    rem_sets = remaining_subsets\n    rem_elems = remaining_elements\n\n    rem_set = set(rem_elems)\n\n    # count how many times each element has already been covered\n    covered_counts: Dict[int, int] = {}\n    for s in sel_sets:\n        for e in set(s):\n            covered_counts[e] = covered_counts.get(e, 0) + 1\n\n    best_subset: Optional[List[int]] = None\n    best_score = -np.inf\n    rng = np.random.default_rng()\n\n    for s in rem_sets:\n        s_set = set(s)\n        new_covered = [e for e in s_set if e in rem_set]\n        if not new_covered:\n            continue\n\n        penalty = sum(covered_counts.get(e, 0) for e in s_set)\n        score = len(new_covered) - 0.25 * penalty\n\n        # deterministic tie\u2011breaking noise\n        noise = rng.uniform(0, 1e-6)\n        score += noise\n\n        if score > best_score:\n            best_score, best_subset = score, s\n\n    return best_subset\n\n",
  "element_cover_count_penalty_aug_224": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n\n    # build coverage dictionary\n    covered = {}\n    for s in selected_subsets:\n        for e in set(s):\n            covered[e] = covered.get(e, 0) + 1\n\n    # gather candidates and raw scores\n    candidates, raw_scores = [], []\n    for s in remaining_subsets:\n        s_set = set(s)\n        new = [e for e in s_set if e in rem_set]\n        if not new:\n            continue\n        penalty = sum(covered.get(e, 0) for e in s_set)\n        score = len(new) - 0.5 * penalty\n        candidates.append(s)\n        raw_scores.append(score)\n\n    if not candidates:\n        return None\n\n    raw_scores = np.array(raw_scores, dtype=np.float64)\n\n    # soft\u2011min probabilities (invert exponential)\n    inv = 1.0 / (np.exp(raw_scores) + 1e-12)          # epsilon to avoid div\u2011by\u2011zero\n    probs = inv / (np.sum(inv) + 1e-12)\n\n    # pick randomly among the top\u2011k candidates\n    top_k = min(3, len(candidates))\n    top_indices = np.argpartition(-raw_scores, top_k - 1)[:top_k]\n    top_candidates = [candidates[i] for i in top_indices]\n    chosen = np.random.default_rng().choice(top_candidates)\n    return chosen\n\n",
  "element_cover_count_penalty_aug_225": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n\n    # coverage counts\n    cover_counts = {}\n    for s in selected_subsets:\n        for e in set(s):\n            cover_counts[e] = cover_counts.get(e, 0) + 1\n\n    best_subset: Optional[List[int]] = None\n    best_value = -np.inf\n    rng = np.random.default_rng()\n\n    for s in remaining_subsets:\n        s_set = set(s)\n        new_elems = [e for e in s_set if e in rem_set]\n        if not new_elems:\n            continue\n\n        # penalty via median coverage\n        penalty_vals = [cover_counts.get(e, 0) for e in s_set]\n        penalty = np.median(penalty_vals)\n\n        # value: number of new elements minus scaled penalty\n        value = len(new_elems) - 0.3 * penalty\n\n        # deterministic tie\u2011breaking noise\n        value += rng.uniform(0, 1e-5)\n\n        if value > best_value:\n            best_value, best_subset = value, s\n\n    return best_subset\n\n",
  "element_cover_count_penalty_aug_226": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n\n    # build a vectorised coverage array\n    all_cov = [e for s in selected_subsets for e in set(s)]\n    if all_cov:\n        max_val = max(all_cov)\n        cover_arr = np.bincount(all_cov, minlength=max_val + 1)\n    else:\n        cover_arr = np.array([], dtype=int)\n\n    candidates, scores = [], []\n\n    for s in remaining_subsets:\n        s_set = set(s)\n        new = [e for e in s_set if e in rem_set]\n        if not new:\n            continue\n\n        # mean penalty via vectorised lookup\n        idx = np.array(list(s_set), dtype=int)\n        penalty_vals = cover_arr[idx] if cover_arr.size > 0 else np.zeros(len(s_set))\n        penalty = np.mean(penalty_vals)\n\n        score = len(new) - 0.4 * penalty\n        candidates.append(s)\n        scores.append(score)\n\n    if not candidates:\n        return None\n\n    # clip to avoid extreme values\n    scores = np.clip(np.array(scores), -1e12, 1e12)\n\n    # random choice among top\u2011k (k=5 or fewer)\n    top_k = min(5, len(candidates))\n    top_indices = np.argpartition(-np.array(scores), top_k - 1)[:top_k]\n    top_candidates = [candidates[i] for i in top_indices]\n    chosen = np.random.default_rng().choice(top_candidates)\n    return chosen\n\n",
  "replacement_potential_aug_227": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Syntactic rewrite: list comprehensions, renamed variables, inverted logic.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    sel_sets: List[Set[int]] = [set(s) for s in selected_subsets]\n    union_sel: Set[int] = set().union(*sel_sets) if sel_sets else set()\n\n    # Build candidate list with scores\n    candidates = [\n        (s,\n         len(rem & set(s)),\n         sum(1 for t in sel_sets if len(t) > 0 and t <= union_sel | set(s)),\n         len(s))\n        for s in remaining_subsets\n        if len(rem & set(s)) > 0\n    ]\n\n    if not candidates:\n        return None\n\n    # Compute score for each candidate\n    scored = [\n        (cand,\n         cand_score[1] + 0.8 * cand_score[2] - 0.05 * cand_score[3])\n        for cand, *cand_score in candidates\n    ]\n\n    # Select the candidate with the highest score\n    best_subset, _ = max(scored, key=lambda x: x[1])\n    return best_subset\n\n",
  "replacement_potential_aug_228": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Parameter tuning: altered weights, top\u2011k random choice, epsilon\u2011protected division.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    sel_sets = [set(s) for s in selected_subsets]\n    union_sel = set().union(*sel_sets) if sel_sets else set()\n\n    # Gather all viable candidates\n    viable = [\n        (s,\n         len(rem & set(s)),\n         sum(1 for t in sel_sets if len(t) > 0 and t <= union_sel | set(s)),\n         len(s))\n        for s in remaining_subsets\n        if len(rem & set(s)) > 0\n    ]\n    if not viable:\n        return None\n\n    # Compute score with tweaked coefficients\n    scored = [\n        (cand,\n         cand_score[1] + 0.6 * cand_score[2] - 0.1 * cand_score[3])\n        for cand, *cand_score in viable\n    ]\n\n    # Select top\u2011k candidates\n    k = 7\n    top_k = sorted(scored, key=lambda x: x[1], reverse=True)[:k]\n    if not top_k:\n        return None\n\n    # Randomly pick one from the top\u2011k (softmax probability)\n    scores = np.array([s[1] for s in top_k], dtype=np.float64)\n    probs = np.exp(scores - np.max(scores))  # numeric stability\n    probs /= (probs.sum() + 1e-12)            # epsilon to avoid div\u2011by\u2011zero\n    probs = np.clip(probs, 0, 1)              # ensure valid probabilities\n\n    idx = np.random.choice(len(top_k), p=probs)\n    return top_k[idx][0]\n\n",
  "replacement_potential_aug_229": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Behavioral diversity: deterministic noise, softmax selection, noise for tie\u2011breaking.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    sel_sets = [set(s) for s in selected_subsets]\n    union_sel = set().union(*sel_sets) if sel_sets else set()\n\n    # Candidate evaluation with deterministic noise\n    candidates = []\n    for idx, s in enumerate(remaining_subsets):\n        cand = set(s)\n        gain = len(rem & cand)\n        if gain == 0:\n            continue\n        union_new = union_sel | cand\n        redundant = sum(1 for t in sel_sets if len(t) > 0 and t <= union_new)\n        score = gain + 0.8 * redundant - 0.05 * len(cand)\n        # Add tiny deterministic noise based on index to break ties\n        score += 1e-6 * idx\n        candidates.append((s, score))\n\n    if not candidates:\n        return None\n\n    # Softmax weighting\n    scores = np.array([c[1] for c in candidates], dtype=np.float64)\n    exp_scores = np.exp(scores - np.max(scores))  # stability\n    probs = exp_scores / (exp_scores.sum() + 1e-12)\n    probs = np.clip(probs, 0, 1)\n\n    chosen_idx = np.random.choice(len(candidates), p=probs)\n    return candidates[chosen_idx][0]\n\n",
  "replacement_potential_aug_230": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorized approach: numpy aggregation, epsilon protection, clipping.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    sel_sets = [set(s) for s in selected_subsets]\n    union_sel = set().union(*sel_sets) if sel_sets else set()\n\n    # Pre\u2011compute gains using numpy\n    gains = []\n    redundants = []\n    lengths = []\n    for s in remaining_subsets:\n        cand = set(s)\n        gain = len(rem & cand)\n        if gain == 0:\n            gains.append(0)\n            redundants.append(0)\n            lengths.append(len(cand))\n            continue\n        union_new = union_sel | cand\n        redundant = sum(1 for t in sel_sets if len(t) > 0 and t <= union_new)\n        gains.append(gain)\n        redundants.append(redundant)\n        lengths.append(len(cand))\n\n    gains = np.array(gains, dtype=np.float64)\n    redundants = np.array(redundants, dtype=np.float64)\n    lengths = np.array(lengths, dtype=np.float64)\n\n    # Compute score vector\n    score = gains + 0.8 * redundants - 0.05 * lengths\n    score = np.clip(score, -1e9, 1e9)  # prevent overflow\n\n    # Select the index with maximum score (ties broken by first occurrence)\n    best_idx = np.argmax(score)\n    if gains[best_idx] == 0:\n        return None\n\n    return remaining_subsets[int(best_idx)]\n\n",
  "minimize_new_support_variance_aug_231": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_el = set(remaining_elements)\n    if not rem_el:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem_el}\n    for s in remaining_subsets:\n        for e in s:\n            if e in support:\n                support[e] += 1\n\n    best_set = None\n    best_score = None\n    idx = 0\n    for s in remaining_subsets:\n        idx += 1\n        new_el = [e for e in s if e in rem_el]\n        if not new_el:\n            continue\n        gain = len(new_el)\n        vals = np.array([support[e] for e in new_el], dtype=float)\n        vals = np.clip(vals, 0, 1e9)\n        var = float(vals.var() + 1e-12)\n        # deterministic tie\u2011breaker\n        score = (gain, -var, -len(s), 1e-6 * idx)\n        if best_score is None or score > best_score:\n            best_score, best_set = score, s\n    return best_set\n\n",
  "minimize_new_support_variance_aug_232": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_el = set(remaining_elements)\n    if not rem_el:\n        return None\n\n    support = {e: 0 for e in rem_el}\n    for s in remaining_subsets:\n        for e in s:\n            if e in support:\n                support[e] += 1\n\n    best_set = None\n    best_score = -np.inf\n    for idx, s in enumerate(remaining_subsets):\n        new_el = [e for e in s if e in rem_el]\n        if not new_el:\n            continue\n        gain = len(new_el)\n        vals = np.array([support[e] for e in new_el], dtype=float)\n        var = float(vals.var() + 1e-12)\n        var = np.clip(var, 0, 10)\n        # weighted scoring\n        score = 0.6 * gain - 0.3 * var - 0.1 * len(s)\n        # deterministic noise for stable tie\u2011breaking\n        score += 1e-7 * idx\n        if score > best_score:\n            best_score, best_set = score, s\n    return best_set\n\n",
  "minimize_new_support_variance_aug_233": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_el = set(remaining_elements)\n    if not rem_el:\n        return None\n\n    support = {e: 0 for e in rem_el}\n    for s in remaining_subsets:\n        for e in s:\n            if e in support:\n                support[e] += 1\n\n    scores = []\n    for s in remaining_subsets:\n        new_el = [e for e in s if e in rem_el]\n        if not new_el:\n            continue\n        gain = len(new_el)\n        vals = np.array([support[e] for e in new_el], dtype=float)\n        mean_sup = float(vals.mean() + 1e-12)\n        mean_sup = np.clip(mean_sup, 0, np.inf)\n        scores.append((gain, -mean_sup, -len(s), s))\n\n    if not scores:\n        return None\n\n    # sort by composite score\n    scores.sort(reverse=True, key=lambda x: (x[0], x[1], x[2]))\n    top_k = min(5, len(scores))\n    top_sets = [s for _, _, _, s in scores[:top_k]]\n    chosen = np.random.choice(np.array(top_sets, dtype=object))\n    return chosen\n\n",
  "minimize_new_support_variance_aug_234": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_el = set(remaining_elements)\n    if not rem_el:\n        return None\n\n    support = {e: 0 for e in rem_el}\n    for s in remaining_subsets:\n        for e in s:\n            if e in support:\n                support[e] += 1\n\n    sup_arr = np.array([support[e] for e in rem_el], dtype=float)\n    sup_arr = np.clip(sup_arr, 0, 1e9)\n\n    scores = []\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        new_el = [e for e in s if e in rem_el]\n        if new_el:\n            gain = len(new_el)\n            vals = np.array([support[e] for e in new_el], dtype=float)\n            mean_sup = float(vals.mean() + 1e-12)\n            mean_sup = np.clip(mean_sup, 0, 10)\n            proxy = 1.0 / (mean_sup + 1e-12)\n            score = 0.5 * gain + 0.4 * proxy - 0.1 * len(s)\n            score += 1e-8 * idx  # deterministic tie\u2011breaker\n            scores.append((score, s))\n        idx += 1\n\n    if not scores:\n        return None\n\n    top_k = min(7, len(scores))\n    scores.sort(reverse=True, key=lambda x: x[0])\n    top_sets = [s for _, s in scores[:top_k]]\n    chosen = np.random.choice(np.array(top_sets, dtype=object))\n    return chosen\n\n",
  "minimax_remaining_support_drop_aug_235": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selection with deterministic noise and a proxy for worst drop.\n    Uses vectorised support counting and an epsilon\u2011protected denominator.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # --- Support counting with NumPy ---------------------------------------\n    elems = np.array(list(rem_set), dtype=int)\n    support = np.zeros(len(elems), dtype=int)\n\n    for s in remaining_subsets:\n        mask = np.isin(elems, s)            # binary mask for this subset\n        support += mask.astype(int)         # accumulate support counts\n\n    # Normalise support to avoid division by zero\n    support_ratio = support / (len(remaining_subsets) + 1e-12)\n    mean_support = np.clip(support_ratio.mean(), 0.0, 1.0)   # keep in [0,1]\n\n    # --- Candidate evaluation ---------------------------------------------\n    best_subset, best_score = None, -np.inf\n    rng = np.random.default_rng(seed=42)          # deterministic RNG\n\n    for s in remaining_subsets:\n        ss = set(s)\n        gain = len(rem_set & ss)\n        if gain == 0:\n            continue\n\n        rem_after = rem_set - ss\n        if not rem_after:\n            return s\n\n        # Proxy for worst drop: fraction of elements lost from remaining set\n        drop_frac = 1.0 - (len(rem_after) / (len(rem_set) + 1e-12))\n        drop_frac = np.clip(drop_frac, 0.0, 1.0)\n\n        # Deterministic tie\u2011breaker: small noise derived from subset hash\n        noise = 1e-6 * (hash(tuple(sorted(s))) % 1000)\n\n        # Score blends gain, support, drop and noise\n        score = (\n            gain * 0.6\n            + mean_support * 0.2\n            - drop_frac * 0.2\n            + noise\n        )\n\n        if score > best_score:\n            best_score = score\n            best_subset = s\n\n    return best_subset\n\n",
  "minimax_remaining_support_drop_aug_236": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Randomised top\u2011k selection with a soft\u2011min style scoring.\n    Uses list comprehensions, inverted logic, and an epsilon\u2011protected drop.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Compute support per element\n    support: Dict[int, int] = {e: 0 for e in rem_set}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    # Prepare candidate scores\n    candidates = []\n    for s in remaining_subsets:\n        ss = set(s)\n        gain = len(rem_set & ss)\n        if gain == 0:\n            continue\n\n        rem_after = rem_set - ss\n        if not rem_after:\n            return s\n\n        drop_frac = 1.0 - (len(rem_after) / (len(rem_set) + 1e-12))\n        drop_frac = np.clip(drop_frac, 0.0, 1.0)\n\n        # Soft\u2011min style score (higher is better)\n        score = gain * 0.6 - drop_frac * 0.4\n        candidates.append((score, s))\n\n    if not candidates:\n        return None\n\n    # Select top\u2011k by score\n    k = min(3, len(candidates))\n    top_k = sorted(candidates, key=lambda x: x[0], reverse=True)[:k]\n\n    # Randomly choose among the top\u2011k\n    rng = np.random.default_rng(seed=123)\n    chosen = rng.choice([s for _, s in top_k])\n\n    return chosen\n\n",
  "minimax_remaining_support_drop_aug_237": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    While\u2011loop implementation with a mean\u2011based support metric\n    and a deterministic tie\u2011breaker using a small noise term.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Vectorised support calculation\n    elems = np.array(list(rem_set), dtype=int)\n    support = np.zeros(len(elems), dtype=int)\n    for s in remaining_subsets:\n        support += np.isin(elems, s).astype(int)\n\n    # Use mean support as a proxy\n    mean_support = np.mean(support) / (len(remaining_subsets) + 1e-12)\n    mean_support = np.clip(mean_support, 0.0, 1.0)\n\n    best_subset, best_score = None, -np.inf\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        ss = set(s)\n        gain = len(rem_set & ss)\n        if gain > 0:\n            rem_after = rem_set - ss\n            if not rem_after:\n                return s\n\n            drop_frac = 1.0 - (len(rem_after) / (len(rem_set) + 1e-12))\n            drop_frac = np.clip(drop_frac, 0.0, 1.0)\n\n            noise = 1e-7 * (hash(tuple(sorted(s))) % 10000)\n            score = (\n                gain * 0.5\n                + mean_support * 0.3\n                - drop_frac * 0.2\n                + noise\n            )\n            if score > best_score:\n                best_score = score\n                best_subset = s\n        idx += 1\n\n    return best_subset\n\n",
  "minimax_remaining_support_drop_aug_238": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Fully vectorised selection using np.argmax and a soft\u2011max style score.\n    Includes epsilon protection and clipping for numeric safety.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Support vectorised\n    elems = np.array(list(rem_set), dtype=int)\n    support = np.zeros(len(elems), dtype=int)\n    for s in remaining_subsets:\n        support += np.isin(elems, s).astype(int)\n\n    # Compute per\u2011subset gains\n    gains = np.array([len(rem_set & set(s)) for s in remaining_subsets], dtype=int)\n    valid = gains > 0\n    if not np.any(valid):\n        return None\n\n    # Worst\u2011drop proxy: fraction of elements lost\n    drop_fracs = np.array([\n        1.0 - (len(rem_set - set(s)) / (len(rem_set) + 1e-12))\n        for s in remaining_subsets\n    ])\n    drop_fracs = np.clip(drop_fracs, 0.0, 1.0)\n\n    # Support normalisation for each subset\n    support_norm = support / (len(remaining_subsets) + 1e-12)\n    support_mean = np.mean(support_norm)\n\n    # Score combines gain, support, and drop\n    scores = (\n        gains * 0.6\n        + support_mean * 0.2\n        - drop_fracs * 0.2\n    )\n    scores = np.where(valid, scores, -np.inf)\n\n    # Soft\u2011max probabilities for random choice\n    exp_scores = np.exp(scores - np.max(scores))  # shift for stability\n    probs = exp_scores / (np.sum(exp_scores) + 1e-12)\n\n    rng = np.random.default_rng(seed=7)\n    chosen_idx = rng.choice(len(remaining_subsets), p=probs)\n    return remaining_subsets[chosen_idx]\n\n",
  "gain_under_adaptive_size_cap_aug_239": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Choose the next subset using a cap on size and a small deterministic noise for tie\u2011breaking.\"\"\"\n    rem = set(remaining_elements)\n    if not rem or not remaining_subsets:\n        return None\n\n    # sizes of all remaining subsets\n    sizes = np.array([len(s) for s in remaining_subsets], dtype=float)\n\n    # progress metric (clipped to avoid division by zero)\n    n_rem = max(1, len(rem))\n    n_total = n_rem + len({e for ss in selected_subsets for e in ss})\n    progress = 1.0 - (n_rem / (max(1, n_total) + 1e-12))\n    progress = np.clip(progress, 0.0, 1.0)\n\n    # size cap that tightens over time\n    q = 0.80 - 0.40 * progress\n    cap = float(np.quantile(sizes, q))\n    cap = max(0.0, cap)\n\n    best, best_gain = None, -1\n    fallback, fallback_gain = None, -1\n\n    # deterministic tiny noise based on the subset contents\n    def _noise(s: List[int]) -> float:\n        return (hash(tuple(sorted(s))) % 1000) / 1e9\n\n    for s in remaining_subsets:\n        g = len(rem.intersection(s))\n        if g <= 0:\n            continue\n        g_noisy = g + _noise(s)\n        if len(s) <= cap and g_noisy > best_gain:\n            best_gain, best = g_noisy, s\n        if g_noisy > fallback_gain:\n            fallback_gain, fallback = g_noisy, s\n\n    return best if best is not None else fallback\n\n",
  "gain_under_adaptive_size_cap_aug_240": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Iterative selection using softmax weighting and a relaxed size cap.\"\"\"\n    rem = set(remaining_elements)\n    if not rem or not remaining_subsets:\n        return None\n\n    sizes = np.array([len(s) for s in remaining_subsets], dtype=float)\n\n    n_rem = max(1, len(rem))\n    n_total = n_rem + len({e for ss in selected_subsets for e in ss})\n    progress = 1.0 - (n_rem / (max(1, n_total) + 1e-12))\n    progress = np.clip(progress, 0.0, 1.0)\n\n    q = 0.90 - 0.50 * progress\n    cap = float(np.quantile(sizes, q))\n    cap = max(0.0, cap)\n\n    best, best_score = None, -1.0\n    fallback, fallback_score = None, -1.0\n\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        g = len(rem.intersection(s))\n        if g > 0:\n            # softmax weight\n            weight = np.exp(g / (1.0 + 1e-12))\n            if len(s) <= cap and weight > best_score:\n                best_score, best = weight, s\n            if weight > fallback_score:\n                fallback_score, fallback = weight, s\n        idx += 1\n\n    return best if best is not None else fallback\n\n",
  "gain_under_adaptive_size_cap_aug_241": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorised selection with median\u2011based progress and a strict size cap.\"\"\"\n    rem = set(remaining_elements)\n    if not rem or not remaining_subsets:\n        return None\n\n    sizes = np.array([len(s) for s in remaining_subsets], dtype=float)\n\n    n_rem = max(1, len(rem))\n    n_total = n_rem + len({e for ss in selected_subsets for e in ss})\n    progress = 1.0 - (n_rem / (max(1, n_total) + 1e-12))\n    progress = np.clip(progress, 0.0, 1.0)\n\n    q = 0.85 - 0.35 * progress\n    cap = float(np.quantile(sizes, q))\n    cap = max(0.0, cap)\n\n    gains = np.array([len(rem.intersection(s)) for s in remaining_subsets], dtype=float)\n    mask = gains > 0\n    if not np.any(mask):\n        return None\n\n    cap_mask = np.array([len(s) <= cap for s in remaining_subsets], dtype=bool)\n    candidate_idx = np.where(mask & cap_mask)[0]\n\n    if candidate_idx.size > 0:\n        best_idx = candidate_idx[np.argmax(gains[candidate_idx])]\n        return remaining_subsets[best_idx]\n\n    # fallback to the subset with the largest gain\n    best_idx = np.argmax(gains)\n    return remaining_subsets[best_idx]\n\n",
  "gain_under_adaptive_size_cap_aug_242": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Randomly pick among the top\u2011k best subsets within a size cap.\"\"\"\n    rem = set(remaining_elements)\n    if not rem or not remaining_subsets:\n        return None\n\n    sizes = np.array([len(s) for s in remaining_subsets], dtype=float)\n\n    n_rem = max(1, len(rem))\n    n_total = n_rem + len({e for ss in selected_subsets for e in ss})\n    progress = 1.0 - (n_rem / (max(1, n_total) + 1e-12))\n    progress = np.clip(progress, 0.0, 1.0)\n\n    q = 0.70 - 0.30 * progress\n    cap = float(np.quantile(sizes, q))\n    cap = max(0.0, cap)\n\n    gains = np.array([len(rem.intersection(s)) for s in remaining_subsets], dtype=float)\n    mask = gains > 0\n    if not np.any(mask):\n        return None\n\n    capped_mask = np.array([len(s) <= cap for s in remaining_subsets], dtype=bool)\n    valid_idx = np.where(mask & capped_mask)[0]\n\n    if valid_idx.size == 0:\n        valid_idx = np.where(mask)[0]\n\n    top_k = min(5, valid_idx.size)\n    top_indices = valid_idx[np.argsort(gains[valid_idx])[-top_k:]]\n    chosen_idx = np.random.choice(top_indices)\n    return remaining_subsets[chosen_idx]\n\n",
  "gain_per_new_element_support_sum_aug_243": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Vectorised support counting with numpy.  Uses a simple\n    score = gain / (1 + sum_support_of_new_elements + eps).\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Pad subsets to a common length for vectorised support calculation\n    max_len = max((len(s) for s in remaining_subsets), default=0)\n    padded = np.full((len(remaining_subsets), max_len), -1, dtype=int)\n    for i, s in enumerate(remaining_subsets):\n        padded[i, :len(s)] = s\n\n    # Support counts: number of subsets that contain each remaining element\n    support = {}\n    for e in rem_set:\n        support[e] = np.sum((padded == e).any(axis=1))\n\n    eps = 1e-12\n    best_subset, best_score = None, -np.inf\n\n    for s in remaining_subsets:\n        new_elems = [e for e in s if e in rem_set]\n        if not new_elems:\n            continue\n        gain = len(new_elems)\n        denom = 1.0 + sum(support[e] for e in new_elems) + eps\n        score = np.clip(gain / denom, 0, 1e6)\n        if score > best_score:\n            best_score = score\n            best_subset = s\n\n    return best_subset\n\n",
  "gain_per_new_element_support_sum_aug_244": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Uses deterministic noise for tie\u2011breaking and a top\u2011k selection.\n    Median support is used instead of a sum.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Vectorised support counts\n    max_len = max((len(s) for s in remaining_subsets), default=0)\n    padded = np.full((len(remaining_subsets), max_len), -1, dtype=int)\n    for i, s in enumerate(remaining_subsets):\n        padded[i, :len(s)] = s\n\n    support = {}\n    for e in rem_set:\n        support[e] = np.sum((padded == e).any(axis=1))\n\n    # Compute scores for all subsets\n    scores = []\n    for idx, s in enumerate(remaining_subsets):\n        new_elems = [e for e in s if e in rem_set]\n        if not new_elems:\n            scores.append(-np.inf)\n            continue\n        gain = len(new_elems)\n        med = np.median([support[e] for e in new_elems])\n        denom = 1.0 + med + 1e-12\n        score = np.clip(gain / denom, 0, 1e6)\n        # deterministic noise for tie\u2011breaking\n        noise = 1e-9 * idx\n        scores.append(score + noise)\n\n    scores = np.array(scores)\n\n    # Top\u2011k selection\n    top_k = 5\n    if top_k > len(remaining_subsets):\n        top_k = len(remaining_subsets)\n    top_indices = np.argpartition(-scores, top_k - 1)[:top_k]\n    best_idx = top_indices[np.argmax(scores[top_indices])]\n    return remaining_subsets[best_idx]\n\n",
  "gain_per_new_element_support_sum_aug_245": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Applies a soft\u2011min weighting scheme: subsets with smaller\n    score (gain / denom) receive higher weight.\n    Median support is used for denom calculation.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Vectorised support counts\n    max_len = max((len(s) for s in remaining_subsets), default=0)\n    padded = np.full((len(remaining_subsets), max_len), -1, dtype=int)\n    for i, s in enumerate(remaining_subsets):\n        padded[i, :len(s)] = s\n\n    support = {}\n    for e in rem_set:\n        support[e] = np.sum((padded == e).any(axis=1))\n\n    scores = []\n    for s in remaining_subsets:\n        new_elems = [e for e in s if e in rem_set]\n        if not new_elems:\n            scores.append(-np.inf)\n            continue\n        gain = len(new_elems)\n        med = np.median([support[e] for e in new_elems])\n        denom = 1.0 + med + 1e-12\n        score = np.clip(gain / denom, 0, 1e6)\n        scores.append(score)\n\n    scores = np.array(scores)\n\n    # Soft\u2011min weighting\n    beta = 10.0\n    weights = np.exp(-scores * beta)\n    weights = np.clip(weights, 1e-12, None)\n    best_idx = np.argmax(weights)\n    return remaining_subsets[best_idx]\n\n",
  "gain_per_new_element_support_sum_aug_246": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Uses an approximate support calculation (max support per new element)\n    and selects randomly from the top\u2011k subsets.  Randomness is\n    deterministic because the seed is fixed.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Approximate support counts via flattened array\n    all_elems = [e for s in remaining_subsets for e in s]\n    uniq, counts = np.unique(all_elems, return_counts=True)\n    support = dict(zip(uniq, counts))\n\n    scores = []\n    for s in remaining_subsets:\n        new_elems = [e for e in s if e in rem_set]\n        if not new_elems:\n            scores.append(-np.inf)\n            continue\n        gain = len(new_elems)\n        max_sup = np.max([support.get(e, 0) for e in new_elems])\n        denom = 1.0 + max_sup + 1e-12\n        score = np.clip(gain / denom, 0, 1e6)\n        scores.append(score)\n\n    scores = np.array(scores)\n\n    # Top\u2011k selection\n    top_k = 7\n    if top_k > len(remaining_subsets):\n        top_k = len(remaining_subsets)\n    top_indices = np.argpartition(-scores, top_k - 1)[:top_k]\n\n    rng = np.random.default_rng(42)   # deterministic seed\n    chosen_idx = rng.choice(top_indices)\n    return remaining_subsets[chosen_idx]\n\n",
  "maximize_new_element_support_sum_aug_247": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorised implementation using numpy arrays for score calculation.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Build support array (index by element value)\n    max_elem = max(rem_set) if rem_set else 0\n    support = np.zeros(max_elem + 1, dtype=int)\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem_set:\n                support[e] += 1\n\n    best_subset = None\n    best_score = -np.inf\n    for s in remaining_subsets:\n        new = [e for e in set(s) if e in rem_set]\n        if not new:\n            continue\n        # Use numpy to sum support values\n        score = np.sum(support[new]) + 1e-12  # epsilon to keep score non\u2011zero\n        if score > best_score:\n            best_score, best_subset = score, s\n    return best_subset\n\n",
  "maximize_new_element_support_sum_aug_248": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Weighted scoring with random tie\u2011breaking among top\u2011k candidates.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Compute support counts\n    support = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                support[e] = support.get(e, 0) + 1\n\n    # Score each subset: weighted sum of support + element count\n    scores = []\n    for s in remaining_subsets:\n        new = [e for e in set(s) if e in rem]\n        if not new:\n            scores.append(-np.inf)\n            continue\n        score = 0.7 * sum(support[e] for e in new) + 0.3 * len(new)\n        # Deterministic noise for tie\u2011breaking\n        score += 1e-6 * hash(tuple(sorted(s)))\n        scores.append(score)\n\n    scores = np.clip(np.array(scores, dtype=float), -1e9, 1e9)\n\n    # Randomly choose among the top\u2011k candidates\n    top_k = 5\n    if len(scores) <= top_k:\n        candidate_idx = np.arange(len(scores))\n    else:\n        candidate_idx = np.argpartition(-scores, top_k - 1)[:top_k]\n    chosen_idx = np.random.choice(candidate_idx)\n    return remaining_subsets[chosen_idx]\n\n",
  "maximize_new_element_support_sum_aug_249": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Soft\u2011min based scoring that favours rare elements.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                support[e] = support.get(e, 0) + 1\n\n    eps = 1e-12\n    scores = []\n    for s in remaining_subsets:\n        new = [e for e in set(s) if e in rem]\n        if not new:\n            scores.append(np.inf)\n            continue\n        # Soft\u2011min: lower support yields higher score\n        sup_arr = np.array([support[e] for e in new], dtype=float)\n        score = np.sum(1.0 / (sup_arr + eps))\n        # Median of supports as secondary aggregation\n        score += np.median(sup_arr + eps)\n        scores.append(score)\n\n    scores = np.clip(np.array(scores, dtype=float), -1e9, 1e9)\n    best_idx = np.argmin(scores)  # lower score is better\n    return remaining_subsets[best_idx]\n\n",
  "maximize_new_element_support_sum_aug_250": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Greedy selection based on maximum support of newly covered elements.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                support[e] = support.get(e, 0) + 1\n\n    eps = 1e-12\n    scores = []\n    for s in remaining_subsets:\n        new = [e for e in set(s) if e in rem]\n        if not new:\n            scores.append(-np.inf)\n            continue\n        max_sup = np.max([support[e] for e in new])\n        # Deterministic noise for tie\u2011breaking\n        noise = 1e-8 * hash(tuple(sorted(s)))\n        score = max_sup + noise\n        scores.append(score)\n\n    scores = np.clip(np.array(scores, dtype=float), -1e9, 1e9)\n    best_idx = np.argmax(scores)\n    return remaining_subsets[best_idx]\n\n",
  "minhash_dissimilarity_bonus_aug_251": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef _minhash_signature(s: Set[int], hash_params: np.ndarray) -> np.ndarray:\n    \"\"\"Compute a 16\u2011dimensional MinHash signature for a set of integers.\"\"\"\n    if not s:\n        return np.full(hash_params.shape[0], 2**31 - 1, dtype=np.int64)\n    arr = np.fromiter(s, dtype=np.int64)\n    p = np.int64(2147483647)\n    a, b = hash_params[:, 0], hash_params[:, 1]\n    vals = (a[:, None] * arr[None, :] + b[:, None]) % p\n    return vals.min(axis=1)\n\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    remaining_set = set(remaining_elements)\n    if not remaining_set:\n        return None\n\n    rng = np.random.RandomState(7)\n    hash_params = rng.randint(1, 2**31 - 1, size=(16, 2), dtype=np.int64)\n\n    selected_sets = [set(s) for s in selected_subsets] if selected_subsets else []\n    selected_sigs = [_minhash_signature(t, hash_params) for t in selected_sets]\n\n    best_subset, best_score = None, -np.inf\n    for cand in remaining_subsets:\n        cand_set = set(cand)\n        new_items = remaining_set & cand_set\n        if not new_items:\n            continue\n        gain = len(new_items)\n\n        if not selected_sigs:\n            dissim = 1.0\n        else:\n            sig = _minhash_signature(cand_set, hash_params)\n            sims = [float((sig == tsig).mean()) for tsig in selected_sigs]\n            dissim = 1.0 - np.median(sims)\n\n        score = gain * (1.0 + 0.5 * dissim) - 0.02 * len(cand)\n        score = np.clip(score, 0, None)\n\n        # deterministic noise to break ties\n        noise = (hash(tuple(cand)) % 1000) * 1e-6\n        score += noise\n\n        if score > best_score:\n            best_score, best_subset = score, cand\n    return best_subset\n\n",
  "minhash_dissimilarity_bonus_aug_252": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef _minhash_signature(s: Set[int], hash_params: np.ndarray) -> np.ndarray:\n    if not s:\n        return np.full(hash_params.shape[0], 2**31 - 1, dtype=np.int64)\n    arr = np.fromiter(s, dtype=np.int64)\n    p = np.int64(2147483647)\n    a, b = hash_params[:, 0], hash_params[:, 1]\n    vals = (a[:, None] * arr[None, :] + b[:, None]) % p\n    return vals.min(axis=1)\n\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    rng = np.random.RandomState(42)\n    hash_params = rng.randint(1, 2**31 - 1, size=(32, 2), dtype=np.int64)\n\n    selected_sets = [set(s) for s in selected_subsets] if selected_subsets else []\n    selected_sigs = [_minhash_signature(t, hash_params) for t in selected_sets]\n\n    cand_iter = iter(remaining_subsets)\n    cand_scores = []\n    cand_list = []\n\n    while True:\n        try:\n            cand = next(cand_iter)\n        except StopIteration:\n            break\n        cand_set = set(cand)\n        new_items = rem_set & cand_set\n        if not new_items:\n            continue\n        gain = len(new_items)\n\n        if not selected_sigs:\n            dissim = 1.0\n        else:\n            sig = _minhash_signature(cand_set, hash_params)\n            sims = [float((sig == tsig).mean()) for tsig in selected_sigs]\n            dissim = 1.0 - np.median(sims)\n\n        score = gain * (1.0 + 0.6 * dissim) - 0.025 * len(cand)\n        score = np.clip(score, 0, None)\n        cand_scores.append(score)\n        cand_list.append(cand)\n\n    if not cand_scores:\n        return None\n\n    scores_arr = np.array(cand_scores, dtype=np.float64)\n    eps = 1e-12\n    inv_scores = 1.0 / (scores_arr + eps)\n    probs = inv_scores / (inv_scores.sum() + eps)\n    probs = np.clip(probs, 0, 1)\n\n    chosen_idx = rng.choice(len(cand_list), p=probs)\n    return cand_list[chosen_idx]\n\n",
  "minhash_dissimilarity_bonus_aug_253": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef _minhash_signature(s: Set[int], hash_params: np.ndarray) -> np.ndarray:\n    if not s:\n        return np.full(hash_params.shape[0], 2**31 - 1, dtype=np.int64)\n    arr = np.fromiter(s, dtype=np.int64)\n    p = np.int64(2147483647)\n    a, b = hash_params[:, 0], hash_params[:, 1]\n    vals = (a[:, None] * arr[None, :] + b[:, None]) % p\n    return vals.min(axis=1)\n\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    rng = np.random.RandomState(13)\n    hash_params = rng.randint(1, 2**31 - 1, size=(24, 2), dtype=np.int64)\n\n    sel_sets = [set(s) for s in selected_subsets] if selected_subsets else []\n    sel_sigs = [_minhash_signature(t, hash_params) for t in sel_sets]\n\n    candidates = [\n        (cand,\n         len(rem_set & set(cand)),\n         _minhash_signature(set(cand), hash_params))\n        for cand in remaining_subsets\n        if rem_set & set(cand)\n    ]\n\n    if not candidates:\n        return None\n\n    scores = []\n    for cand, gain, sig in candidates:\n        if not sel_sigs:\n            dissim = 1.0\n        else:\n            sims = [float((sig == tsig).mean()) for tsig in sel_sigs]\n            dissim = 1.0 - np.max(sims)\n        score = gain * (1.0 + 0.4 * dissim) - 0.03 * len(cand)\n        score = np.clip(score, 0, None)\n        noise = (hash(tuple(cand)) % 500) * 1e-6\n        scores.append(score + noise)\n\n    top_k = min(3, len(scores))\n    top_indices = np.argpartition(-np.array(scores), top_k - 1)[:top_k]\n    chosen_idx = rng.choice(top_indices)\n    return candidates[chosen_idx][0]\n\n",
  "minhash_dissimilarity_bonus_aug_254": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef _minhash_signature(s: Set[int], hash_params: np.ndarray) -> np.ndarray:\n    if not s:\n        return np.full(hash_params.shape[0], 2**31 - 1, dtype=np.int64)\n    arr = np.fromiter(s, dtype=np.int64)\n    p = np.int64(2147483647)\n    a, b = hash_params[:, 0], hash_params[:, 1]\n    vals = (a[:, None] * arr[None, :] + b[:, None]) % p\n    return vals.min(axis=1)\n\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    rng = np.random.RandomState()\n    hash_params = rng.randint(1, 2**31 - 1, size=(40, 2), dtype=np.int64)\n\n    sel_sets = [set(s) for s in selected_subsets] if selected_subsets else []\n    sel_sigs = [_minhash_signature(t, hash_params) for t in sel_sets]\n\n    best_subset, best_score = None, -np.inf\n    for cand in remaining_subsets:\n        cand_set = set(cand)\n        new_items = rem_set & cand_set\n        if not new_items:\n            continue\n        gain = len(new_items)\n\n        if not sel_sigs:\n            dissim = 1.0\n        else:\n            sig = _minhash_signature(cand_set, hash_params)\n            sims = [float((sig == tsig).sum() / (len(sig) + 1e-12)) for tsig in sel_sigs]\n            dissim = 1.0 - np.mean(sims)\n\n        score = gain * (1.0 + 0.7 * dissim) - 0.04 * len(cand)\n        score = np.clip(score, 0, None)\n\n        noise = (hash(tuple(cand)) % 1000) * 1e-6\n        score += noise\n\n        if score > best_score:\n            best_score, best_subset = score, cand\n    return best_subset\n\n",
  "hypergraph_core_peel_aug_255": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Peel heuristic with deterministic noise, weighted scoring and median core.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Build support counts using a list comprehension\n    support = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    if not support:\n        return None\n\n    # Use median as threshold instead of 75th percentile\n    vals = np.array(list(support.values()), dtype=float)\n    thr = np.median(vals)\n    # Clip threshold to avoid extreme values\n    thr = np.clip(thr, 0, len(rem))\n\n    # Core elements: support >= median threshold\n    core = {e for e, c in support.items() if c >= thr}\n\n    # Deterministic noise for tie breaking\n    noise = {e: (hash(e) % 1000) * 1e-6 for e in rem}\n\n    best = None\n    best_score = -np.inf\n\n    for s in remaining_subsets:\n        ss = set(s)\n        new = rem.intersection(ss)\n        if not new:\n            continue\n        cc = len(core.intersection(ss))\n        # Weighted score: 0.6 * core coverage + 0.4 * new coverage\n        score = 0.6 * cc + 0.4 * len(new)\n        # Add deterministic noise to break ties\n        score += sum(noise[e] for e in new)\n        if score > best_score:\n            best_score = score\n            best = s\n\n    return best\n\n",
  "hypergraph_core_peel_aug_256": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Peel heuristic with while-loop iteration, 80th percentile core and softmin scoring.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem}\n    idx = 0\n    # While-loop to iterate over remaining_subsets\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n        idx += 1\n\n    if not support:\n        return None\n\n    vals = np.array(list(support.values()), dtype=float)\n    # 80th percentile threshold\n    thr = np.quantile(vals, 0.8)\n    thr = np.clip(thr, 0, len(rem))\n\n    core = {e for e, c in support.items() if c >= thr}\n\n    # Softmin scoring: smaller values are better, we invert to use argmax\n    scores = []\n    subsets = []\n    for s in remaining_subsets:\n        ss = set(s)\n        new = rem.intersection(ss)\n        if not new:\n            continue\n        cc = len(core.intersection(ss))\n        # Softmin weight: exp(-score)\n        raw = 0.5 * cc + 0.5 * len(new)\n        scores.append(np.exp(-raw + 1e-12))  # epsilon to avoid zero\n        subsets.append(s)\n\n    if not scores:\n        return None\n\n    scores = np.array(scores)\n    # Randomly pick among top 3 softmin values\n    rng = np.random.default_rng(0)  # deterministic seed\n    top_k = min(3, len(scores))\n    top_indices = rng.choice(np.argsort(scores)[-top_k:], size=top_k, replace=False)\n    chosen_idx = top_indices[0]\n    return subsets[chosen_idx]\n\n",
  "hypergraph_core_peel_aug_257": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Peel heuristic with mean-based core, inverted scoring, and size penalty.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    if not support:\n        return None\n\n    vals = np.array(list(support.values()), dtype=float)\n    # Use mean as threshold\n    thr = np.mean(vals) + 1e-12  # epsilon added\n    thr = np.clip(thr, 0, len(rem))\n\n    core = {e for e, c in support.items() if c >= thr}\n\n    # Inverted scoring: prefer many new elements, penalize size heavily\n    best = None\n    best_val = -np.inf\n    for s in remaining_subsets:\n        ss = set(s)\n        new = rem.intersection(ss)\n        if not new:\n            continue\n        cc = len(core.intersection(ss))\n        # Penalize size: subtract 0.7 * size\n        val = len(new) - 0.7 * len(ss) + 0.3 * cc\n        if val > best_val:\n            best_val = val\n            best = s\n\n    return best\n\n",
  "hypergraph_core_peel_aug_258": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Peel heuristic with sampled support estimation, clipped threshold, and random top-2 selection.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Approximate support by sampling up to 10 subsets\n    sample_size = min(10, len(remaining_subsets))\n    rng = np.random.default_rng(0)\n    sample_indices = rng.choice(len(remaining_subsets), size=sample_size, replace=False)\n    support = {e: 0 for e in rem}\n    for idx in sample_indices:\n        s = remaining_subsets[idx]\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    if not support:\n        return None\n\n    vals = np.array(list(support.values()), dtype=float)\n    # Clip threshold to [0, len(rem)]\n    thr = np.clip(np.quantile(vals, 0.75), 0, len(rem))\n\n    core = {e for e, c in support.items() if c >= thr}\n\n    # Randomly choose among top 2 scoring subsets\n    scores = []\n    subsets = []\n    for s in remaining_subsets:\n        ss = set(s)\n        new = rem.intersection(ss)\n        if not new:\n            continue\n        cc = len(core.intersection(ss))\n        # Simple score: new coverage minus size penalty\n        score = len(new) - 0.5 * len(ss) + 0.5 * cc\n        scores.append(score)\n        subsets.append(s)\n\n    if not scores:\n        return None\n\n    scores = np.array(scores)\n    top_k = min(2, len(scores))\n    top_indices = rng.choice(np.argsort(scores)[-top_k:], size=top_k, replace=False)\n    chosen_idx = top_indices[0]\n    return subsets[chosen_idx]\n\n",
  "rare_fringe_peel_aug_259": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Build support dictionary with a list comprehension\n    support = {e: sum(1 for s in remaining_subsets if e in s) for e in rem}\n\n    # Convert support counts to a NumPy array for percentile calculation\n    support_vals = np.array([c for c in support.values() if c > 0], dtype=float)\n    if support_vals.size == 0:\n        return None\n\n    thr = np.quantile(support_vals, 0.25) + 1e-12\n    thr = max(thr, 1e-12)                     # clip to avoid zero threshold\n    fringe = {e for e, c in support.items() if 0 < c <= thr}\n\n    best_subset = None\n    best_score = None\n    rng = np.random.default_rng(42)\n    for s in remaining_subsets:\n        ss = set(s)\n        new = rem & ss\n        if not new:\n            continue\n        # deterministic noise for tie\u2011breaking\n        noise = (hash(tuple(sorted(s))) % 1000) * 1e-6\n        fc = len(fringe & ss)\n        score = (fc + noise, len(new), -len(ss))\n        if best_score is None or score > best_score:\n            best_score = score\n            best_subset = s\n    return best_subset\n\n",
  "rare_fringe_peel_aug_260": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    support_vals = np.array(list(support.values()), dtype=float)\n    if support_vals.size == 0:\n        return None\n    thr = np.quantile(support_vals, 0.3) + 1e-12\n    thr = max(thr, 1e-12)\n    fringe = {e for e, c in support.items() if 0 < c <= thr}\n\n    scores = []\n    subsets = []\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        idx += 1\n        ss = set(s)\n        new = rem & ss\n        if not new:\n            continue\n        fc = len(fringe & ss)\n        # weighted score (negative for soft\u2011min selection)\n        score = -(fc * 2 + len(new) * 0.5 - len(ss) * 0.1)\n        scores.append(score)\n        subsets.append(s)\n\n    if not scores:\n        return None\n\n    scores_arr = np.array(scores)\n    top_k = min(3, len(scores_arr))\n    top_indices = np.argpartition(scores_arr, top_k - 1)[:top_k]\n    rng = np.random.default_rng(123)\n    chosen_idx = rng.choice(top_indices)\n    return subsets[chosen_idx]\n\n",
  "rare_fringe_peel_aug_261": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support = {e: 0 for e in rem}\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        idx += 1\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    support_vals = np.array(list(support.values()), dtype=float)\n    if support_vals.size == 0:\n        return None\n    thr = np.median(support_vals) + 1e-12\n    thr = max(thr, 1e-12)\n    fringe = {e for e, c in support.items() if 0 < c <= thr}\n\n    scores = []\n    subsets = []\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        idx += 1\n        ss = set(s)\n        new = rem & ss\n        if not new:\n            continue\n        fc = len(fringe & ss)\n        fc_clipped = np.clip(fc, 0, len(ss))  # ensure valid range\n        score = fc_clipped * 1.5 + len(new) * 0.8 - len(ss) * 0.2\n        scores.append(score)\n        subsets.append(s)\n\n    if not scores:\n        return None\n\n    scores_arr = np.array(scores)\n    top_k = min(5, len(scores_arr))\n    top_indices = np.argpartition(-scores_arr, top_k - 1)[:top_k]  # top scores\n    rng = np.random.default_rng(456)\n    chosen_idx = rng.choice(top_indices)\n    return subsets[chosen_idx]\n\n",
  "rare_fringe_peel_aug_262": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    support_vals = np.array(list(support.values()), dtype=float)\n    if support_vals.size == 0:\n        return None\n    thr = np.quantile(support_vals, 0.4) + 1e-12\n    thr = max(thr, 1e-12)\n    fringe = {e for e, c in support.items() if 0 < c <= thr}\n\n    scores = []\n    subsets = []\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        idx += 1\n        ss = set(s)\n        new = rem & ss\n        if not new:\n            continue\n        fc = len(fringe & ss)\n        noise = (hash(tuple(sorted(s))) % 100) * 1e-7\n        score = -(fc * 1.2 + len(new) * 0.6 - len(ss) * 0.3 + noise)  # soft\u2011min\n        scores.append(score)\n        subsets.append(s)\n\n    if not scores:\n        return None\n\n    scores_arr = np.array(scores)\n    top_k = min(4, len(scores_arr))\n    top_indices = np.argpartition(scores_arr, top_k - 1)[:top_k]   # smallest scores\n    rng = np.random.default_rng(789)\n    chosen_idx = rng.choice(top_indices)\n    return subsets[chosen_idx]\n\n",
  "support_geometric_mean_bonus_aug_263": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Gain plus bonus for higher geometric mean of supports among newly covered elements.\n    Variant 1: List comprehensions, deterministic tie\u2011breaking, altered weights.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Build support dictionary using a list comprehension\n    supp_dict: Dict[int, int] = {e: 0 for e in rem_set}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in supp_dict:\n                supp_dict[e] += 1\n\n    best_subset = None\n    best_score = -np.inf\n\n    for s in remaining_subsets:\n        ss = set(s)\n        new_elems = [e for e in ss if e in rem_set]\n        if not new_elems:\n            continue\n\n        gain = len(new_elems)\n        # Avoid log(0) by adding a tiny epsilon\n        vals = np.array([max(1, supp_dict.get(e, 1)) for e in new_elems], dtype=float)\n        vals += 1e-12\n        gmean = np.exp(np.mean(np.log(vals)))  # geometric mean\n        gmean = np.clip(gmean, 0, 20)          # bound to prevent overflow\n\n        # Deterministic noise for tie\u2011breaking\n        noise = sum(hash(e) for e in new_elems) * 1e-8\n\n        score = gain + 0.15 * gmean - 0.025 * len(ss) + noise\n        if score > best_score:\n            best_score, best_subset = score, s\n\n    return best_subset\n\n",
  "support_geometric_mean_bonus_aug_264": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 2: While loop, median aggregation, random top\u20113 choice.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Support counts via dictionary\n    support = {e: 0 for e in rem_set}\n    for subset in remaining_subsets:\n        for e in set(subset):\n            if e in support:\n                support[e] += 1\n\n    scores = []\n    subsets = []\n\n    # Iterate with a while loop\n    idx = 0\n    while idx < len(remaining_subsets):\n        subset = remaining_subsets[idx]\n        idx += 1\n\n        new_elems = [e for e in set(subset) if e in rem_set]\n        if not new_elems:\n            continue\n\n        gain = len(new_elems)\n        vals = np.array([support.get(e, 1) for e in new_elems], dtype=float)\n        gmean = np.median(vals)  # use median instead of mean\n\n        score = gain + 0.10 * gmean - 0.02 * len(subset)\n        scores.append(score)\n        subsets.append(subset)\n\n    if not scores:\n        return None\n\n    # Select top\u20113 subsets and randomly pick one\n    top_k = min(3, len(scores))\n    top_indices = np.argsort(scores)[-top_k:]\n    chosen_idx = np.random.choice(top_indices)\n    return subsets[chosen_idx]\n\n",
  "support_geometric_mean_bonus_aug_265": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 3: Boolean matrix support, max aggregation, softmin selection.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Map each element to an index\n    elem_to_idx = {e: i for i, e in enumerate(rem_set)}\n    n_elem = len(rem_set)\n\n    # Build binary matrix of shape (num_subsets, n_elem)\n    mat = np.zeros((len(remaining_subsets), n_elem), dtype=int)\n    for i, subset in enumerate(remaining_subsets):\n        for e in set(subset):\n            if e in elem_to_idx:\n                mat[i, elem_to_idx[e]] = 1\n\n    # Support counts for each element\n    support_counts = mat.sum(axis=0) + 1e-12  # avoid division by zero\n\n    # Compute scores using max aggregation\n    scores = []\n    for i, subset in enumerate(remaining_subsets):\n        indices = [elem_to_idx[e] for e in set(subset) if e in elem_to_idx]\n        if not indices:\n            continue\n        new_count = len(indices)\n        vals = support_counts[indices]\n        gmean = np.max(vals)  # use max instead of mean\n        score = new_count + 0.12 * gmean - 0.02 * len(subset)\n        scores.append(score)\n\n    if not scores:\n        return None\n\n    # Softmin probabilities (lower score -> higher probability)\n    neg_scores = -np.array(scores)\n    exp_vals = np.exp(neg_scores - np.max(neg_scores))  # for numerical stability\n    probs = exp_vals / (exp_vals.sum() + 1e-12)\n    probs = np.clip(probs, 0, 1)\n\n    chosen_idx = np.argmax(probs)\n    return remaining_subsets[chosen_idx]\n\n",
  "support_geometric_mean_bonus_aug_266": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Variant 4: Sum aggregation, clipped gmean, weighted scoring, deterministic noise.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Support counts using a dictionary\n    support = {e: 0 for e in rem_set}\n    for subset in remaining_subsets:\n        for e in set(subset):\n            if e in support:\n                support[e] += 1\n\n    best_subset = None\n    best_score = -np.inf\n\n    for subset in remaining_subsets:\n        new_elems = [e for e in set(subset) if e in rem_set]\n        if not new_elems:\n            continue\n\n        gain = len(new_elems)\n        vals = np.array([support.get(e, 1) for e in new_elems], dtype=float)\n        gmean = np.mean(vals)          # mean aggregation\n        gmean = np.clip(gmean, 0, 10)   # bound to prevent overflow\n\n        # Deterministic noise using element IDs\n        noise = sum(e for e in new_elems) * 1e-9\n\n        score = 0.8 * gain + 0.1 * gmean - 0.05 * len(subset) + noise\n        if score > best_score:\n            best_score, best_subset = score, subset\n\n    return best_subset\n\n",
  "support_geometric_mean_penalty_aug_267": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Gain minus penalty for higher geometric mean support among newly covered elements (preserve flexible items).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Build support counts using a while loop\n    support: Dict[int, int] = {e: 0 for e in rem}\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n        idx += 1\n\n    best_subset = None\n    best_score = -np.inf\n    for s in remaining_subsets:\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            continue\n        gain = len(new)\n        vals = np.array([support[e] for e in new], dtype=float)\n        # avoid log(0) by clipping\n        vals = np.clip(vals, 1e-12, None)\n        gmean = np.exp(np.mean(np.log(vals)))\n        # deterministic noise based on subset hash\n        noise = (hash(tuple(sorted(s))) % 1000) * 1e-6\n        score = gain - 0.12 * gmean - 0.015 * len(ss) + noise\n        if score > best_score:\n            best_score, best_subset = score, s\n    return best_subset\n\n",
  "support_geometric_mean_penalty_aug_268": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Select subset with highest softmax-weighted score based on median support.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    scores = []\n    for s in remaining_subsets:\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            scores.append(-np.inf)\n            continue\n        gain = len(new)\n        vals = np.array([support[e] for e in new], dtype=float)\n        vals = np.clip(vals, 1e-12, None)\n        gmean = np.median(vals)\n        score = gain - 0.08 * gmean - 0.02 * len(ss)\n        scores.append(score)\n\n    scores_np = np.array(scores)\n    # softmax with epsilon to avoid overflow\n    max_score = np.max(scores_np)\n    exp_scores = np.exp(scores_np - max_score)\n    probs = exp_scores / (np.sum(exp_scores) + 1e-12)\n    # choose subset with highest probability\n    idx = np.argmax(probs)\n    return remaining_subsets[idx] if np.isfinite(scores_np[idx]) else None\n\n",
  "support_geometric_mean_penalty_aug_269": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Randomly pick among top-5 subsets with median support penalty.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    scored = []\n    for s in remaining_subsets:\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            continue\n        gain = len(new)\n        vals = np.array([support[e] for e in new], dtype=float)\n        vals = np.clip(vals, 1e-12, None)\n        gmean = np.median(vals)\n        noise = (hash(tuple(sorted(s))) % 1000) * 1e-6\n        score = gain - 0.10 * gmean - 0.01 * len(ss) + noise\n        scored.append((score, s))\n\n    if not scored:\n        return None\n\n    scored.sort(key=lambda x: x[0], reverse=True)\n    top_k = min(5, len(scored))\n    top_candidates = [s for _, s in scored[:top_k]]\n    # deterministic selection using hash\n    idx = hash(tuple(sorted(top_candidates[0]))) % top_k\n    return top_candidates[idx]\n\n",
  "support_geometric_mean_penalty_aug_270": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Binary vector support with norm penalty and softmax selection.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Map elements to indices\n    elem_to_idx = {e: i for i, e in enumerate(rem)}\n    m = len(rem)\n    # Build binary matrix\n    mat = np.zeros((len(remaining_subsets), m), dtype=int)\n    for i, s in enumerate(remaining_subsets):\n        for e in set(s):\n            if e in elem_to_idx:\n                mat[i, elem_to_idx[e]] = 1\n\n    support = mat.sum(axis=0).astype(float)\n    scores = []\n    for i, s in enumerate(remaining_subsets):\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            scores.append(-np.inf)\n            continue\n        gain = len(new)\n        vals = support[[elem_to_idx[e] for e in new]]\n        vals = np.clip(vals, 1e-12, None)\n        gmean = np.exp(np.mean(np.log(vals)))\n        # norm penalty\n        norm_pen = np.linalg.norm(vals)\n        score = gain - 0.15 * gmean - 0.02 * len(ss) - 0.01 * norm_pen\n        scores.append(score)\n\n    scores_np = np.array(scores)\n    max_score = np.max(scores_np)\n    exp_scores = np.exp(scores_np - max_score)\n    probs = exp_scores / (np.sum(exp_scores) + 1e-12)\n    idx = np.argmax(probs)\n    return remaining_subsets[idx] if np.isfinite(scores_np[idx]) else None\n\n",
  "adaptive_lambda_overlap_by_density_aug_271": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Select next subset using median support and deterministic noise for tie\u2011breaking.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # union of already chosen elements\n    covered_selected = set().union(*selected_subsets)\n\n    # compute support counts for remaining elements\n    support: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    # median of support counts, add epsilon to avoid empty division\n    avg_sup = np.median(list(support.values())) if support else 0.0\n    lam = 0.15 + 0.08 * avg_sup\n    lam = np.clip(lam, 0.1, 0.5)          # bound lambda\n\n    best, best_score = None, -np.inf\n    # small deterministic noise to break ties\n    noise = np.arange(len(remaining_subsets)) * 1e-6\n    for idx, s in enumerate(remaining_subsets):\n        ss = set(s)\n        gain = len(rem & ss)\n        if gain <= 0:\n            continue\n        ov = len(ss & covered_selected)\n        score = gain - lam * ov - 0.01 * len(ss) + noise[idx]\n        if score > best_score:\n            best_score, best = score, s\n    return best\n\n",
  "adaptive_lambda_overlap_by_density_aug_272": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Greedy selection with soft\u2011top\u2011k random choice.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    covered_selected = set().union(*selected_subsets)\n\n    # Build support count array for vectorised operations\n    rem_list = list(rem)\n    rem_index = {e: i for i, e in enumerate(rem_list)}\n    support_counts = np.zeros(len(rem_list), dtype=int)\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem_index:\n                support_counts[rem_index[e]] += 1\n\n    avg_sup = np.sum(support_counts) / (len(support_counts) + 1e-12)\n    lam = 0.10 + 0.10 * avg_sup\n    lam = np.clip(lam, 0.05, 0.4)\n\n    scores = []\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        ss = set(s)\n        gain = len(rem & ss)\n        if gain > 0:\n            ov = len(ss & covered_selected)\n            score = gain - lam * ov - 0.01 * len(ss)\n            scores.append(score)\n        else:\n            scores.append(-np.inf)\n        idx += 1\n\n    scores_arr = np.array(scores)\n    top_k = min(3, len(scores_arr))\n    top_indices = np.argpartition(-scores_arr, top_k - 1)[:top_k]\n    chosen_idx = np.random.choice(top_indices)\n    return remaining_subsets[chosen_idx]\n\n",
  "adaptive_lambda_overlap_by_density_aug_273": "import numpy as np\nfrom typing import List, Optional, Set, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Backward iteration with squared penalty and clipped lambda.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    covered_selected = set().union(*selected_subsets)\n\n    support: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    avg_sup = np.median(list(support.values())) if support else 0.0\n    lam = 0.20 + 0.06 * avg_sup\n    lam = np.clip(lam, 0.12, 0.6)\n\n    best, best_score = None, -np.inf\n    idx = len(remaining_subsets) - 1\n    while idx >= 0:\n        s = remaining_subsets[idx]\n        ss = set(s)\n        gain = len(rem & ss)\n        if gain > 0:\n            ov = len(ss & covered_selected)\n            penalty = 0.01 * np.square(len(ss))\n            score = gain - lam * ov - penalty\n            if score > best_score:\n                best_score, best = score, s\n        idx -= 1\n    return best\n\n",
  "adaptive_lambda_overlap_by_density_aug_274": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Proxy approximation for average support and random top\u2011k selection.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    covered_selected = set().union(*selected_subsets)\n\n    rem_list = list(rem)\n    rem_index = {e: i for i, e in enumerate(rem_list)}\n    support_counts = np.zeros(len(rem_list), dtype=int)\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem_index:\n                support_counts[rem_index[e]] += 1\n\n    avg_sup = np.sum(support_counts) / (len(support_counts) + 1e-12)\n    lam = 0.12 + 0.09 * avg_sup\n    lam = np.clip(lam, 0.08, 0.45)\n\n    scores = []\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        ss = set(s)\n        gain = len(rem & ss)\n        if gain > 0:\n            ov = len(ss & covered_selected)\n            score = gain - lam * ov - 0.01 * len(ss)\n            scores.append((score, s))\n        idx += 1\n\n    if not scores:\n        return None\n\n    scores.sort(key=lambda x: x[0], reverse=True)\n    top_k = min(5, len(scores))\n    top_candidates = [s for _, s in scores[:top_k]]\n    chosen = np.random.choice(top_candidates)\n    return chosen\n\n",
  "cover_median_support_elements_aug_275": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Greedy selection preferring mid\u2011support elements with deterministic tie\u2011break.\"\"\"\n    remaining = set(remaining_elements)\n    if not remaining:\n        return None\n\n    # Build support counts using a list comprehension\n    support: Dict[int, int] = {}\n    for elem in remaining:\n        support[elem] = sum(1 for s in remaining_subsets if elem in s)\n\n    if not support:\n        return None\n\n    vals = np.array(list(support.values()), dtype=float)\n    lo, hi = np.quantile(vals, 0.35), np.quantile(vals, 0.65)\n    mid = {e for e, c in support.items() if lo <= c <= hi}\n\n    best_subset = None\n    best_tuple: Optional[tuple] = None\n    for s in remaining_subsets:\n        ss = set(s)\n        new = remaining & ss\n        if not new:\n            continue\n        mc = len(mid & ss)\n        t = (mc, len(new), -len(ss))\n        # deterministic noise to break ties\n        noise = 1e-9 * (hash(tuple(sorted(s))) % 1000)\n        t = (t[0] + noise, t[1], t[2])\n        if best_tuple is None or t > best_tuple:\n            best_tuple, best_subset = t, s\n    return best_subset\n\n",
  "cover_median_support_elements_aug_276": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Weighted scoring with clipping and deterministic noise.\"\"\"\n    remaining = set(remaining_elements)\n    if not remaining:\n        return None\n\n    support: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in remaining:\n                support[e] = support.get(e, 0) + 1\n\n    if not support:\n        return None\n\n    vals = np.array(list(support.values()), dtype=float)\n    lo, hi = np.quantile(vals, 0.35), np.quantile(vals, 0.65)\n    mid = {e for e, c in support.items() if lo <= c <= hi}\n\n    best_subset = None\n    best_score = -np.inf\n    for s in remaining_subsets:\n        ss = set(s)\n        new = remaining & ss\n        if not new:\n            continue\n        mc = len(mid & ss)\n        gain = len(new)\n        size = len(ss)\n\n        # Weighted score with clipping\n        score = 0.6 * mc + 0.3 * gain - 0.1 * size\n        score = np.clip(score, -1e3, 1e3)\n\n        # deterministic noise for tie\u2011breaking\n        noise = (hash(tuple(sorted(s))) & 0xFFFF) * 1e-6\n        score += noise\n\n        if score > best_score:\n            best_score = score\n            best_subset = s\n    return best_subset\n\n",
  "cover_median_support_elements_aug_277": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Softmax\u2011based selection with temperature scaling.\"\"\"\n    remaining = set(remaining_elements)\n    if not remaining:\n        return None\n\n    support: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in remaining:\n                support[e] = support.get(e, 0) + 1\n\n    if not support:\n        return None\n\n    vals = np.array(list(support.values()), dtype=float)\n    lo, hi = np.quantile(vals, 0.35), np.quantile(vals, 0.65)\n    mid = {e for e, c in support.items() if lo <= c <= hi}\n\n    raw_scores = []\n    subsets = []\n    for s in remaining_subsets:\n        ss = set(s)\n        new = remaining & ss\n        if not new:\n            continue\n        mc = len(mid & ss)\n        gain = len(new)\n        size = len(ss)\n        raw = mc + 0.5 * gain - 0.2 * size\n        raw_scores.append(raw)\n        subsets.append(s)\n\n    if not raw_scores:\n        return None\n\n    raw_scores = np.array(raw_scores, dtype=float)\n    temp = 0.5\n    exp_scores = np.exp((raw_scores - np.max(raw_scores)) / (temp + 1e-12))\n    probs = exp_scores / (np.sum(exp_scores) + 1e-12)\n    idx = np.argmax(probs)\n    return subsets[idx]\n\n",
  "cover_median_support_elements_aug_278": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Approximate support counts with np.bincount and clipped tuples.\"\"\"\n    remaining = set(remaining_elements)\n    if not remaining:\n        return None\n\n    # Flatten all subsets for efficient counting\n    flat = np.concatenate([np.array(s, dtype=int) for s in remaining_subsets])\n    max_elem = flat.max() if flat.size else -1\n    counts = np.bincount(flat, minlength=max_elem + 1)\n\n    support = {e: counts[e] for e in remaining if e <= max_elem}\n    if not support:\n        return None\n\n    vals = np.array(list(support.values()), dtype=float)\n    lo, hi = np.quantile(vals, 0.35), np.quantile(vals, 0.65)\n    mid = {e for e, c in support.items() if lo <= c <= hi}\n\n    best_subset = None\n    best_tuple: Optional[tuple] = None\n    for s in remaining_subsets:\n        ss = set(s)\n        new = remaining & ss\n        if not new:\n            continue\n        mc = len(mid & ss)\n        t = (mc, len(new), -len(ss))\n        # Clip each component\n        t = tuple(np.clip(ti, -1e6, 1e6) for ti in t)\n        if best_tuple is None or t > best_tuple:\n            best_tuple, best_subset = t, s\n    return best_subset\n\n",
  "quantile_rarity_bucket_vector_aug_279": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Variant 1 \u2013 uses deterministic noise for tie\u2011breaking and\n    a bucket system based on quantiles.  All thresholds are\n    clipped to avoid invalid bucket values.\n    \"\"\"\n    remaining_set = set(remaining_elements)\n    if not remaining_set:\n        return None\n\n    # Count support for each uncovered element\n    support = {}\n    for subset in remaining_subsets:\n        for elem in set(subset):\n            if elem in remaining_set:\n                support[elem] = support.get(elem, 0) + 1\n\n    if not support:\n        return None\n\n    counts = np.array(list(support.values()), dtype=float)\n    # Add epsilon to avoid division by zero in quantile calculation\n    q1 = float(np.quantile(counts, 0.25) + 1e-12)\n    q2 = float(np.quantile(counts, 0.50) + 1e-12)\n    q3 = float(np.quantile(counts, 0.75) + 1e-12)\n\n    # Ensure bucket thresholds stay within a sane range\n    q1, q2, q3 = np.clip([q1, q2, q3], 0, np.inf)\n\n    def bucket(c: int) -> int:\n        if c <= q1:\n            return 0\n        if c <= q2:\n            return 1\n        if c <= q3:\n            return 2\n        return 3\n\n    best_subset, best_vec = None, None\n    for subset in remaining_subsets:\n        uncovered = [e for e in set(subset) if e in remaining_set]\n        if not uncovered:\n            continue\n\n        counts_arr = [0, 0, 0, 0]\n        for e in uncovered:\n            counts_arr[bucket(support[e])] += 1\n\n        # Deterministic noise derived from hash of the subset\n        noise = (hash(tuple(sorted(subset))) % 1000) * 1e-6\n        vec = (counts_arr[0], counts_arr[1], counts_arr[2], counts_arr[3],\n               -len(subset), noise)\n        if best_vec is None or vec > best_vec:\n            best_vec, best_subset = vec, subset\n\n    return best_subset\n\n",
  "quantile_rarity_bucket_vector_aug_280": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Variant 3 \u2013 uses np.median for bucket thresholds,\n    np.max for aggregation, and random tie\u2011breaking\n    among the best candidates.  All operations are\n    bounded with np.clip.\n    \"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support = {}\n    for subset in remaining_subsets:\n        for e in set(subset):\n            if e in rem:\n                support[e] = support.get(e, 0) + 1\n\n    if not support:\n        return None\n\n    counts = np.array(list(support.values()), dtype=float)\n    q1 = float(np.quantile(counts, 0.25) + 1e-12)\n    q2 = float(np.quantile(counts, 0.50) + 1e-12)\n    q3 = float(np.quantile(counts, 0.75) + 1e-12)\n\n    q1, q2, q3 = np.clip([q1, q2, q3], 0, np.inf)\n\n    def bucket(c: int) -> int:\n        if c <= q1:\n            return 0\n        if c <= q2:\n            return 1\n        if c <= q3:\n            return 2\n        return 3\n\n    best_vec = None\n    best_subset = None\n    for subset in remaining_subsets:\n        uncovered = [e for e in set(subset) if e in rem]\n        if not uncovered:\n            continue\n\n        counts_arr = [0, 0, 0, 0]\n        for e in uncovered:\n            counts_arr[bucket(support[e])] += 1\n\n        # Use np.max to aggregate bucket counts\n        vec = (np.max(counts_arr), -len(subset))\n        if best_vec is None or vec > best_vec:\n            best_vec, best_subset = vec, subset\n        elif vec == best_vec:\n            # Random tie\u2011break among equally good subsets\n            if np.random.rand() < 0.5:\n                best_subset = subset\n\n    return best_subset\n\n",
  "quantile_rarity_bucket_vector_aug_281": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Variant 4 \u2013 while\u2011loop control flow, inverted conditionals,\n    proxy approximation of support using np.mean, and\n    np.clip to keep bucket indices in [0,3].\n    \"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support = {}\n    i = 0\n    while i < len(remaining_subsets):\n        subset = remaining_subsets[i]\n        for e in set(subset):\n            if e in rem:\n                support[e] = support.get(e, 0) + 1\n        i += 1\n\n    if not support:\n        return None\n\n    counts = np.array(list(support.values()), dtype=float)\n    # Proxy approximation: use mean instead of exact support\n    mean_val = np.mean(counts) + 1e-12\n    q1 = float(np.quantile(counts, 0.25) + 1e-12)\n    q2 = float(np.quantile(counts, 0.50) + 1e-12)\n    q3 = float(np.quantile(counts, 0.75) + 1e-12)\n\n    q1, q2, q3 = np.clip([q1, q2, q3], 0, np.inf)\n\n    def bucket(c: int) -> int:\n        if c >= q3:\n            return 3\n        if c >= q2:\n            return 2\n        if c >= q1:\n            return 1\n        return 0\n\n    best_subset = None\n    best_score = -np.inf\n    j = 0\n    while j < len(remaining_subsets):\n        subset = remaining_subsets[j]\n        uncovered = [e for e in set(subset) if e in rem]\n        if uncovered:\n            # Approximate score: sum of bucket indices weighted by support\n            score = sum(bucket(support[e]) * support[e] for e in uncovered)\n            if score > best_score:\n                best_score, best_subset = score, subset\n        j += 1\n\n    return best_subset\n\n",
  "stochastic_two_arm_bandit_gain_vs_rarity_aug_282": "import numpy as np\nimport random\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Randomly choose between (max gain) and (max rarity-weighted) using a progress-based mixing probability.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    n_rem = max(1, len(rem_set))\n    n_total = n_rem + sum(len(set(s)) for s in selected_subsets)\n    progress = 1.0 - n_rem / max(1, n_total)\n    progress = np.clip(progress, 0.0, 1.0)\n\n    p_rarity = 0.20 + 0.60 * progress\n\n    if random.random() > p_rarity:\n        best = None\n        best_gain = 0\n        i = 0\n        while i < len(remaining_subsets):\n            s = remaining_subsets[i]\n            g = len(rem_set.intersection(s))\n            if g > best_gain:\n                best_gain, best = g, s\n            i += 1\n        return best if best_gain > 0 else None\n\n    freq: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem_set:\n                freq[e] = freq.get(e, 0) + 1\n\n    scores = []\n    for s in remaining_subsets:\n        sc = 0.0\n        for e in set(s):\n            if e in rem_set:\n                sc += 1.0 / (freq.get(e, 1) + 1e-12)\n        scores.append(sc)\n\n    scores = np.array(scores)\n    if np.all(scores <= 0):\n        return None\n    top_k = min(5, len(scores))\n    top_indices = np.argsort(-scores)[:top_k]\n    chosen = random.choice(top_indices)\n    return remaining_subsets[chosen]\n\n",
  "stochastic_two_arm_bandit_gain_vs_rarity_aug_283": "import numpy as np\nimport random\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    n_rem = max(1, len(rem_set))\n    n_total = n_rem + sum(len(set(s)) for s in selected_subsets)\n    progress = 1.0 - n_rem / max(1, n_total)\n    progress = np.clip(progress, 0.0, 1.0)\n\n    p_rarity = 0.15 + 0.65 * progress\n\n    if random.random() <= p_rarity:\n        freq: Dict[int, int] = {}\n        for s in remaining_subsets:\n            for e in set(s):\n                if e in rem_set:\n                    freq[e] = freq.get(e, 0) + 1\n\n        scores = np.array([np.mean([1.0 / (freq.get(e, 1) + 1e-12) for e in set(s) if e in rem_set])\n                           for s in remaining_subsets])\n\n        if np.all(scores <= 0):\n            return None\n\n        # deterministic noise for tie\u2011breaking\n        deterministic_noise = np.array([(hash(tuple(sorted(s))) % 1000) * 1e-6\n                                         for s in remaining_subsets])\n        scores += deterministic_noise\n\n        top_k = min(3, len(scores))\n        top_indices = np.argsort(-scores)[:top_k]\n        chosen = random.choice(top_indices)\n        return remaining_subsets[chosen]\n    else:\n        gains = np.array([len(rem_set.intersection(s)) for s in remaining_subsets])\n        max_gain = np.max(gains)\n        if max_gain == 0:\n            return None\n        best_idx = np.argmax(gains)\n        return remaining_subsets[best_idx]\n\n",
  "stochastic_two_arm_bandit_gain_vs_rarity_aug_284": "import numpy as np\nimport random\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    n_rem = max(1, len(rem_set))\n    n_total = n_rem + sum(len(set(s)) for s in selected_subsets)\n    progress = 1.0 - n_rem / max(1, n_total)\n    progress = np.clip(progress, 0.0, 1.0)\n\n    p_rarity = 0.30 + 0.50 * progress\n\n    if random.random() > p_rarity:\n        gains = np.array([len(rem_set.intersection(s)) for s in remaining_subsets])\n        max_gain = np.max(gains)\n        if max_gain == 0:\n            return None\n        best_idx = np.argmax(gains)\n        return remaining_subsets[best_idx]\n\n    # approximate frequencies using a random sample of subsets\n    sample_size = min(10, len(remaining_subsets))\n    sample = random.sample(remaining_subsets, sample_size)\n    freq: Dict[int, int] = {}\n    for s in sample:\n        for e in set(s):\n            if e in rem_set:\n                freq[e] = freq.get(e, 0) + 1\n\n    scores = []\n    for s in remaining_subsets:\n        sc = 0.0\n        for e in set(s):\n            if e in rem_set:\n                sc += 1.0 / (freq.get(e, 1) + 1e-12)\n        scores.append(sc)\n\n    scores = np.array(scores)\n    if np.all(scores <= 0):\n        return None\n\n    top_k = min(4, len(scores))\n    top_indices = np.argsort(-scores)[:top_k]\n    chosen = random.choice(top_indices)\n    return remaining_subsets[chosen]\n\n",
  "topk_then_best_rarity_aug_285": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Syntactic rewrite with list comprehensions and deterministic noise.\"\"\"\n    rem_set = set(remaining_elements)\n    scored = []\n\n    i = 0\n    while i < len(remaining_subsets):\n        s = remaining_subsets[i]\n        inter = rem_set.intersection(s)\n        g = len(inter)\n        if g > 0:\n            scored.append((g, s))\n        i += 1\n\n    if not scored:\n        return None\n\n    scored.sort(key=lambda x: x[0], reverse=True)\n    top_k = max(1, int(np.ceil(0.20 * len(scored))))\n    top = [s for _, s in scored[:top_k]]\n\n    freq: dict[int, int] = {}\n    for s in top:\n        for e in set(s):\n            if e in rem_set:\n                freq[e] = freq.get(e, 0) + 1\n\n    best = None\n    best_score = -np.inf\n    for s in top:\n        ss = set(s)\n        new_elems = [e for e in ss if e in rem_set]\n        if not new_elems:\n            continue\n        # deterministic noise based on subset contents\n        noise = (hash(tuple(sorted(new_elems))) % 1000) / 1e6\n        rarity = sum(1.0 / (freq.get(e, 1) + 1e-12) for e in new_elems)\n        score = rarity + 0.07 * len(new_elems) + noise\n        score = np.clip(score, 0, 1e9)\n        if score > best_score:\n            best_score = score\n            best = s\n    return best\n\n",
  "topk_then_best_rarity_aug_286": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorized numpy implementation with median-based scoring.\"\"\"\n    rem_arr = np.array(remaining_elements, dtype=int)\n    scored = []\n\n    for s in remaining_subsets:\n        s_arr = np.unique(np.array(s, dtype=int))\n        inter = np.intersect1d(rem_arr, s_arr)\n        g = len(inter)\n        if g > 0:\n            scored.append((g, s, inter))\n\n    if not scored:\n        return None\n\n    scored.sort(key=lambda x: x[0], reverse=True)\n    top_k = 5\n    top = [item[1] for item in scored[:top_k]]\n\n    freq: dict[int, int] = {}\n    for s in top:\n        for e in set(s):\n            if e in rem_arr:\n                freq[e] = freq.get(e, 0) + 1\n\n    best = None\n    best_score = -np.inf\n    for s in top:\n        ss = set(s)\n        new_elems = [e for e in ss if e in rem_arr]\n        if not new_elems:\n            continue\n        # compute mean reciprocal frequency\n        recips = [1.0 / (freq.get(e, 1) + 1e-12) for e in new_elems]\n        rarity = np.mean(recips)\n        score = rarity + 0.04 * len(new_elems)\n        if score > best_score or (abs(score - best_score) < 1e-12 and sum(new_elems) < sum(best)):\n            best_score = score\n            best = s\n    return best\n\n",
  "topk_then_best_rarity_aug_287": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"While-loop implementation with np.max aggregation and deterministic noise.\"\"\"\n    rem_set = set(remaining_elements)\n    scored = []\n\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        g = len(rem_set.intersection(s))\n        if g > 0:\n            scored.append((g, s))\n        idx += 1\n\n    if not scored:\n        return None\n\n    scored.sort(key=lambda x: x[0], reverse=True)\n    top_k = max(1, int(np.ceil(0.25 * len(scored))))\n    top = [s for _, s in scored[:top_k]]\n\n    freq: dict[int, int] = {}\n    for s in top:\n        for e in set(s):\n            if e in rem_set:\n                freq[e] = freq.get(e, 0) + 1\n\n    best = None\n    best_score = -np.inf\n    for s in top:\n        ss = set(s)\n        new_elems = [e for e in ss if e in rem_set]\n        if not new_elems:\n            continue\n        # use np.max of reciprocal frequencies\n        recips = np.array([1.0 / (freq.get(e, 1) + 1e-12) for e in new_elems])\n        rarity = np.max(recips)\n        # deterministic noise\n        noise = (sum(new_elems) % 1000) / 1e6\n        score = rarity + 0.06 * len(new_elems) + noise\n        score = np.clip(score, 0, 1e9)\n        if score > best_score:\n            best_score = score\n            best = s\n    return best\n\n",
  "topk_then_best_diversity_aug_288": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selection with a 25\u202f% top\u2011k filter, Jaccard distance averaged over\n    already selected sets, and a small random perturbation for deterministic\n    tie\u2011breaking.  All divisions are protected with a tiny epsilon and\n    distances are clipped to [0, 1].\n    \"\"\"\n    # Current uncovered elements\n    rem_set: Set[int] = set(remaining_elements)\n\n    # Compute gain for each remaining subset\n    scored_gain = [\n        (len(rem_set & set(s)), s)\n        for s in remaining_subsets\n        if len(rem_set & set(s)) > 0\n    ]\n    if not scored_gain:\n        return None\n\n    # Sort by gain and keep the top 25\u202f% (at least one)\n    scored_gain.sort(key=lambda x: x[0], reverse=True)\n    k = max(1, int(np.ceil(0.25 * len(scored_gain))))\n    top_subsets = [s for _, s in scored_gain[:k]]\n\n    # Pre\u2011convert selected subsets to sets for distance calculations\n    sel_sets = [set(s) for s in selected_subsets] if selected_subsets else []\n\n    best_subset = None\n    best_score = -np.inf\n\n    for s in top_subsets:\n        s_set = set(s)\n        gain = len(rem_set & s_set)\n        if gain == 0:\n            continue\n\n        if not sel_sets:\n            dist = 1.0\n        else:\n            # Average Jaccard distance to already selected subsets\n            dists = [\n                1.0 - (len(s_set & t) / (len(s_set | t) + 1e-12))\n                for t in sel_sets\n            ]\n            dist = np.clip(np.mean(dists), 0.0, 1.0)\n\n        # Small random noise to break ties deterministically\n        noise = 1e-6 * np.random.rand()\n        score = gain + 0.8 * dist - 0.015 * len(s_set) + noise\n\n        if score > best_score:\n            best_score, best_subset = score, s\n\n    return best_subset\n\n",
  "topk_then_best_diversity_aug_289": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Uses a 30\u202f% top\u2011k filter, replaces the mean Jaccard distance with its\n    maximum value, and selects a random element among the top\u2011k when scores\n    are equal.  Distances are clipped and all divisions are protected.\n    \"\"\"\n    universe = set(remaining_elements)\n    # Compute gains and keep those with positive gain\n    gains = [(len(universe & set(s)), s) for s in remaining_subsets]\n    gains = [(g, s) for g, s in gains if g > 0]\n    if not gains:\n        return None\n\n    # 30\u202f% top\u2011k\n    gains.sort(key=lambda x: x[0], reverse=True)\n    k = max(1, int(np.ceil(0.30 * len(gains))))\n    top_k = [s for _, s in gains[:k]]\n\n    sel_sets = [set(s) for s in selected_subsets] if selected_subsets else []\n\n    best_sub = None\n    best_val = -np.inf\n    candidates = []\n\n    for s in top_k:\n        s_set = set(s)\n        gain = len(universe & s_set)\n        if gain == 0:\n            continue\n\n        if not sel_sets:\n            dist = 1.0\n        else:\n            # Max Jaccard distance to selected subsets\n            dists = [\n                1.0 - (len(s_set & t) / (len(s_set | t) + 1e-12))\n                for t in sel_sets\n            ]\n            dist = np.clip(max(dists), 0.0, 1.0)\n\n        score = 0.7 * dist + 0.3 * gain - 0.03 * len(s_set)\n\n        if score > best_val:\n            best_val, best_sub, candidates = score, s, [s]\n        elif np.isclose(score, best_val, atol=1e-12):\n            candidates.append(s)\n\n    # Randomly pick among tied candidates\n    return np.random.choice(candidates) if candidates else best_sub\n\n",
  "topk_then_best_diversity_aug_290": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Implements a soft\u2011min selection over the top\u2011k subsets and uses a\n    proxy Jaccard distance based on set size ratios to avoid explicit set\n    operations.  Distances are clipped and an epsilon protects denominators.\n    \"\"\"\n    rem = set(remaining_elements)\n\n    # Gain via intersection size\n    gains = [(len(rem & set(s)), s) for s in remaining_subsets]\n    gains = [(g, s) for g, s in gains if g > 0]\n    if not gains:\n        return None\n\n    # 20\u202f% top\u2011k\n    gains.sort(key=lambda x: x[0], reverse=True)\n    k = max(1, int(np.ceil(0.20 * len(gains))))\n    top = [s for _, s in gains[:k]]\n\n    sel = [set(s) for s in selected_subsets] if selected_subsets else []\n\n    # Pre\u2011compute sizes for proxy distance\n    sel_sizes = [len(t) for t in sel]\n\n    best, best_score = None, -np.inf\n\n    for s in top:\n        s_set = set(s)\n        gain = len(rem & s_set)\n        if gain == 0:\n            continue\n\n        if not sel:\n            dist = 1.0\n        else:\n            # Proxy: |s| / (|s| + |t|) as a surrogate for Jaccard\n            proxies = [\n                1.0 - (len(s_set & t) / (len(s_set) + len(t) + 1e-12))\n                for t in sel\n            ]\n            dist = np.clip(np.median(proxies), 0.0, 1.0)\n\n        # Soft\u2011min weighting: use exp(-score) to favor lower scores\n        score = gain + 0.75 * dist - 0.02 * len(s_set)\n        soft = np.exp(-score)\n        if soft > best_score:\n            best_score, best = soft, s\n\n    return best\n\n",
  "topk_then_best_diversity_aug_291": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Vectorised implementation: each subset is encoded as a binary vector\n    over the universe of remaining elements.  Gains are dot products,\n    Jaccard distances are computed in bulk, and a softmax over the\n    weighted scores chooses the next subset.  All divisions are\n    protected with epsilon and outputs are clipped to valid ranges.\n    \"\"\"\n    # Build mapping from element to column index\n    elem_to_idx = {e: i for i, e in enumerate(remaining_elements)}\n    n = len(remaining_elements)\n\n    # Encode remaining subsets as binary matrix\n    mat = np.zeros((len(remaining_subsets), n), dtype=np.int8)\n    for i, s in enumerate(remaining_subsets):\n        for e in s:\n            if e in elem_to_idx:\n                mat[i, elem_to_idx[e]] = 1\n\n    # Current uncovered mask\n    uncovered = np.ones(n, dtype=np.int8)\n\n    # Compute gains (dot product)\n    gains = mat @ uncovered\n    valid = gains > 0\n    if not np.any(valid):\n        return None\n\n    # 25\u202f% top\u2011k\n    sorted_idx = np.argsort(-gains)\n    k = max(1, int(np.ceil(0.25 * np.count_nonzero(valid))))\n    top_idx = sorted_idx[valid[sorted_idx]][:k]\n\n    # Selected subsets encoded as binary matrix\n    if selected_subsets:\n        sel_mat = np.zeros((len(selected_subsets), n), dtype=np.int8)\n        for i, s in enumerate(selected_subsets):\n            for e in s:\n                if e in elem_to_idx:\n                    sel_mat[i, elem_to_idx[e]] = 1\n        # Compute Jaccard distance to each selected set\n        inter = sel_mat @ mat[top_idx].T  # shape (|sel|, k)\n        unions = sel_mat.sum(axis=1, keepdims=True) + mat[top_idx].sum(axis=1) - inter\n        dist = 1.0 - inter / (unions + 1e-12)  # shape (|sel|, k)\n        avg_dist = np.clip(dist.mean(axis=0), 0.0, 1.0)  # shape (k,)\n    else:\n        avg_dist = np.ones(len(top_idx))\n\n    # Scores: weighted sum of gain and distance, penalise size\n    sizes = np.array([len(s) for s in remaining_subsets])\n    score = gains[top_idx] + 0.8 * avg_dist - 0.01 * sizes[top_idx]\n    # Softmax for selection\n    exp_score = np.exp(score - np.max(score))\n    probs = exp_score / (exp_score.sum() + 1e-12)\n    chosen_idx = np.random.choice(top_idx, p=probs)\n\n    return remaining_subsets[chosen_idx]\n\n",
  "cover_last_uncovered_cluster_aug_292": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Late\u2011stage bias: choose the set that maximizes gain while minimizing waste.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    if len(rem_set) <= 10:\n        # Vectorised computation of gains and wastes\n        gains = np.array([len(rem_set & set(s)) for s in remaining_subsets], dtype=int)\n        wastes = np.array([len(set(s) - rem_set) for s in remaining_subsets], dtype=int)\n        mask = gains > 0\n        if not mask.any():\n            return None\n        # Ratio of gain to waste, with epsilon to avoid division by zero\n        ratio = gains / (np.clip(wastes, 1, None) + 1e-12)\n        # Deterministic tie\u2011breaking noise\n        ratio += np.arange(len(remaining_subsets)) * 1e-6\n        best_idx = int(np.argmax(ratio))\n        return remaining_subsets[best_idx]\n    else:\n        # Normal max\u2011gain selection\n        gains = np.array([len(rem_set & set(s)) for s in remaining_subsets], dtype=int)\n        if (gains <= 0).all():\n            return None\n        best_idx = int(np.argmax(gains))\n        return remaining_subsets[best_idx]\n\n",
  "cover_last_uncovered_cluster_aug_293": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Weighted selection with top\u2011k random choice and deterministic noise.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    rng = np.random.default_rng(42)  # deterministic seed\n\n    if len(rem_set) <= 10:\n        best_score = -np.inf\n        best_subset = None\n        for idx, s in enumerate(remaining_subsets):\n            ss = set(s)\n            g = len(rem_set & ss)\n            if g == 0:\n                continue\n            w = len(ss - rem_set)\n            # Weighted scoring\n            score = 0.6 * g - 0.4 * w\n            score = np.clip(score, 0, 100)  # bounded\n            score += idx * 1e-7  # deterministic noise\n            if score > best_score:\n                best_score, best_subset = score, s\n\n        if best_subset is None:\n            return None\n\n        # Build score list for top\u2011k selection\n        scores = np.array([0.6 * len(rem_set & set(s)) - 0.4 * len(set(s) - rem_set)\n                           for s in remaining_subsets], dtype=float)\n        scores = np.clip(scores, 0, 100)\n        top_k = min(7, len(scores))\n        top_indices = np.argpartition(-scores, top_k - 1)[:top_k]\n        chosen_idx = rng.choice(top_indices)\n        return remaining_subsets[chosen_idx]\n    else:\n        gains = np.array([len(rem_set & set(s)) for s in remaining_subsets], dtype=int)\n        if (gains <= 0).all():\n            return None\n        best_idx = int(np.argmax(gains))\n        return remaining_subsets[best_idx]\n\n",
  "cover_last_uncovered_cluster_aug_294": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Softmin probability selection with deterministic noise.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    rng = np.random.default_rng(123)  # deterministic seed\n\n    if len(rem_set) <= 10:\n        gains = np.array([len(rem_set & set(s)) for s in remaining_subsets], dtype=int)\n        wastes = np.array([len(set(s) - rem_set) for s in remaining_subsets], dtype=int)\n        mask = gains > 0\n        if not mask.any():\n            return None\n        ratio = gains / (np.clip(wastes, 1, None) + 1e-12)\n        ratio += np.arange(len(remaining_subsets)) * 1e-8  # deterministic noise\n        # Softmin weights\n        weights = np.exp(-ratio)\n        probs = weights / weights.sum()\n        chosen_idx = rng.choice(len(remaining_subsets), p=probs)\n        return remaining_subsets[chosen_idx]\n    else:\n        gains = np.array([len(rem_set & set(s)) for s in remaining_subsets], dtype=int)\n        if (gains <= 0).all():\n            return None\n        best_idx = int(np.argmax(gains))\n        return remaining_subsets[best_idx]\n\n",
  "cover_last_uncovered_cluster_aug_295": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Median\u2011thresholded ratio selection with minimal waste tie\u2011break.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    if len(rem_set) <= 10:\n        gains = np.array([len(rem_set & set(s)) for s in remaining_subsets], dtype=int)\n        wastes = np.array([len(set(s) - rem_set) for s in remaining_subsets], dtype=int)\n        mask = gains > 0\n        if not mask.any():\n            return None\n        ratio = gains / (np.clip(wastes, 1, None) + 1e-12)\n        median_ratio = np.median(ratio[mask])\n        # Candidate indices with ratio above median\n        cand_idx = np.where((ratio >= median_ratio) & mask)[0]\n        if cand_idx.size == 0:\n            cand_idx = np.where(mask)[0]\n        # Choose candidate with minimal waste\n        cand_waste = wastes[cand_idx]\n        min_waste = np.min(cand_waste)\n        best_candidates = cand_idx[np.where(cand_waste == min_waste)[0]]\n        # Deterministic noise for tie\u2011break\n        best_idx = best_candidates[0]  # first after noise addition\n        return remaining_subsets[best_idx]\n    else:\n        gains = np.array([len(rem_set & set(s)) for s in remaining_subsets], dtype=int)\n        if (gains <= 0).all():\n            return None\n        wastes = np.array([len(set(s) - rem_set) for s in remaining_subsets], dtype=int)\n        median_gain = np.median(gains)\n        mask = gains >= median_gain\n        if not mask.any():\n            mask = gains > 0\n        cand_idx = np.where(mask)[0]\n        cand_waste = wastes[cand_idx]\n        min_waste = np.min(cand_waste)\n        best_candidates = cand_idx[np.where(cand_waste == min_waste)[0]]\n        best_idx = best_candidates[0]\n        return remaining_subsets[best_idx]\n\n",
  "min_waste_per_gain_aug_296": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorised greedy selector with deterministic noise for tie\u2011breaking.\"\"\"\n    # Convert the remaining elements to a set for fast membership tests\n    elems = set(remaining_elements)\n\n    # Compute waste/gain for every subset in a vectorised manner\n    gains = np.array([len(elems & set(s)) for s in remaining_subsets], dtype=np.float64)\n    wastes = np.array([len(set(s) - elems) for s in remaining_subsets], dtype=np.float64)\n\n    # Avoid division by zero with a tiny epsilon\n    eps = 1e-12\n    scores = np.divide(wastes, gains + eps)\n\n    # Clip scores to a sensible range to avoid overflow\n    scores = np.clip(scores, 0.0, 1e6)\n\n    # Add a tiny deterministic noise to break ties in a reproducible way\n    noise = np.arange(len(scores), dtype=np.float64) * 1e-6\n    scores += noise\n\n    # Choose the subset with the minimal score\n    best_idx = int(np.argmin(scores))\n    best_subset = remaining_subsets[best_idx] if remaining_subsets else None\n    return best_subset\n\n",
  "min_waste_per_gain_aug_297": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Greedy selector using a soft\u2011min probability distribution.\"\"\"\n    elems = set(remaining_elements)\n\n    # Gather scores in a loop (syntactic change: while instead of for)\n    scores = []\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        ss = set(s)\n        gain = len(elems & ss)\n        if gain > 0:\n            waste = len(ss - elems)\n            # Soft\u2011min: use exponential weighting\n            score = np.exp(-waste / (gain + 1e-12))\n            scores.append(score)\n        else:\n            scores.append(0.0)\n        idx += 1\n\n    scores = np.array(scores, dtype=np.float64)\n    # Normalise to probabilities\n    total = np.sum(scores) + 1e-12\n    probs = scores / total\n\n    # Deterministic random choice based on the fixed seed\n    rng = np.random.default_rng(12345)\n    chosen_idx = int(rng.choice(len(remaining_subsets), p=probs))\n    return remaining_subsets[chosen_idx] if remaining_subsets else None\n\n",
  "min_waste_per_gain_aug_298": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Median\u2011based scoring with random top\u2011k selection.\"\"\"\n    elems = set(remaining_elements)\n\n    # Compute waste and gain for all subsets\n    waste_arr = np.array([len(set(s) - elems) for s in remaining_subsets], dtype=np.float64)\n    gain_arr = np.array([len(elems & set(s)) for s in remaining_subsets], dtype=np.float64)\n\n    # Use median of waste as a normalising factor\n    median_waste = np.median(waste_arr)\n    eps = 1e-12\n    scores = (waste_arr - median_waste) / (gain_arr + eps)\n\n    # Clip to avoid extreme values\n    scores = np.clip(scores, -1e6, 1e6)\n\n    # Select indices with the lowest scores (top\u2011k)\n    k = min(3, len(scores))\n    top_k_indices = np.argpartition(scores, k-1)[:k]\n    # Randomly pick one of the top\u2011k candidates\n    rng = np.random.default_rng(9876)\n    chosen_idx = int(rng.choice(top_k_indices))\n    return remaining_subsets[chosen_idx] if remaining_subsets else None\n\n",
  "min_waste_per_gain_aug_299": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Selector with clipped scores and deterministic noise for tie\u2011breaking.\"\"\"\n    elems = set(remaining_elements)\n\n    # Vectorised calculation of gains and wastes\n    gains = np.array([len(elems & set(s)) for s in remaining_subsets], dtype=np.float64)\n    wastes = np.array([len(set(s) - elems) for s in remaining_subsets], dtype=np.float64)\n\n    # Compute raw scores, ensuring no division by zero\n    eps = 1e-12\n    raw_scores = np.divide(wastes, gains + eps)\n\n    # Clip scores to a bounded interval [0, 1e4] to avoid overflow\n    scores = np.clip(raw_scores, 0.0, 1e4)\n\n    # Add deterministic noise for tie\u2011breaking (same seed each run)\n    rng = np.random.default_rng(42)\n    noise = rng.uniform(-1e-6, 1e-6, size=scores.shape)\n    scores += noise\n\n    # Pick the subset with the minimal (noisy) score\n    best_idx = int(np.argmin(scores))\n    return remaining_subsets[best_idx] if remaining_subsets else None\n\n",
  "gain_minus_waste_aug_300": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_arr = np.array(remaining_elements, dtype=int)\n    alpha = 0.35\n    best_subset = None\n    best_score = -np.inf\n\n    for idx, subset in enumerate(remaining_subsets):\n        subset_arr = np.array(subset, dtype=int)\n        gain = np.sum(np.isin(subset_arr, rem_arr))\n        if gain == 0:\n            continue\n        waste = len(subset) - gain\n        score = gain - alpha * waste\n        # deterministic tie\u2011breaking noise\n        score += 1e-6 * idx\n        score = np.clip(score, -1e18, 1e18)\n        if score > best_score:\n            best_score, best_subset = score, subset\n\n    return best_subset\n\n",
  "gain_minus_waste_aug_301": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    alpha = 0.4\n    best_subset = None\n    best_score = -np.inf\n    idx = 0\n\n    while idx < len(remaining_subsets):\n        subset = remaining_subsets[idx]\n        ss = set(subset)\n        gain = len(rem_set.intersection(ss))\n        # Invert conditional: only process when gain is positive\n        if gain > 0:\n            waste = len(ss - rem_set)\n            score = gain - alpha * waste\n            # clip to avoid extreme values\n            score = np.clip(score, -1e18, 1e18)\n            # deterministic tie\u2011breaking noise\n            score += 1e-7 * idx\n            if score > best_score:\n                best_score, best_subset = score, subset\n        idx += 1\n\n    return best_subset\n\n",
  "gain_minus_waste_aug_302": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    alpha = 0.35\n    scores = []\n\n    for subset in remaining_subsets:\n        ss = set(subset)\n        gain = len(rem_set.intersection(ss))\n        if gain == 0:\n            scores.append(-np.inf)\n            continue\n        waste = len(ss - rem_set)\n        scores.append(gain - alpha * waste)\n\n    scores_np = np.array(scores, dtype=float)\n\n    # Softmax probabilities for selection\n    max_score = np.max(scores_np)\n    exp_scores = np.exp(scores_np - max_score)  # numerical stability\n    probs = exp_scores / (np.sum(exp_scores) + 1e-12)  # epsilon to avoid div\u2011by\u2011zero\n\n    # Randomly pick among the top\u2011k subsets\n    top_k = min(3, len(remaining_subsets))\n    top_indices = np.argpartition(-scores_np, top_k - 1)[:top_k]\n    top_probs = probs[top_indices]\n    top_probs = top_probs / (np.sum(top_probs) + 1e-12)\n\n    chosen_idx = np.random.choice(top_indices, p=top_probs)\n    return remaining_subsets[chosen_idx]\n\n",
  "gain_minus_waste_aug_303": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    alpha = 0.5  # different weight for waste\n    best_subset = None\n    best_score = -np.inf\n\n    for subset in remaining_subsets:\n        ss = set(subset)\n        gain = len(rem_set.intersection(ss))\n        if gain == 0:\n            continue\n\n        # Proxy waste: ratio of non\u2011remaining elements in the subset\n        waste_ratio = len(ss - rem_set) / (len(ss) + 1e-12)  # epsilon to avoid div\u2011by\u2011zero\n        waste_ratio = np.clip(waste_ratio, 0.0, 1.0)          # bound to [0, 1]\n\n        # Score uses the waste ratio as a proxy\n        score = gain - alpha * waste_ratio * len(ss)\n        score = np.clip(score, -1e18, 1e18)\n\n        if score > best_score:\n            best_score, best_subset = score, subset\n\n    return best_subset\n\n",
  "coverage_balance_to_target_aug_304": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Vectorized selection using lexicographic score (distance to target, then gain).\n    \"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Target: 12\u202f% of the remaining elements, but at least 1\n    target = max(1, int(np.ceil(0.12 * len(rem))))\n\n    # Compute gains for all remaining subsets\n    gains = np.array([len(rem & set(s)) for s in remaining_subsets], dtype=int)\n\n    # Keep only subsets that actually cover something\n    valid = gains > 0\n    if not np.any(valid):\n        return None\n\n    # Build a 2\u2011column score: first column is -|gain\u2011target|, second is gain\n    scores = np.column_stack((-np.abs(gains - target), gains))\n\n    # Lexicographic sort (higher values first) gives index of best score\n    best_idx = np.lexsort((scores[:, 1], scores[:, 0]))[-1]\n    return remaining_subsets[best_idx]\n\n",
  "coverage_balance_to_target_aug_305": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Randomly pick among the top\u20113 subsets by a noisy score.\n    \"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Raise the target to 15\u202f% of the remaining elements\n    target = max(1, int(np.ceil(0.15 * len(rem))))\n\n    # Vectorized gains\n    gains = np.array([len(rem & set(s)) for s in remaining_subsets], dtype=int)\n    valid_idx = np.where(gains > 0)[0]\n    if valid_idx.size == 0:\n        return None\n\n    # Deterministic tiny noise for stable tie\u2011breaking\n    rng = np.random.default_rng(42)\n    noise = rng.uniform(-1e-6, 1e-6, size=gains.shape)\n\n    # Score: closer to target is better, with noise added\n    scores = -np.abs(gains - target) + noise\n\n    # Pick the top\u20113 scores\n    top_k = min(3, valid_idx.size)\n    top_indices = np.argpartition(-scores, top_k - 1)[:top_k]\n\n    # Randomly choose one of the top\u20113\n    chosen = rng.choice(valid_idx[top_indices])\n    return remaining_subsets[chosen]\n\n",
  "coverage_balance_to_target_aug_306": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Soft\u2011min selection: probability proportional to exp(-|gain\u2011target|).\n    \"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    target = max(1, int(np.ceil(0.12 * len(rem))))\n\n    gains = np.array([len(rem & set(s)) for s in remaining_subsets], dtype=int)\n    valid_idx = np.where(gains > 0)[0]\n    if valid_idx.size == 0:\n        return None\n\n    # Soft\u2011min weights: closer to target => larger weight\n    diff = np.abs(gains[valid_idx] - target).astype(float)\n    diff = np.clip(diff, 1e-12, None)          # avoid division by zero\n    weights = np.exp(-diff)\n\n    # Normalise to probabilities\n    total = np.sum(weights)\n    probs = weights / (total + 1e-12)\n\n    rng = np.random.default_rng(123)\n    chosen = rng.choice(valid_idx, p=probs)\n    return remaining_subsets[chosen]\n\n",
  "coverage_balance_to_target_aug_307": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Proxy\u2011based scoring with deterministic noise and bounded target.\n    \"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Clip target to a valid range\n    target = int(np.clip(np.ceil(0.12 * len(rem)), 1, len(rem)))\n\n    # Proxy for gain: set size weighted by fraction of remaining elements\n    total_rem = len(rem)\n    gains = np.array([len(s) * (total_rem / (total_rem + 1e-12))\n                      for s in remaining_subsets], dtype=float)\n\n    # Keep only subsets that actually intersect the remaining set\n    valid_idx = [i for i, s in enumerate(remaining_subsets) if rem & set(s)]\n    if not valid_idx:\n        return None\n    gains = gains[valid_idx]\n\n    # Deterministic noise for tie\u2011breaking\n    rng = np.random.default_rng(7)\n    noise = rng.uniform(-1e-6, 1e-6, size=gains.shape)\n\n    # Score: closer to target is better, plus noise\n    scores = -np.abs(gains - target) + noise\n    best_idx = np.argmax(scores)\n    return remaining_subsets[valid_idx[best_idx]]\n\n",
  "multiplicative_weights_element_importance_aug_308": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy set cover selector that uses a multiplicative\u2011weights style rule.\n    This variation uses list comprehensions, median for scaling, clipping of\n    weights, and deterministic tie\u2011breaking noise.\n    \"\"\"\n    rem_set: set[int] = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Count support of each remaining element\n    support: Dict[int, int] = {e: 0 for e in rem_set}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    vals = np.array([c for c in support.values() if c > 0], dtype=float)\n    if vals.size == 0:\n        return None\n\n    # Scale based on median (plus epsilon) and clip weights to [0,1]\n    scale = max(1.0, np.median(vals) + 1e-12)\n    w: Dict[int, float] = {}\n    for e, c in support.items():\n        if c > 0:\n            weight = np.exp(-c / scale)\n            w[e] = np.clip(weight, 0.0, 1.0)\n        else:\n            w[e] = 0.0\n\n    best, best_score = None, -1e18\n    for s in remaining_subsets:\n        sc = sum(w.get(e, 0.0) for e in set(s) if e in rem_set)\n        # deterministic tie\u2011breaking: add tiny noise proportional to element sum\n        tie_noise = 1e-9 * sum(e for e in set(s) if e in rem_set)\n        sc += tie_noise\n        if sc > best_score and sc > 0.0:\n            best_score, best = sc, s\n    return best\n\n",
  "multiplicative_weights_element_importance_aug_309": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy set cover selector that employs a while\u2011loop, mean\u2011based scaling,\n    softmax\u2011style weights, and deterministic noise for tie\u2011breaking.\n    \"\"\"\n    rem_set: set[int] = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem_set}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    vals = np.array([c for c in support.values() if c > 0], dtype=float)\n    if vals.size == 0:\n        return None\n\n    scale = max(1.0, np.mean(vals) + 1e-12)\n    exp_vals = np.exp(-np.array(list(support.values()), dtype=float) / scale)\n    denom = np.sum(exp_vals) + 1e-12  # epsilon to avoid division by zero\n    soft_weights = exp_vals / denom\n    w: Dict[int, float] = {e: np.clip(soft_weights[idx], 0.0, 1.0)\n                           for idx, e in enumerate(support)}\n\n    best, best_score = None, -1e18\n    idx, n = 0, len(remaining_subsets)\n    while idx < n:\n        s = remaining_subsets[idx]\n        sc = sum(w.get(e, 0.0) for e in set(s) if e in rem_set)\n        # deterministic noise: sum of element IDs scaled down\n        noise = 1e-8 * sum(e for e in set(s) if e in rem_set)\n        sc += noise\n        if sc > best_score and sc > 0.0:\n            best_score, best = sc, s\n        idx += 1\n    return best\n\n",
  "multiplicative_weights_element_importance_aug_310": "import numpy as np\nimport random\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy set cover selector that uses a soft\u2011max weight scheme,\n    selects among the top\u20113 candidates, and incorporates\n    deterministic tie\u2011breaking noise.\n    \"\"\"\n    rem_set: set[int] = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem_set}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    vals = np.array([c for c in support.values() if c > 0], dtype=float)\n    if vals.size == 0:\n        return None\n\n    scale = max(1.0, np.mean(vals) + 1e-12)\n    exp_vals = np.exp(-np.array(list(support.values()), dtype=float) / scale)\n    denom = np.sum(exp_vals) + 1e-12\n    soft_weights = exp_vals / denom\n    w: Dict[int, float] = {e: np.clip(soft_weights[idx], 0.0, 1.0)\n                           for idx, e in enumerate(support)}\n\n    scores = []\n    for s in remaining_subsets:\n        sc = sum(w.get(e, 0.0) for e in set(s) if e in rem_set)\n        # add deterministic noise proportional to subset length\n        sc += 1e-9 * len(set(s) & rem_set)\n        scores.append(sc)\n\n    if not scores:\n        return None\n\n    top_k = 3\n    top_indices = np.argsort(scores)[-top_k:]  # indices of top\u2011k scores\n    chosen_idx = random.choice(top_indices)\n    return remaining_subsets[chosen_idx]\n\n",
  "multiplicative_weights_element_importance_aug_311": "import numpy as np\nimport random\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy set cover selector that uses the maximum weighted element\n    in each subset, applies a 50\u202f% threshold filter, and picks a random\n    candidate among the remaining ones.\n    \"\"\"\n    rem_set: set[int] = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    support: Dict[int, int] = {e: 0 for e in rem_set}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    vals = np.array([c for c in support.values() if c > 0], dtype=float)\n    if vals.size == 0:\n        return None\n\n    scale = max(1.0, np.median(vals) + 1e-12)\n    w: Dict[int, float] = {}\n    for e, c in support.items():\n        if c > 0:\n            w[e] = np.clip(np.exp(-c / scale), 0.0, 1.0)\n        else:\n            w[e] = 0.0\n\n    scores = []\n    for s in remaining_subsets:\n        # use maximum weight among the subset's remaining elements\n        max_w = 0.0\n        for e in set(s):\n            if e in rem_set:\n                max_w = max(max_w, w.get(e, 0.0))\n        scores.append(max_w)\n\n    if not scores:\n        return None\n\n    max_score = max(scores)\n    threshold = 0.5 * max_score\n    candidate_indices = [i for i, sc in enumerate(scores) if sc >= threshold and sc > 0.0]\n\n    if not candidate_indices:\n        return None\n\n    chosen_idx = random.choice(candidate_indices)\n    return remaining_subsets[chosen_idx]\n\n",
  "greedy_dual_fitting_proxy_aug_312": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    remaining_set = set(remaining_elements)\n    if not remaining_set:\n        return None\n\n    # Map each element to a unique index\n    elem_to_idx = {e: i for i, e in enumerate(remaining_set)}\n    support_counts = np.zeros(len(remaining_set), dtype=int)\n\n    # Build support counts with a while loop\n    idx = 0\n    while idx < len(remaining_subsets):\n        subset = remaining_subsets[idx]\n        for e in set(subset):\n            if e in elem_to_idx:\n                support_counts[elem_to_idx[e]] += 1\n        idx += 1\n\n    best_subset = None\n    best_score = -np.inf\n    noise_factor = 1e-6  # deterministic noise for tie\u2011breaking\n\n    # Iterate through remaining subsets with a while loop\n    i = 0\n    while i < len(remaining_subsets):\n        subset = remaining_subsets[i]\n        subset_set = set(subset)\n        new_elems = [e for e in subset_set if e in remaining_set]\n        if not new_elems:\n            i += 1\n            continue\n\n        indices = [elem_to_idx[e] for e in new_elems]\n        denom = 1.0 + support_counts[indices].astype(float) + 1e-12\n        weighted = np.sum(1.0 / denom)\n        score = weighted / max(1.0, float(len(subset_set)))\n        score += noise_factor * i  # deterministic tie\u2011breaking\n\n        if score > best_score:\n            best_score, best_subset = score, subset\n        i += 1\n\n    return best_subset\n\n",
  "greedy_dual_fitting_proxy_aug_313": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    remaining_set = set(remaining_elements)\n    if not remaining_set:\n        return None\n\n    elem_list = list(remaining_set)\n    elem_to_idx = {e: i for i, e in enumerate(elem_list)}\n    num_elems = len(elem_list)\n\n    # Build a binary matrix: rows = subsets, columns = elements\n    subs_matrix = np.zeros((len(remaining_subsets), num_elems), dtype=int)\n    for r, subset in enumerate(remaining_subsets):\n        for e in set(subset):\n            if e in elem_to_idx:\n                subs_matrix[r, elem_to_idx[e]] = 1\n\n    # Support counts per element (avoid division by zero)\n    support = np.sum(subs_matrix, axis=0).astype(float) + 1e-12\n\n    # Weighted scores: sum of 1/(1+support) for elements present in each subset\n    inv_weights = 1.0 / (1.0 + support)\n    weighted_scores = np.sum(subs_matrix * inv_weights, axis=1)\n\n    # Normalise by subset size\n    subset_sizes = np.array([len(set(sub)) for sub in remaining_subsets], dtype=float)\n    norm_factors = np.maximum(1.0, subset_sizes)\n    scores = weighted_scores / norm_factors\n\n    # Hyper\u2011parameter tuning: blend with inverse size component\n    alpha = 0.6\n    size_penalty = 1.0 / (subset_sizes + 1e-12)\n    scores = alpha * scores + (1 - alpha) * size_penalty\n\n    # Clip to a safe range\n    scores = np.clip(scores, 0.0, 1.0)\n\n    # Random tie\u2011breaking among the top\u2011k candidates\n    top_k = 3\n    top_indices = np.argsort(-scores)[:top_k]\n    chosen_idx = np.random.choice(top_indices)\n    return remaining_subsets[chosen_idx]\n\n",
  "random_projection_signature_diversity_aug_314": "import numpy as np\nfrom typing import List, Optional\n\ndef _sig(vec: np.ndarray) -> np.ndarray:\n    \"\"\"Return binary signature of a vector.\"\"\"\n    return (vec > 0).astype(np.int8)\n\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Mapping from element to index in the reduced universe\n    rem_list = list(rem_set)\n    idx_map = {e: i for i, e in enumerate(rem_list)}\n    d = len(rem_list)\n    k = min(24, d)\n\n    rng = np.random.RandomState(11)\n    P = rng.randn(k, d)  # random projection matrix\n\n    # Signatures of the already selected subsets\n    if selected_subsets:\n        sel_sigs = []\n        for s in selected_subsets:\n            vec = np.zeros(d, dtype=float)\n            for e in s:\n                if e in idx_map:\n                    vec[idx_map[e]] = 1.0\n            sel_sigs.append(_sig(P @ vec))\n        sel_sigs = np.stack(sel_sigs)  # shape (num_selected, k)\n    else:\n        sel_sigs = np.empty((0, k), dtype=np.int8)\n\n    best = None\n    best_score = -np.inf\n    # small deterministic noise for tie\u2011breaking\n    noise = rng.uniform(-1e-6, 1e-6, size=len(remaining_subsets))\n\n    for i, s in enumerate(remaining_subsets):\n        ss = set(s)\n        gain = len(rem_set & ss)\n        if gain <= 0:\n            continue\n\n        vec = np.zeros(d, dtype=float)\n        for e in ss:\n            if e in idx_map:\n                vec[idx_map[e]] = 1.0\n        sig = _sig(P @ vec)\n\n        if sel_sigs.size == 0:\n            dist = 1.0\n        else:\n            dists = np.mean(sig != sel_sigs, axis=1)\n            dist = np.clip((np.sum(dists) + 1e-12) / (len(dists) + 1e-12), 0, 1)\n\n        # scoring: favour large gain, high diversity, small cost\n        score = gain * (1.0 + 0.6 * dist) - 0.03 * len(ss) + noise[i]\n        if score > best_score:\n            best_score, best = score, s\n    return best\n\n",
  "random_projection_signature_diversity_aug_315": "import numpy as np\nfrom typing import List, Optional\n\ndef _sig(vec: np.ndarray) -> np.ndarray:\n    return (vec > 0).astype(np.int8)\n\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    rem_list = list(rem_set)\n    idx_map = {e: i for i, e in enumerate(rem_list)}\n    d = len(rem_list)\n    k = min(24, d)\n\n    rng = np.random.RandomState(42)\n    P = rng.randn(k, d)\n\n    # Build signatures for selected subsets with a list comprehension\n    sel_sigs = np.array([_sig(P @ np.array([1.0 if e in s else 0.0 for e in rem_list]))\n                         for s in selected_subsets])\n\n    scores = []\n    candidates = []\n\n    i = 0\n    while i < len(remaining_subsets):\n        s = remaining_subsets[i]\n        ss = set(s)\n        gain = len(rem_set & ss)\n        if gain <= 0:\n            i += 1\n            continue\n\n        vec = np.array([1.0 if e in ss else 0.0 for e in rem_list])\n        sig = _sig(P @ vec)\n\n        if sel_sigs.size == 0:\n            dist = 1.0\n        else:\n            dists = np.median(sig != sel_sigs, axis=1)\n            dist = np.clip((np.sum(dists) + 1e-12) / (len(dists) + 1e-12), 0, 1)\n\n        # deterministic noise for tie\u2011breaking\n        noise = rng.uniform(-1e-6, 1e-6)\n        score = gain * (1.0 + 0.4 * dist) - 0.025 * len(ss) + noise\n        scores.append(score)\n        candidates.append(s)\n        i += 1\n\n    if not scores:\n        return None\n\n    # Softmax weighting and random selection\n    exp_scores = np.exp(scores - np.max(scores)) + 1e-12\n    probs = exp_scores / (np.sum(exp_scores) + 1e-12)\n    probs = np.clip(probs, 0, 1)\n    chosen_index = rng.choice(len(candidates), p=probs)\n    return candidates[chosen_index]\n\n",
  "kcenter_on_elements_then_pick_aug_316": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorized support calculation with median pivot and deterministic scoring.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Build a binary matrix (subsets \u00d7 elements)\n    elem_list = sorted(rem_set)\n    elem_index = {e: i for i, e in enumerate(elem_list)}\n    mat = np.zeros((len(remaining_subsets), len(elem_list)), dtype=int)\n    for i, s in enumerate(remaining_subsets):\n        for e in set(s):\n            if e in elem_index:\n                mat[i, elem_index[e]] = 1\n\n    support_counts = mat.sum(axis=0)\n    if support_counts.size == 0:\n        return None\n\n    # Median support pivot\n    median_idx = np.argsort(support_counts)[len(support_counts) // 2]\n    pivot = elem_list[median_idx]\n\n    # Greedy score: gain / size (with epsilon)\n    best_subset, best_score = None, -1e18\n    for s in remaining_subsets:\n        if pivot not in s:\n            continue\n        ss_set = set(s)\n        gain = len(rem_set & ss_set)\n        if gain <= 0:\n            continue\n        score = gain / max(1.0 + 1e-12, float(len(ss_set)))\n        if score > best_score:\n            best_score, best_subset = score, s\n\n    if best_subset is not None:\n        return best_subset\n\n    # Fallback: maximum gain\n    best_subset, best_gain = None, 0\n    for s in remaining_subsets:\n        g = len(rem_set & set(s))\n        if g > best_gain:\n            best_gain, best_subset = g, s\n    return best_subset if best_gain > 0 else None\n\n",
  "kcenter_on_elements_then_pick_aug_317": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Deterministic noise in tie\u2011breaking and random top\u2011k fallback.\"\"\"\n    rem_elems = set(remaining_elements)\n    if not rem_elems:\n        return None\n\n    # Support counts using a dictionary\n    support = {e: 0 for e in rem_elems}\n    for subset in remaining_subsets:\n        for e in set(subset):\n            if e in support:\n                support[e] += 1\n\n    # Pivot: median support element\n    sorted_sup = sorted(support.items(), key=lambda x: x[1])\n    pivot = sorted_sup[len(sorted_sup) // 2][0]\n\n    # Scoring with weighted gain and deterministic noise\n    best_subset, best_score = None, -np.inf\n    for subset in remaining_subsets:\n        if pivot not in subset:\n            continue\n        ss = set(subset)\n        g = len(rem_elems & ss)\n        if g == 0:\n            continue\n        score = 0.7 * g / max(1.0 + 1e-12, float(len(subset)))\n        noise = 1e-6 * (hash(tuple(subset)) & 0xFFFFFFFF)\n        if score + noise > best_score:\n            best_score = score + noise\n            best_subset = subset\n\n    if best_subset is not None:\n        return best_subset\n\n    # Fallback: random choice among the top\u2011k highest\u2011gain subsets\n    gains = [(len(rem_elems & set(s)), s) for s in remaining_subsets]\n    gains.sort(key=lambda x: x[0], reverse=True)\n    top_k = min(5, len(gains))\n    candidates = [s for g, s in gains[:top_k] if g > 0]\n    if not candidates:\n        return None\n    return np.random.choice(candidates)\n\n",
  "kcenter_on_elements_then_pick_aug_318": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Approximate support with random sampling and softmax scoring.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Randomly sample a subset of remaining sets to estimate support\n    sample_size = max(1, len(remaining_subsets) // 10)\n    sampled = np.random.choice(remaining_subsets, size=sample_size, replace=False)\n    support = {e: 0 for e in rem}\n    for s in sampled:\n        for e in set(s):\n            if e in support:\n                support[e] += 1\n\n    # Pivot: element with median estimated support\n    counts = np.array(list(support.values()))\n    median_val = np.median(counts)\n    pivot_candidates = [e for e, c in support.items() if c == median_val]\n    pivot = np.random.choice(pivot_candidates)\n\n    # Softmax scoring over all remaining subsets\n    scores = []\n    for s in remaining_subsets:\n        if pivot not in s:\n            scores.append(-np.inf)\n            continue\n        ss = set(s)\n        g = len(rem & ss)\n        if g == 0:\n            scores.append(-np.inf)\n            continue\n        denom = max(1.0 + 1e-12, float(len(ss)))\n        scores.append(g / denom)\n    scores = np.array(scores)\n\n    exp_scores = np.exp(scores - np.max(scores))\n    probs = exp_scores / np.clip(np.sum(exp_scores), 1e-12, None)\n    chosen_idx = np.random.choice(len(remaining_subsets), p=probs)\n    return remaining_subsets[chosen_idx]\n\n",
  "kcenter_on_elements_then_pick_aug_319": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"List\u2011comprehension heavy version with np.clip for safety.\"\"\"\n    remaining = set(remaining_elements)\n    if not remaining:\n        return None\n\n    # Support counts via comprehension\n    support = {e: sum(e in set(s) for s in remaining_subsets) for e in remaining}\n    if not support:\n        return None\n\n    # Pivot: median support element\n    pivot = sorted(support.keys(), key=lambda e: support[e])[len(support) // 2]\n\n    # Greedy score: gain / size\n    best_subset, best_score = None, -np.inf\n    for s in remaining_subsets:\n        if pivot not in s:\n            continue\n        g = len(remaining & set(s))\n        if g == 0:\n            continue\n        score = g / np.clip(len(s), 1e-12, None)\n        if score > best_score:\n            best_score, best_subset = score, s\n\n    if best_subset is not None:\n        return best_subset\n\n    # Fallback: choose one of the subsets with maximal gain\n    gains = [(len(remaining & set(s)), s) for s in remaining_subsets]\n    max_gain = max(gains, key=lambda x: x[0])[0]\n    candidates = [s for g, s in gains if g == max_gain and g > 0]\n    return candidates[0] if candidates else None\n\n",
  "min_remaining_support_ones_after_pick_aug_320": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selection that prefers sets reducing the number of uncovered elements.\n    Uses list\u2011comprehensions, vectorised operations, deterministic noise and\n    a soft\u2011max style scoring.\n    \"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    # Support count for each remaining element\n    support0: Dict[int, int] = {e: sum(e in s for s in remaining_subsets) for e in rem0}\n\n    sets = [set(s) for s in remaining_subsets]\n    best, best_score = None, -np.inf\n\n    for s, ss in zip(remaining_subsets, sets):\n        gain = len(rem0 & ss)\n        if gain <= 0:\n            continue\n\n        rem1 = rem0 - ss\n        ones = sum(1 for e in rem1 if support0.get(e, 0) == 1)\n\n        # Soft\u2011max style score with small deterministic noise\n        denom = 1.0 + 1.2 * ones + 0.02 * len(ss) + 1e-12\n        score = gain / denom\n        score += np.random.uniform(-1e-6, 1e-6)          # tie\u2011breaking noise\n        score = np.clip(score, -1e6, 1e6)                # safety bound\n\n        if score > best_score:\n            best_score, best = score, s\n\n    return best\n\n",
  "min_remaining_support_ones_after_pick_aug_321": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selection implemented with a while loop.\n    Uses np.clip for score bounding and an inverted conditional style.\n    \"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    support0: Dict[int, int] = {e: 0 for e in rem0}\n    for s in remaining_subsets:\n        for e in s:\n            if e in support0:\n                support0[e] = support0.get(e, 0) + 1\n\n    sets = [set(s) for s in remaining_subsets]\n    idx = 0\n    best, best_score = None, -1e12\n\n    while idx < len(remaining_subsets):\n        s, ss = remaining_subsets[idx], sets[idx]\n        gain = len(rem0 & ss)\n        if gain > 0:\n            rem1 = rem0 - ss\n            ones = sum(1 for e in rem1 if support0.get(e, 0) == 1)\n\n            denom = 1.0 + 1.05 * ones + 0.015 * len(ss) + 1e-12\n            score = gain / denom\n            score = np.clip(score, -1e5, 1e5)            # bound the score\n\n            if score > best_score:\n                best_score, best = score, s\n        idx += 1\n\n    return best\n\n",
  "min_remaining_support_ones_after_pick_aug_322": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selection that evaluates all candidate sets, keeps the top\u2011k\n    candidates and picks one uniformly at random.\n    \"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    support0: Dict[int, int] = {e: sum(e in s for s in remaining_subsets) for e in rem0}\n    sets = [set(s) for s in remaining_subsets]\n\n    candidates, scores = [], []\n\n    for s, ss in zip(remaining_subsets, sets):\n        gain = len(rem0 & ss)\n        if gain <= 0:\n            continue\n        rem1 = rem0 - ss\n        ones = sum(1 for e in rem1 if support0.get(e, 0) == 1)\n\n        denom = 1.0 + 1.1 * ones + 0.01 * len(ss) + 1e-12\n        score = gain / denom\n        candidates.append(s)\n        scores.append(score)\n\n    if not candidates:\n        return None\n\n    scores = np.array(scores, dtype=np.float64)\n    top_k = min(3, len(scores))\n    top_indices = np.argsort(-scores)[:top_k]\n    top_candidates = [candidates[i] for i in top_indices]\n\n    return np.random.choice(top_candidates)\n\n",
  "min_remaining_support_ones_after_pick_aug_323": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selection using mathematical synonyms, inverted logic,\n    and np.max for auxiliary aggregation.\n    \"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    support0: Dict[int, int] = {e: 0 for e in rem0}\n    for s in remaining_subsets:\n        for e in s:\n            if e in support0:\n                support0[e] = support0.get(e, 0) + 1\n\n    sets = [set(s) for s in remaining_subsets]\n    best, best_score = None, -1e18\n\n    for s, ss in zip(remaining_subsets, sets):\n        gain = len(rem0 & ss)\n        if gain <= 0:\n            continue\n\n        rem1 = rem0 - ss\n        ones = sum(1 for e in rem1 if support0.get(e, 0) == 1)\n\n        # Auxiliary aggregation with np.max (unused in score but demonstrates the strategy)\n        max_support = np.max(list(support0.values())) if support0 else 0.0\n\n        denom = 1.0 + 1.05 * ones + 0.02 * len(ss) + 1e-12\n        score = gain / denom\n        score = np.clip(score, -1e6, 1e6)  # safety bound\n\n        if score > best_score:\n            best_score, best = score, s\n\n    return best\n\n",
  "maximize_coverage_of_support_one_aug_324": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selection that maximizes the number of uncovered elements\n    with support==1 covered by the chosen set.\n    Uses list comprehensions, deterministic noise for tie\u2011breaking\n    and a small epsilon to avoid division by zero in ratio calculations.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Build support counts with numpy arrays\n    elem_list = list(rem_set)\n    idx_map = {e: i for i, e in enumerate(elem_list)}\n    support_counts = np.zeros(len(elem_list), dtype=int)\n\n    for s in remaining_subsets:\n        for e in s:\n            if e in idx_map:\n                support_counts[idx_map[e]] += 1\n\n    must_indices = np.where(support_counts == 1)[0]\n    must_set = {elem_list[i] for i in must_indices}\n\n    best_subset = None\n    best_score = None\n\n    for s in remaining_subsets:\n        new = rem_set.intersection(s)\n        if not new:\n            continue\n\n        mc = len(must_set.intersection(s))\n        # ratio of must elements to subset size, with epsilon\n        ratio = mc / (len(s) + 1e-12)\n        ratio = np.clip(ratio, 0, 1)\n\n        # deterministic noise based on subset contents\n        noise = (hash(tuple(sorted(s))) & 0xffff) * 1e-6\n\n        # composite score: must + weighted new + size penalty\n        score = (mc + noise, len(new), -len(s))\n        if best_score is None or score > best_score:\n            best_score = score\n            best_subset = s\n\n    return best_subset\n\n",
  "maximize_coverage_of_support_one_aug_325": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selection using while loops, inverted condition logic,\n    weighted scoring, and top\u2011k random choice.\n    \"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support = {e: 0 for e in rem}\n    idx = 0\n    n = len(remaining_subsets)\n    while idx < n:\n        s = remaining_subsets[idx]\n        for e in s:\n            if e in support:\n                support[e] += 1\n        idx += 1\n\n    must = {e for e, c in support.items() if c == 1}\n\n    scores = []\n    subsets = []\n    for s in remaining_subsets:\n        new = rem.intersection(s)\n        if not new:\n            continue\n\n        mc = len(must.intersection(s))\n        ratio = mc / (len(s) + 1e-12)\n        ratio = np.clip(ratio, 0, 1)\n\n        # Weighted score with different hyper\u2011parameters\n        score = mc + 0.5 * len(new) - 0.3 * len(s) + ratio\n        scores.append(score)\n        subsets.append(s)\n\n    if not subsets:\n        return None\n\n    top_k = min(5, len(subsets))\n    top_indices = np.argpartition(scores, -top_k)[-top_k:]\n    top_scores = [scores[i] for i in top_indices]\n\n    # deterministic tie\u2011breaking: choose the first maximum among top\u2011k\n    max_idx = top_indices[np.argmax(top_scores)]\n    return subsets[max_idx]\n\n",
  "maximize_coverage_of_support_one_aug_326": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Soft\u2011min selection: compute scores, transform to probabilities with\n    soft\u2011min, and randomly sample a subset. Uses epsilon in denominator\n    and np.clip to keep probabilities valid.\n    \"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Support counts via numpy\n    elem_list = list(rem)\n    idx_map = {e: i for i, e in enumerate(elem_list)}\n    support = np.zeros(len(elem_list), dtype=int)\n\n    for s in remaining_subsets:\n        for e in s:\n            if e in idx_map:\n                support[idx_map[e]] += 1\n\n    must = {elem_list[i] for i, c in enumerate(support) if c == 1}\n\n    scores = []\n    for s in remaining_subsets:\n        new = rem.intersection(s)\n        if not new:\n            scores.append(-np.inf)\n            continue\n        mc = len(must.intersection(s))\n        score = mc + len(new) - len(s)\n        scores.append(score)\n\n    scores = np.array(scores, dtype=float)\n\n    # Convert to soft\u2011min probabilities\n    max_score = np.max(scores)\n    exp_vals = np.exp(-(scores - max_score))\n    denom = np.sum(exp_vals) + 1e-12\n    probs = exp_vals / denom\n    probs = np.clip(probs, 0, 1)\n\n    # Sample according to probabilities\n    choice = np.random.choice(len(remaining_subsets), p=probs)\n    return remaining_subsets[choice]\n\n",
  "maximize_coverage_of_support_one_aug_327": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Uses a proxy must set (support <= 2), median support as aggregation,\n    deterministic noise for tie\u2011breaking, and np.clip for safety.\n    \"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in s:\n            if e in support:\n                support[e] += 1\n\n    # Proxy must: elements with support <= 2\n    must = {e for e, c in support.items() if c <= 2}\n\n    best_subset = None\n    best_key = None\n\n    for s in remaining_subsets:\n        new = rem.intersection(s)\n        if not new:\n            continue\n\n        mc = len(must.intersection(s))\n        sup_counts = [support.get(e, 0) for e in new]\n        median_sup = np.median(sup_counts) if sup_counts else 0\n\n        # Composite score: must + new size - median support\n        score = mc - median_sup + len(new)\n\n        # Deterministic noise for tie\u2011breaking\n        noise = (hash(tuple(sorted(s))) % 1000) * 1e-5\n        key = score + noise\n\n        if best_key is None or key > best_key:\n            best_key = key\n            best_subset = s\n\n    return best_subset\n\n",
  "entropy_of_new_supports_aug_328": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef _entropy_from_counts(x: np.ndarray) -> float:\n    \"\"\"Compute entropy of support counts with numerical safety.\"\"\"\n    s = float(x.sum()) + 1e-12          # epsilon to avoid division by zero\n    if s <= 0:\n        return 0.0\n    p = x / s\n    p = np.clip(p, 1e-12, None)         # clip to avoid log(0)\n    return float(-(p * np.log(p)).sum())\n\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Count how many remaining subsets contain each element\n    support: Dict[int, int] = {}\n    for sub in remaining_subsets:\n        for el in set(sub):\n            if el in rem:\n                support[el] = support.get(el, 0) + 1\n\n    best = None\n    best_score = -1e18\n    idx = 0\n    while idx < len(remaining_subsets):\n        sub = remaining_subsets[idx]\n        new_el = [el for el in set(sub) if el in rem]\n        if new_el:\n            gain = len(new_el)\n            vals = np.array([support.get(el, 0) for el in new_el], dtype=float)\n            ent = _entropy_from_counts(vals)\n            # deterministic tie\u2011breaker based on element IDs\n            noise = 1e-6 * sum(new_el)\n            score = gain + 0.35 * ent - 0.02 * len(sub) + noise\n            if score > best_score:\n                best_score = score\n                best = sub\n        idx += 1\n    return best\n\n",
  "entropy_of_new_supports_aug_329": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef _entropy_from_counts(x: np.ndarray) -> float:\n    \"\"\"Entropy with a tiny epsilon to avoid division by zero.\"\"\"\n    s = float(x.sum()) + 1e-12\n    if s <= 0:\n        return 0.0\n    p = x / s\n    p = np.clip(p, 1e-12, None)\n    return float(-(p * np.log(p)).sum())\n\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_elems = set(remaining_elements)\n    if not rem_elems:\n        return None\n\n    support: Dict[int, int] = {}\n    for sub in remaining_subsets:\n        for el in set(sub):\n            if el in rem_elems:\n                support[el] = support.get(el, 0) + 1\n\n    best = None\n    best_score = -1e18\n    for sub in remaining_subsets:\n        new_el = [el for el in set(sub) if el in rem_elems]\n        if not new_el:\n            continue\n        gain = len(new_el)\n        vals = np.array([support.get(el, 0) for el in new_el], dtype=float)\n        ent = _entropy_from_counts(vals)\n        avg_sup = np.mean(vals)                    # new aggregation\n        score = 0.6 * gain + 0.4 * ent - 0.015 * len(sub) + 0.01 * avg_sup\n        if score > best_score:\n            best_score = score\n            best = sub\n    return best\n\n",
  "entropy_of_new_supports_aug_330": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef _entropy_from_counts(x: np.ndarray) -> float:\n    \"\"\"Entropy calculation with clipping for numerical stability.\"\"\"\n    s = float(x.sum()) + 1e-12\n    if s <= 0:\n        return 0.0\n    p = x / s\n    p = np.clip(p, 1e-12, None)\n    return float(-(p * np.log(p)).sum())\n\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    support: Dict[int, int] = {}\n    for sub in remaining_subsets:\n        for el in set(sub):\n            if el in rem_set:\n                support[el] = support.get(el, 0) + 1\n\n    scores = []\n    for sub in remaining_subsets:\n        new_el = [el for el in set(sub) if el in rem_set]\n        if not new_el:\n            scores.append(-np.inf)\n            continue\n        gain = len(new_el)\n        vals = np.array([support.get(el, 0) for el in new_el], dtype=float)\n        ent = _entropy_from_counts(vals)\n        med_sup = np.median(vals)                 # median support penalty\n        score = gain + 0.35 * ent - 0.02 * len(sub) - 0.01 * med_sup\n        scores.append(score)\n\n    scores_np = np.array(scores)\n    # Keep only subsets that actually cover new elements\n    valid = [(score, sub) for score, sub in zip(scores_np, remaining_subsets)\n             if not np.isneginf(score)]\n    if not valid:\n        return None\n\n    valid_scores = np.array([v[0] for v in valid])\n    top_k = min(3, len(valid_scores))\n    top_indices = np.argpartition(-valid_scores, top_k-1)[:top_k]\n    rng = np.random.default_rng(42)          # deterministic RNG\n    chosen = valid[rng.choice(top_indices)]\n    return chosen[1]\n\n",
  "entropy_of_new_supports_aug_331": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef _entropy_from_counts(x: np.ndarray) -> float:\n    \"\"\"Standard entropy with epsilon safeguards.\"\"\"\n    s = float(x.sum()) + 1e-12\n    if s <= 0:\n        return 0.0\n    p = x / s\n    p = np.clip(p, 1e-12, None)\n    return float(-(p * np.log(p)).sum())\n\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    support: Dict[int, int] = {}\n    for sub in remaining_subsets:\n        for el in set(sub):\n            if el in rem:\n                support[el] = support.get(el, 0) + 1\n\n    scores = []\n    for sub in remaining_subsets:\n        new_el = [el for el in set(sub) if el in rem]\n        if not new_el:\n            scores.append(-np.inf)\n            continue\n        gain = len(new_el)\n        vals = np.array([support.get(el, 0) for el in new_el], dtype=float)\n        # proxy entropy: sum of log\u20111\u2011plus of support counts\n        ent = np.sum(np.log1p(vals + 1e-12))\n        score = 0.5 * gain + 0.5 * ent - 0.01 * len(sub)\n        scores.append(score)\n\n    scores_np = np.array(scores)\n    valid = [(score, sub) for score, sub in zip(scores_np, remaining_subsets)\n             if not np.isneginf(score)]\n    if not valid:\n        return None\n\n    valid_scores = np.array([v[0] for v in valid])\n    top_k = min(5, len(valid_scores))\n    top_indices = np.argpartition(-valid_scores, top_k-1)[:top_k]\n    rng = np.random.default_rng(123)          # deterministic RNG\n    chosen = valid[rng.choice(top_indices)]\n    return chosen[1]\n\n",
  "maximize_jaccard_to_remaining_elements_aug_332": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Greedy selection using vectorised Jaccard similarity with clipped scores.\"\"\"\n    rem_set: Set[int] = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    best_subset: Optional[List[int]] = None\n    best_score: float = -1.0\n    eps: float = 1e-12\n\n    # Iterate using a while loop for structural change\n    idx = 0\n    while idx < len(remaining_subsets):\n        cur = remaining_subsets[idx]\n        cur_set = set(cur)\n        inter_size = len(cur_set & rem_set)\n        if inter_size == 0:\n            idx += 1\n            continue\n        union_size = len(cur_set | rem_set)\n        # Clip score to avoid division by zero and keep within [0,1]\n        score = inter_size / (union_size + eps)\n        score = np.clip(score, 0.0, 1.0)\n        if score > best_score:\n            best_score, best_subset = score, cur\n        idx += 1\n\n    return best_subset\n\n",
  "maximize_jaccard_to_remaining_elements_aug_333": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Select subset by weighted similarity and random tie\u2011breaking among top\u2011k.\"\"\"\n    rem_set: Set[int] = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    eps: float = 1e-12\n    top_k: int = 5  # hyper\u2011parameter tuning\n\n    scores = []\n    for s in remaining_subsets:\n        s_set = set(s)\n        inter = len(s_set & rem_set)\n        if inter == 0:\n            scores.append(-np.inf)\n            continue\n        # Modified weighting: 0.7 * inter + 0.3 * (len(rem_set) - inter)\n        weighted = 0.7 * inter + 0.3 * (len(rem_set) - inter)\n        union = len(s_set | rem_set)\n        score = weighted / (union + eps)\n        # Add deterministic small noise for tie\u2011breaking\n        noise = np.random.uniform(-1e-6, 1e-6)\n        scores.append(score + noise)\n\n    scores = np.array(scores)\n    # Keep only non\u2011negative scores\n    valid = scores > -np.inf\n    if not np.any(valid):\n        return None\n\n    # Pick top\u2011k indices\n    top_indices = np.argpartition(-scores, top_k - 1)[:top_k]\n    # Randomly choose among the top\u2011k\n    chosen_idx = np.random.choice(top_indices)\n    return remaining_subsets[chosen_idx]\n\n",
  "maximize_jaccard_to_remaining_elements_aug_334": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Use a soft\u2011min approach to pick the subset with the lowest exponential penalty.\"\"\"\n    rem_set: Set[int] = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    eps: float = 1e-12\n    scores = []\n    for s in remaining_subsets:\n        s_set = set(s)\n        inter = len(s_set & rem_set)\n        if inter == 0:\n            scores.append(np.inf)\n            continue\n        union = len(s_set | rem_set)\n        jaccard = inter / (union + eps)\n        # Soft\u2011min weight: exp(-jaccard)\n        weight = np.exp(-jaccard)\n        scores.append(weight)\n\n    scores = np.array(scores)\n    if np.all(np.isinf(scores)):\n        return None\n\n    # Choose subset with minimal soft\u2011min weight (i.e., highest similarity)\n    chosen_idx = np.argmin(scores)\n    return remaining_subsets[chosen_idx]\n\n",
  "maximize_jaccard_to_remaining_elements_aug_335": "import numpy as np\nfrom typing import List, Optional, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Approximate similarity via mean intersection ratio and deterministic noise.\"\"\"\n    rem_set: Set[int] = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    eps: float = 1e-12\n    scores = []\n    for s in remaining_subsets:\n        s_set = set(s)\n        inter = len(s_set & rem_set)\n        if inter == 0:\n            scores.append(-np.inf)\n            continue\n        # Proxy: intersection size divided by mean remaining elements\n        proxy = inter / (len(rem_set) + eps)\n        # Deterministic noise from hash of the subset\n        noise = (hash(tuple(sorted(s))) % 1000) / 1e6\n        scores.append(proxy + noise)\n\n    scores = np.array(scores)\n    valid = scores > -np.inf\n    if not np.any(valid):\n        return None\n\n    # Clip scores to [0,1] before picking maximum\n    clipped = np.clip(scores, 0.0, 1.0)\n    chosen_idx = np.argmax(clipped)\n    return remaining_subsets[chosen_idx]\n\n",
  "maximize_f1_to_remaining_aug_336": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selection using a weighted F1 score and a tiny deterministic noise term\n    to break ties.  All denominators are clipped to avoid division by zero.\n    \"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    best_subset = None\n    best_score = -np.inf\n    rng = np.random.default_rng(seed=42)\n\n    for s in remaining_subsets:\n        ss = set(s)\n        tp = len(ss & rem)\n        if tp <= 0:\n            continue\n\n        prec = tp / np.clip(len(ss), 1.0, None)\n        rec  = tp / np.clip(len(rem), 1.0, None)\n        f1   = (2.0 * prec * rec) / np.clip(prec + rec, 1e-12, None)\n\n        # Slightly different weighting and deterministic noise for tie\u2011breaking\n        score = f1 + 0.05 * tp + rng.uniform(0, 1e-6)\n\n        if score > best_score:\n            best_score, best_subset = score, s\n\n    return best_subset\n\n",
  "maximize_f1_to_remaining_aug_337": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Candidate subsets are pre\u2011filtered, then a top\u2011k soft choice is made.\n    Uses a different weighting for the F1 score and a higher noise magnitude.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Filter out subsets that do not intersect the remaining elements\n    candidates = [s for s in remaining_subsets if len(set(s) & rem_set) > 0]\n    if not candidates:\n        return None\n\n    scores = []\n    rng = np.random.default_rng(seed=24)\n\n    for s in candidates:\n        ss = set(s)\n        tp = len(ss & rem_set)\n        prec = tp / np.clip(len(ss), 1.0, None)\n        rec  = tp / np.clip(len(rem_set), 1.0, None)\n        f1   = (2.0 * prec * rec) / np.clip(prec + rec, 1e-12, None)\n\n        # Different coefficient for tp and larger noise\n        scores.append(f1 + 0.10 * tp + rng.uniform(0, 1e-6))\n\n    scores = np.array(scores)\n    top_k = min(5, len(scores))\n    top_indices = np.argpartition(-scores, top_k - 1)[:top_k]\n    chosen_idx = rng.choice(top_indices)\n    return candidates[chosen_idx]\n\n",
  "maximize_f1_to_remaining_aug_338": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Iteration is performed with a while loop.  The scoring function uses a\n    slightly different weight on tp and adds a small random perturbation.\n    \"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    best_subset = None\n    best_score  = -np.inf\n    rng = np.random.default_rng(seed=7)\n    idx = 0\n\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        ss = set(s)\n        tp = len(ss & rem)\n        if tp > 0:\n            prec = tp / np.clip(len(ss), 1.0, None)\n            rec  = tp / np.clip(len(rem), 1.0, None)\n            f1   = (2.0 * prec * rec) / np.clip(prec + rec, 1e-12, None)\n            score = f1 + 0.03 * tp + rng.uniform(0, 1e-6)\n            if score > best_score:\n                best_score, best_subset = score, s\n        idx += 1\n\n    return best_subset\n\n",
  "maximize_f1_to_remaining_aug_339": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Vectorised implementation that evaluates all subsets at once.\n    Uses np.max to pick the best score and masks non\u2011viable subsets with -inf.\n    \"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    rem_arr = np.array(list(rem))\n    subset_arrs = [np.array(s) for s in remaining_subsets]\n\n    # Count intersections for every subset\n    tp_counts = np.array([len(np.intersect1d(sa, rem_arr, assume_unique=True))\n                          for sa in subset_arrs])\n\n    mask = tp_counts > 0\n    if not np.any(mask):\n        return None\n\n    ss_lengths = np.array([len(sa) for sa in subset_arrs])\n    prec = tp_counts / np.clip(ss_lengths, 1.0, None)\n    rec  = tp_counts / np.clip(len(rem), 1.0, None)\n    f1   = (2.0 * prec * rec) / np.clip(prec + rec, 1e-12, None)\n\n    rng = np.random.default_rng(seed=13)\n    scores = np.where(\n        mask,\n        f1 + 0.04 * tp_counts + rng.uniform(0, 1e-6, size=tp_counts.shape),\n        -np.inf\n    )\n\n    best_idx = np.argmax(scores)\n    return remaining_subsets[best_idx]\n\n",
  "gain_times_precision_aug_340": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    uncovered = set(remaining_elements)\n\n    # Compute a score for every candidate subset\n    scores = []\n    for cand in remaining_subsets:\n        cand_set = set(cand)\n        gain = len(cand_set & uncovered)\n        if gain <= 0:\n            scores.append(-np.inf)\n            continue\n\n        # Precision with a small epsilon to avoid division by zero\n        precision = gain / (max(1.0, float(len(cand_set))) + 1e-12)\n        precision = np.clip(precision, 0.0, 1.0)\n\n        # Small deterministic noise to break ties\n        noise = 1e-6 * sum(cand)\n\n        # Final score: product of gain and precision plus noise\n        scores.append(gain * precision + noise)\n\n    scores = np.array(scores)\n    if np.all(scores == -np.inf):\n        return None\n\n    idx = np.argmax(scores)\n    return remaining_subsets[idx]\n\n",
  "gain_times_precision_aug_341": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    uncovered = set(remaining_elements)\n\n    best_idx = None\n    best_score = -np.inf\n    i = 0\n    while i < len(remaining_subsets):\n        cand = remaining_subsets[i]\n        cand_set = set(cand)\n        gain = len(cand_set & uncovered)\n        if gain > 0:\n            precision = gain / (max(1.0, float(len(cand_set))) + 1e-12)\n            precision = np.clip(precision, 0.0, 1.0)\n\n            # Weighted score: higher weight on gain\n            score = 0.6 * gain + 0.4 * precision\n            if score > best_score:\n                best_score = score\n                best_idx = i\n        i += 1\n\n    if best_idx is None:\n        return None\n\n    # Compute all scores again to perform random tie\u2011breaking among the top\u20113\n    all_scores = np.array([\n        0.6 * len(set(c) & uncovered) + 0.4 *\n        (len(set(c) & uncovered) / (max(1.0, float(len(c))) + 1e-12))\n        for c in remaining_subsets\n    ])\n    top_k = min(3, len(all_scores))\n    top_indices = np.argpartition(-all_scores, top_k - 1)[:top_k]\n    chosen_idx = np.random.choice(top_indices)\n\n    return remaining_subsets[chosen_idx]\n\n",
  "gain_times_precision_aug_342": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    uncovered = set(remaining_elements)\n\n    scores = []\n    for cand in remaining_subsets:\n        cand_set = set(cand)\n        gain = len(cand_set & uncovered)\n        if gain <= 0:\n            scores.append(-np.inf)\n            continue\n\n        precision = gain / (max(1.0, float(len(cand_set))) + 1e-12)\n        precision = np.clip(precision, 0.0, 1.0)\n\n        # Use the maximum of gain and precision as the core score\n        score = np.max([gain, precision]) + 1e-6 * sum(cand)\n        scores.append(score)\n\n    scores = np.array(scores)\n    if np.all(scores == -np.inf):\n        return None\n\n    # Randomly pick among the two best candidates\n    top_k = min(2, len(scores))\n    top_indices = np.argpartition(-scores, top_k - 1)[:top_k]\n    chosen_idx = np.random.choice(top_indices)\n\n    return remaining_subsets[chosen_idx]\n\n",
  "gain_times_precision_aug_343": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    uncovered = set(remaining_elements)\n\n    scores = []\n    for cand in remaining_subsets:\n        cand_set = set(cand)\n\n        # Proxy for intersection: use the smaller cardinality\n        proxy_gain = min(len(cand_set), len(uncovered))\n        if proxy_gain <= 0:\n            scores.append(np.inf)\n            continue\n\n        precision = proxy_gain / (max(1.0, float(len(cand_set))) + 1e-12)\n        precision = np.clip(precision, 0.0, 1.0)\n\n        # Soft\u2011min style score: lower is better\n        score = np.exp(-proxy_gain) + np.exp(-precision) + 1e-12\n        scores.append(score)\n\n    scores = np.array(scores)\n    if np.all(np.isinf(scores)):\n        return None\n\n    min_score = np.min(scores)\n    # Randomly choose among candidates that are within a tiny margin of the best\n    candidates = np.where(scores <= min_score + 1e-8)[0]\n    chosen_idx = np.random.choice(candidates)\n\n    return remaining_subsets[chosen_idx]\n\n",
  "roulette_on_gain_and_precision_aug_344": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Vectorised implementation that uses a deterministic noise term for tie\u2011breaking.\n    \"\"\"\n    rem_set = set(remaining_elements)\n\n    # Compute gains and subset sizes in one pass\n    gains = np.array([len(set(s).intersection(rem_set)) for s in remaining_subsets],\n                     dtype=float)\n    non_zero = gains > 0\n    if not np.any(non_zero):\n        return None\n\n    sizes = np.array([len(s) for s in remaining_subsets], dtype=float)\n    precision = gains / np.maximum(1e-12, sizes)          # epsilon to avoid div\u2011by\u2011zero\n    scores = np.square(gains * precision)\n\n    # Small deterministic noise to break ties\n    noise = np.random.uniform(low=0.0, high=1e-6, size=scores.shape)\n    scores += noise\n\n    # Normalise to obtain probabilities\n    probs = np.clip(scores, a_min=0, a_max=None)\n    probs_sum = np.sum(probs)\n    if probs_sum == 0:\n        return None\n    probs /= probs_sum\n\n    # Choose an index according to the probability distribution\n    indices = np.nonzero(non_zero)[0]\n    chosen_index = int(np.random.choice(indices, p=probs[non_zero]))\n    return remaining_subsets[chosen_index]\n\n",
  "roulette_on_gain_and_precision_aug_345": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Proxy precision calculation with deterministic hash\u2011based noise and max\u2011normalisation.\n    \"\"\"\n    rem_set = set(remaining_elements)\n    cand = []\n    w = []\n\n    for s in remaining_subsets:\n        ss = set(s)\n        g = len(ss & rem_set)\n        if g <= 0:\n            continue\n        size = float(len(s))\n        # Proxy precision: gain / (size + 1)\n        precision = g / (size + 1.0 + 1e-12)\n        # Weighted score with tuned constants\n        score = 0.7 * g + 0.3 * precision\n        # Deterministic noise based on the subset contents\n        noise = (hash(tuple(s)) % 1000) * 1e-6\n        score += noise\n        cand.append(s)\n        w.append(score)\n\n    if not cand:\n        return None\n\n    w = np.array(w, dtype=float)\n    max_w = np.max(w)\n    if max_w == 0:\n        return None\n\n    # Normalise by the maximum weight\n    probs = w / max_w\n    probs = np.clip(probs, a_min=0, a_max=None)\n    probs /= np.sum(probs)\n\n    # Randomly pick among the top\u2011k candidates\n    k = 4\n    topk_idx = np.argpartition(-probs, k-1)[:k]\n    probs_top = probs[topk_idx]\n    probs_top /= np.sum(probs_top)\n\n    chosen = int(np.random.choice(topk_idx, p=probs_top))\n    return cand[chosen]\n\n",
  "pick_set_covering_max_support_span_aug_346": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prefer sets whose newly covered elements span a wide support range (max - min).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Build support dictionary\n    support: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                support[e] = support.get(e, 0) + 1\n\n    best, best_score = None, -np.inf\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            idx += 1\n            continue\n\n        vals = [support.get(e, 0) for e in new]\n        span = max(vals) - min(vals) if vals else 0.0\n\n        # Scoring with a tiny deterministic noise for tie\u2011breaking\n        score = len(new) + 0.25 * span - 0.02 * len(ss)\n        noise = 1e-9 * sum(s)\n        score += noise\n\n        # Clip score to avoid extreme values\n        score = np.clip(score, -1e9, 1e9)\n\n        if score > best_score:\n            best_score, best = score, s\n        idx += 1\n\n    return best\n\n",
  "pick_set_covering_max_support_span_aug_347": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prefer sets whose newly covered elements span a wide support range (max - min).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Build support dictionary\n    support: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                support[e] = support.get(e, 0) + 1\n\n    scores = []\n    for s in remaining_subsets:\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            continue\n\n        vals = [support.get(e, 0) for e in new]\n        span = max(vals) - min(vals) if vals else 0.0\n\n        # Tuned weights and deterministic noise\n        score = 0.3 * len(new) + 0.5 * span - 0.01 * len(ss)\n        noise = 1e-9 * sum(s)\n        score += noise\n\n        # Clip to keep values bounded\n        score = np.clip(score, -1e9, 1e9)\n\n        scores.append((score, s))\n\n    if not scores:\n        return None\n\n    # Select top\u2011k candidates and break ties by minimal sum of elements\n    top_k = 7\n    scores.sort(key=lambda x: x[0], reverse=True)\n    best_subset = min(scores[:top_k], key=lambda x: sum(x[1]))[1]\n    return best_subset\n\n",
  "pick_set_covering_max_support_span_aug_348": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prefer sets whose newly covered elements span a wide support range (max - min).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Build support dictionary\n    support: Dict[int, int] = {}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in rem:\n                support[e] = support.get(e, 0) + 1\n\n    scores = []\n    for idx, s in enumerate(remaining_subsets):\n        ss = set(s)\n        new = [e for e in ss if e in rem]\n        if not new:\n            continue\n\n        vals = [support.get(e, 0) for e in new]\n        # Use median instead of span for robustness\n        median_val = np.median(vals) if vals else 0.0\n\n        # Tuned weights and deterministic noise\n        score = 0.4 * len(new) + 0.6 * median_val - 0.015 * len(ss)\n        noise = 1e-9 * sum(s)\n        score += noise\n\n        # Clip score to avoid overflow in exp\n        score = np.clip(score, -50, 50)\n        scores.append((score, idx))\n\n    if not scores:\n        return None\n\n    # Convert to softmax probabilities and pick argmax deterministically\n    scores_arr = np.array([s[0] for s in scores])\n    exp_scores = np.exp(scores_arr - np.max(scores_arr))\n    probs = exp_scores / (np.sum(exp_scores) + 1e-12)  # epsilon to avoid div\u2011zero\n    chosen_idx = np.argmax(probs)\n    best_subset = remaining_subsets[scores[chosen_idx][1]]\n    return best_subset\n\n",
  "pick_set_covering_max_support_span_aug_349": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Prefer sets whose newly covered elements span a wide support range (max - min).\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Determine maximum element to allocate support array\n    max_elem = max(rem) if rem else 0\n    support_arr = np.zeros(max_elem + 1, dtype=int)\n\n    # Populate support array using numpy operations\n    for s in remaining_subsets:\n        for e in set(s):\n            if 0 <= e <= max_elem:\n                support_arr[e] += 1\n\n    best, best_score = None, -np.inf\n    for s in remaining_subsets:\n        ss = np.array(s, dtype=int)\n        # Boolean mask of elements that are still uncovered\n        mask = np.isin(ss, np.array(list(rem), dtype=int))\n        new_elems = ss[mask]\n        if new_elems.size == 0:\n            continue\n\n        # Support values for newly covered elements\n        vals = support_arr[new_elems]\n        avg_support = np.mean(vals) + 1e-12  # epsilon to avoid division\u2011by\u2011zero\n\n        # Scoring with tuned weights and deterministic noise\n        score = 0.35 * new_elems.size + 0.4 * avg_support - 0.02 * ss.size\n        noise = 1e-9 * new_elems.sum()\n        score += noise\n\n        # Clip score to keep it within a safe range\n        score = np.clip(score, -1e9, 1e9)\n\n        if score > best_score:\n            best_score, best = score, s.tolist()\n\n    return best\n\n",
  "balanced_cover_by_support_bins_aug_350": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Build a support dictionary: how many remaining subsets contain each element\n    support: Dict[int, int] = {}\n    for subset in remaining_subsets:\n        for elem in set(subset):\n            if elem in rem_set:\n                support[elem] = support.get(elem, 0) + 1\n\n    if not support:\n        return None\n\n    # Quantile thresholds to classify rarity\n    support_vals = np.array(list(support.values()), dtype=float)\n    q1 = float(np.quantile(support_vals, 0.33))\n    q2 = float(np.quantile(support_vals, 0.66))\n\n    def bin_of(count: int) -> int:\n        # Invert logic: rarest \u2192 bin 2\n        if count >= q2:\n            return 2\n        if count >= q1:\n            return 1\n        return 0\n\n    best_subset = None\n    best_key = None\n\n    # Iterate with a while loop for variety\n    idx = 0\n    while idx < len(remaining_subsets):\n        subset = remaining_subsets[idx]\n        idx += 1\n        subset_set = set(subset)\n        new_elems = [e for e in subset_set if e in rem_set]\n        if not new_elems:\n            continue\n\n        bins_hit = {bin_of(support.get(e, 0)) for e in new_elems}\n        key = (len(bins_hit), len(new_elems), -len(subset_set))\n        if best_key is None or key > best_key:\n            best_key = key\n            best_subset = subset\n\n    return best_subset\n\n",
  "balanced_cover_by_support_bins_aug_351": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Support counts\n    support: Dict[int, int] = {}\n    for subset in remaining_subsets:\n        for elem in set(subset):\n            if elem in rem_set:\n                support[elem] = support.get(elem, 0) + 1\n\n    if not support:\n        return None\n\n    support_vals = np.array(list(support.values()), dtype=float)\n    q1 = float(np.quantile(support_vals, 0.33))\n    q2 = float(np.quantile(support_vals, 0.66))\n\n    def bin_of(count: int) -> int:\n        if count <= q1:\n            return 0\n        if count <= q2:\n            return 1\n        return 2\n\n    best_subset = None\n    best_score = -np.inf\n\n    # Weighted scoring: rare bins more valuable\n    w_bins = 0.5\n    w_new = 0.3\n    w_size = 0.2\n\n    for subset in remaining_subsets:\n        subset_set = set(subset)\n        new_elems = [e for e in subset_set if e in rem_set]\n        if not new_elems:\n            continue\n\n        bins_hit = {bin_of(support.get(e, 0)) for e in new_elems}\n        # Ratio of bins to new elements, clipped to [0,1]\n        ratio = np.clip(len(bins_hit) / (len(new_elems) + 1e-12), 0.0, 1.0)\n        score = w_bins * ratio + w_new * len(new_elems) - w_size * len(subset_set)\n        if score > best_score:\n            best_score = score\n            best_subset = subset\n\n    return best_subset\n\n",
  "balanced_cover_by_support_bins_aug_352": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    support: Dict[int, int] = {}\n    for subset in remaining_subsets:\n        for elem in set(subset):\n            if elem in rem_set:\n                support[elem] = support.get(elem, 0) + 1\n\n    if not support:\n        return None\n\n    support_vals = np.array(list(support.values()), dtype=float)\n    q1 = float(np.quantile(support_vals, 0.33))\n    q2 = float(np.quantile(support_vals, 0.66))\n\n    def bin_of(count: int) -> int:\n        if count < q1:\n            return 0\n        if count < q2:\n            return 1\n        return 2\n\n    scores = []\n    for subset in remaining_subsets:\n        subset_set = set(subset)\n        new_elems = [e for e in subset_set if e in rem_set]\n        if not new_elems:\n            continue\n        bins_hit = {bin_of(support.get(e, 0)) for e in new_elems}\n        ratio = np.clip(len(bins_hit) / (len(new_elems) + 1e-12), 0.0, 1.0)\n        score = ratio * len(new_elems) / (len(subset_set) + 1e-12)\n        scores.append((score, subset))\n\n    if not scores:\n        return None\n\n    # Sort by score and pick a random element among the top\u2011k\n    scores.sort(key=lambda x: x[0], reverse=True)\n    top_k = min(5, len(scores))\n    top_candidates = [s for _, s in scores[:top_k]]\n\n    rng = np.random.default_rng(seed=1234)\n    chosen = rng.choice(top_candidates)\n    return chosen\n\n",
  "balanced_cover_by_support_bins_aug_353": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    support: Dict[int, int] = {}\n    for subset in remaining_subsets:\n        for elem in set(subset):\n            if elem in rem_set:\n                support[elem] = support.get(elem, 0) + 1\n\n    if not support:\n        return None\n\n    support_vals = np.array(list(support.values()), dtype=float)\n    # Use median as a proxy for rarity thresholds\n    median_val = float(np.median(support_vals))\n    q1 = median_val * 0.75\n    q2 = median_val * 1.25\n\n    def bin_of(count: int) -> int:\n        if count <= q1:\n            return 0\n        if count <= q2:\n            return 1\n        return 2\n\n    best_subset = None\n    best_score = -np.inf\n\n    for subset in remaining_subsets:\n        subset_set = set(subset)\n        new_elems = [e for e in subset_set if e in rem_set]\n        if not new_elems:\n            continue\n        bins_hit = {bin_of(support.get(e, 0)) for e in new_elems}\n        # Reward covering the rarest bin\n        bins_score = np.max(list(bins_hit)) if bins_hit else 0\n        size_factor = len(subset_set) + 1e-12\n        noise = np.sum(subset) * 1e-6  # deterministic tie\u2011breaker\n        score = bins_score * len(new_elems) / size_factor + noise\n        if score > best_score:\n            best_score = score\n            best_subset = subset\n\n    return best_subset\n\n",
  "two_stage_pivot_then_diversify_aug_354": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorised version with deterministic noise for tie breaking.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Frequency of each element in remaining_subsets\n    freq: Dict[int, int] = {e: 0 for e in rem_set}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in freq:\n                freq[e] += 1\n\n    # Choose the rarest element\n    piv = min((e for e, f in freq.items() if f > 0), key=lambda e: freq[e], default=None)\n    if piv is None:\n        return None\n\n    sel_sets = [set(s) for s in selected_subsets] if selected_subsets else []\n\n    best, best_score = None, -np.inf\n    for s in remaining_subsets:\n        ss = set(s)\n        if piv not in ss:\n            continue\n        gain = len(rem_set & ss)\n        if gain == 0:\n            continue\n\n        if not sel_sets:\n            dist = 1.0\n        else:\n            dsum = 0.0\n            for t in sel_sets:\n                inter = len(ss & t)\n                uni = len(ss | t)\n                jaccard = inter / (uni + 1e-12)\n                dsum += 1.0 - np.clip(jaccard, 0.0, 1.0)\n            dist = dsum / len(sel_sets)\n\n        penalty = len(ss)\n        score = gain + 0.6 * dist - 0.01 * penalty\n        # deterministic noise to break ties\n        noise = (hash(tuple(sorted(ss))) % 1000) / 1e6\n        score += noise\n\n        if score > best_score:\n            best_score, best = score, s\n\n    if best is not None:\n        return best\n\n    # fallback: best set by gain\n    best, best_gain = None, 0\n    for s in remaining_subsets:\n        g = len(rem_set & set(s))\n        if g > best_gain:\n            best_gain, best = g, s\n    return best if best_gain > 0 else None\n\n",
  "two_stage_pivot_then_diversify_aug_355": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"While\u2011loop variant with median distance and deterministic top\u2011k selection.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Build frequency dictionary\n    freq: Dict[int, int] = {e: 0 for e in rem_set}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in freq:\n                freq[e] += 1\n\n    # Rarest element\n    piv = None\n    piv_f = np.inf\n    for e, f in freq.items():\n        if 0 < f < piv_f:\n            piv_f, piv = f, e\n    if piv is None:\n        return None\n\n    sel_sets = [set(s) for s in selected_subsets] if selected_subsets else []\n\n    scores = []\n    for s in remaining_subsets:\n        ss = set(s)\n        if piv not in ss:\n            continue\n        gain = len(rem_set & ss)\n        if gain == 0:\n            continue\n\n        # Distances to already selected sets\n        dists = []\n        if sel_sets:\n            for t in sel_sets:\n                inter = len(ss & t)\n                uni = len(ss | t)\n                jaccard = inter / (uni + 1e-12)\n                dists.append(1.0 - np.clip(jaccard, 0.0, 1.0))\n            dist = np.median(dists)\n        else:\n            dist = 1.0\n\n        penalty = len(ss)\n        score = gain + 0.5 * dist - 0.3 * penalty\n        scores.append((score, s))\n\n    if not scores:\n        return None\n\n    # Take top\u20113 scores deterministically\n    scores.sort(key=lambda x: x[0], reverse=True)\n    top_k = min(3, len(scores))\n    top_candidates = scores[:top_k]\n    # deterministic tie\u2011break: smallest hash\n    chosen = min(top_candidates, key=lambda x: hash(tuple(sorted(x[1]))))[1]\n    return chosen\n\n",
  "two_stage_pivot_then_diversify_aug_356": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Soft\u2011min probabilistic selection over the top\u20115 candidates.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Frequency of each element\n    freq: Dict[int, int] = {e: 0 for e in rem_set}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in freq:\n                freq[e] += 1\n\n    # Rarest element\n    piv = min((e for e, f in freq.items() if f > 0), key=lambda e: freq[e], default=None)\n    if piv is None:\n        return None\n\n    sel_sets = [set(s) for s in selected_subsets] if selected_subsets else []\n\n    candidate_scores = []\n    for s in remaining_subsets:\n        ss = set(s)\n        if piv not in ss:\n            continue\n        gain = len(rem_set & ss)\n        if gain == 0:\n            continue\n\n        if sel_sets:\n            dists = []\n            for t in sel_sets:\n                inter = len(ss & t)\n                uni = len(ss | t)\n                jaccard = inter / (uni + 1e-12)\n                dists.append(1.0 - np.clip(jaccard, 0.0, 1.0))\n            dist = np.mean(dists)\n        else:\n            dist = 1.0\n\n        penalty = len(ss)\n        score = gain + 0.4 * dist - 0.2 * penalty\n        candidate_scores.append((score, s))\n\n    if not candidate_scores:\n        return None\n\n    # Keep only top\u20115 scores\n    candidate_scores.sort(key=lambda x: x[0], reverse=True)\n    top_k = min(5, len(candidate_scores))\n    top_candidates = candidate_scores[:top_k]\n\n    # Soft\u2011min probabilities\n    scores = np.array([c[0] for c in top_candidates])\n    weights = np.exp(-scores)  # soft\u2011min\n    probs = weights / (np.sum(weights) + 1e-12)\n\n    chosen_idx = np.random.choice(len(top_candidates), p=probs)\n    return top_candidates[chosen_idx][1]\n\n",
  "two_stage_pivot_then_diversify_aug_357": "import numpy as np\nfrom typing import List, Optional, Dict, Set\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Proxy Jaccard approximation with deterministic tie\u2011break by set size.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    freq: Dict[int, int] = {e: 0 for e in rem_set}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in freq:\n                freq[e] += 1\n\n    piv = min((e for e, f in freq.items() if f > 0), key=lambda e: freq[e], default=None)\n    if piv is None:\n        return None\n\n    sel_sets = [set(s) for s in selected_subsets] if selected_subsets else []\n\n    scores = []\n    for s in remaining_subsets:\n        ss = set(s)\n        if piv not in ss:\n            continue\n        gain = len(rem_set & ss)\n        if gain == 0:\n            continue\n\n        if sel_sets:\n            dists = []\n            for t in sel_sets:\n                inter = len(ss & t)\n                # proxy union: |ss| + |t| - inter\n                union_est = len(ss) + len(t) - inter\n                jaccard = inter / (union_est + 1e-12)\n                dists.append(1.0 - np.clip(jaccard, 0.0, 1.0))\n            dist = np.max(dists)\n        else:\n            dist = 1.0\n\n        penalty = len(ss)\n        score = 0.4 * dist + 0.5 * gain - 0.1 * penalty\n        scores.append((score, s))\n\n    if not scores:\n        return None\n\n    # Top\u20117 candidates\n    scores.sort(key=lambda x: x[0], reverse=True)\n    top_k = min(7, len(scores))\n    top_candidates = scores[:top_k]\n\n    # Deterministic tie\u2011break: smallest set size, then smallest hash\n    chosen = min(top_candidates,\n                 key=lambda x: (len(x[1]), hash(tuple(sorted(x[1])))))[1]\n    return chosen\n\n",
  "singleton_completion_bias_aug_358": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorised implementation that scores subsets by singletons, gain and size.\n    Adds deterministic noise and uses an epsilon in divisions for safety.\"\"\"\n    rem = np.array(remaining_elements, dtype=int)\n    if rem.size == 0:\n        return None\n\n    # Map remaining elements to column indices\n    elem_to_idx = {e: i for i, e in enumerate(rem)}\n    n_sets = len(remaining_subsets)\n    mask = np.zeros((n_sets, rem.size), dtype=bool)\n\n    for i, s in enumerate(remaining_subsets):\n        idxs = [elem_to_idx[e] for e in s if e in elem_to_idx]\n        mask[i, idxs] = True\n\n    # Count how many sets cover each element\n    freq = mask.sum(axis=0)\n    singletons = np.where(freq == 1)[0]\n\n    gains = mask.sum(axis=1)\n    if not np.any(gains > 0):\n        return None\n\n    # Number of singletons each set covers\n    sc = mask[:, singletons].sum(axis=1)\n\n    size = np.array([len(s) for s in remaining_subsets], dtype=float)\n\n    eps = 1e-12\n    # Score: singletons + gain/(1+eps) \u2013 size/(1+eps)\n    score = sc + gains / (1 + eps) - size / (1 + eps)\n    score = np.clip(score, -1e6, 1e6)\n\n    # Deterministic noise for tie\u2011breaking\n    rng = np.random.default_rng(12345)\n    score += rng.uniform(0, 1e-6, size=n_sets)\n\n    idx = int(np.argmax(score))\n    return remaining_subsets[idx]\n\n",
  "singleton_completion_bias_aug_359": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Soft\u2011weighted scoring with tuned coefficients and deterministic noise.\"\"\"\n    rem = np.array(remaining_elements, dtype=int)\n    if rem.size == 0:\n        return None\n\n    elem_to_idx = {e: i for i, e in enumerate(rem)}\n    n_sets = len(remaining_subsets)\n    mask = np.zeros((n_sets, rem.size), dtype=bool)\n\n    for i, s in enumerate(remaining_subsets):\n        idxs = [elem_to_idx[e] for e in s if e in elem_to_idx]\n        mask[i, idxs] = True\n\n    gains = mask.sum(axis=1)\n    if not np.any(gains > 0):\n        return None\n\n    sc = mask[:, mask.sum(axis=0) == 1].sum(axis=1)\n    size = np.array([len(s) for s in remaining_subsets], dtype=float)\n\n    eps = 1e-12\n    # Weighted score: 0.6*sc + 0.3*gain/(1+eps) \u2013 0.05*size/(1+eps)\n    score = 0.6 * sc + 0.3 * (gains / (1 + eps)) - 0.05 * (size / (1 + eps))\n    score = np.clip(score, -1e6, 1e6)\n\n    rng = np.random.default_rng(42)\n    score += rng.uniform(0, 1e-6, size=n_sets)\n\n    idx = int(np.argmax(score))\n    return remaining_subsets[idx]\n\n",
  "singleton_completion_bias_aug_360": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Randomly selects among the top\u2011k best\u2011scoring subsets.\"\"\"\n    rem = np.array(remaining_elements, dtype=int)\n    if rem.size == 0:\n        return None\n\n    elem_to_idx = {e: i for i, e in enumerate(rem)}\n    n_sets = len(remaining_subsets)\n    mask = np.zeros((n_sets, rem.size), dtype=bool)\n\n    for i, s in enumerate(remaining_subsets):\n        idxs = [elem_to_idx[e] for e in s if e in elem_to_idx]\n        mask[i, idxs] = True\n\n    gains = mask.sum(axis=1)\n    if not np.any(gains > 0):\n        return None\n\n    sc = mask[:, mask.sum(axis=0) == 1].sum(axis=1)\n    size = np.array([len(s) for s in remaining_subsets], dtype=float)\n\n    eps = 1e-12\n    # Combined score with tuned coefficients\n    score = 0.5 * sc + 0.4 * (gains / (1 + eps)) - 0.1 * (size / (1 + eps))\n    score = np.clip(score, -1e6, 1e6)\n\n    rng = np.random.default_rng(999)\n    score += rng.uniform(0, 1e-6, size=n_sets)\n\n    top_k = min(3, n_sets)\n    top_indices = np.argpartition(-score, top_k - 1)[:top_k]\n    chosen_idx = int(rng.choice(top_indices))\n    return remaining_subsets[chosen_idx]\n\n",
  "singleton_completion_bias_aug_361": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Median\u2011gain fallback with approximate singleton coverage.\"\"\"\n    rem = np.array(remaining_elements, dtype=int)\n    if rem.size == 0:\n        return None\n\n    elem_to_idx = {e: i for i, e in enumerate(rem)}\n    n_sets = len(remaining_subsets)\n    mask = np.zeros((n_sets, rem.size), dtype=bool)\n\n    for i, s in enumerate(remaining_subsets):\n        idxs = [elem_to_idx[e] for e in s if e in elem_to_idx]\n        mask[i, idxs] = True\n\n    gains = mask.sum(axis=1)\n    if not np.any(gains > 0):\n        return None\n\n    # Approximate singleton coverage: ratio of raw singletons to set size\n    freq = mask.sum(axis=0)\n    singletons = np.where(freq == 1)[0]\n    sc_raw = mask[:, singletons].sum(axis=1)\n    size = np.array([len(s) for s in remaining_subsets], dtype=float)\n\n    eps = 1e-12\n    sc_approx = np.clip(sc_raw / (size + eps), 0, 1e6)\n\n    # Median gain as threshold for candidate selection\n    median_gain = np.median(gains[gains > 0])\n    candidates = np.where(gains >= median_gain)[0]\n    if candidates.size == 0:\n        candidates = np.where(gains > 0)[0]\n\n    best_idx = int(candidates[np.argmax(sc_approx[candidates])])\n    return remaining_subsets[best_idx]\n\n",
  "anti_deadend_guard_aug_362": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selection with vectorized support counting, deterministic noise for tie\u2011breaking,\n    and a soft penalty for low\u2011support elements. Uses np.clip to keep scores bounded.\n    \"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    # Support count for each remaining element\n    support = {e: 0 for e in rem0}\n    for s in remaining_subsets:\n        for e in s:\n            if e in support:\n                support[e] += 1\n\n    # Elements that are covered by at most two remaining subsets\n    low_support = {e for e, c in support.items() if 0 < c <= 2}\n\n    best_subset: Optional[List[int]] = None\n    best_score = -1e12\n\n    for s in remaining_subsets:\n        ss = set(s)\n        gain = len(rem0 & ss)\n        if gain <= 0:\n            continue\n\n        rem1 = rem0 - ss\n        low_left = sum(1 for e in rem1 if e in low_support)\n\n        # Score with a small positive bias for the ratio of low_left to gain\n        score = (\n            gain\n            - 0.8 * low_left\n            - 0.01 * len(ss)\n            + 0.05 * (low_left / (gain + 1e-12))\n        )\n\n        # Deterministic noise for tie\u2011breaking\n        score += np.random.uniform(-1e-6, 1e-6)\n\n        # Keep scores within a safe numerical range\n        score = np.clip(score, -1e9, 1e9)\n\n        if score > best_score:\n            best_score, best_subset = score, s\n\n    return best_subset\n\n",
  "anti_deadend_guard_aug_363": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Uses a while\u2011loop to iterate over subsets and selects one via a softmin\n    probability distribution. np.clip protects against overflow and division by zero.\n    \"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    # Support counting\n    support = {e: 0 for e in rem0}\n    for s in remaining_subsets:\n        for e in s:\n            if e in support:\n                support[e] += 1\n\n    low_support = {e for e, c in support.items() if 0 < c <= 2}\n\n    scores = []\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        ss = set(s)\n        gain = len(rem0 & ss)\n        if gain > 0:\n            rem1 = rem0 - ss\n            low_left = sum(1 for e in rem1 if e in low_support)\n            score = gain - 0.5 * low_left - 0.02 * len(ss)\n            scores.append((s, score))\n        idx += 1\n\n    if not scores:\n        return None\n\n    # Softmin probabilities\n    score_arr = np.array([sc[1] for sc in scores])\n    # Subtract min to avoid large exponents\n    exp_vals = np.exp(-(score_arr - np.min(score_arr)) / (0.5 + 1e-12))\n    exp_vals = np.clip(exp_vals, None, 1e300)  # prevent overflow\n    probs = exp_vals / (np.sum(exp_vals) + 1e-12)\n\n    # Randomly pick according to softmin distribution\n    choice = np.random.choice(len(scores), p=probs)\n    return scores[choice][0]\n\n",
  "anti_deadend_guard_aug_364": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    List\u2011comprehension based scoring with median aggregation of low\u2011support counts.\n    Adds a small epsilon to the denominator to avoid division by zero and clips\n    the final score to keep it bounded.\n    \"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    support = {e: 0 for e in rem0}\n    for s in remaining_subsets:\n        for e in s:\n            if e in support:\n                support[e] += 1\n\n    low_support = {e for e, c in support.items() if 0 < c <= 2}\n\n    # Compute scores for all subsets\n    subset_scores = [\n        (\n            s,\n            (\n                len(rem0 & set(s))\n                - 0.6 * sum(1 for e in (rem0 - set(s)) if e in low_support)\n                - 0.02 * len(s)\n                + 0.03 * (\n                    sum(1 for e in (rem0 - set(s)) if e in low_support)\n                    / (len(rem0 & set(s)) + 1e-12)\n                )\n            )\n        )\n        for s in remaining_subsets\n        if len(rem0 & set(s)) > 0\n    ]\n\n    if not subset_scores:\n        return None\n\n    # Add deterministic noise and clip\n    noisy_scores = [\n        (s, np.clip(score + np.random.uniform(-5e-7, 5e-7), -1e8, 1e8))\n        for s, score in subset_scores\n    ]\n\n    # Select subset with maximum score\n    best_subset = max(noisy_scores, key=lambda x: x[1])[0]\n    return best_subset\n\n",
  "anti_deadend_guard_aug_365": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Proxy\u2011based support counting using np.bincount, top\u2011k selection followed by\n    a weighted random choice. All numerical operations are protected with\n    np.clip and epsilon additions.\n    \"\"\"\n    rem0 = set(remaining_elements)\n    if not rem0:\n        return None\n\n    max_elem = max(rem0) if rem0 else 0\n    # Approximate support counts via bincount\n    support_arr = np.zeros(max_elem + 1, dtype=int)\n    for s in remaining_subsets:\n        for e in s:\n            if e <= max_elem:\n                support_arr[e] += 1\n\n    low_support_arr = (support_arr > 0) & (support_arr <= 2)\n\n    # Vectorized scoring\n    scores = []\n    for s in remaining_subsets:\n        ss = set(s)\n        gain = len(rem0 & ss)\n        if gain == 0:\n            continue\n        rem1 = rem0 - ss\n        low_left = sum(1 for e in rem1 if low_support_arr[e])\n        score = (\n            gain\n            - 0.55 * low_left\n            - 0.015 * len(ss)\n            + 0.04 * (low_left / (gain + 1e-12))\n        )\n        scores.append((s, score))\n\n    if not scores:\n        return None\n\n    # Select top-3 scores\n    top_k = 3\n    scores_sorted = sorted(scores, key=lambda x: x[1], reverse=True)\n    top_scores = scores_sorted[:min(top_k, len(scores_sorted))]\n\n    # Random choice among top\u2011k with softmax weighting\n    score_vals = np.array([sc[1] for sc in top_scores])\n    exp_vals = np.exp((score_vals - np.max(score_vals)) / (0.3 + 1e-12))\n    exp_vals = np.clip(exp_vals, None, 1e300)\n    probs = exp_vals / (np.sum(exp_vals) + 1e-12)\n\n    chosen = np.random.choice(len(top_scores), p=probs)\n    return top_scores[chosen][0]\n\n",
  "weighted_waste_f1_hybrid_aug_366": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Vectorised score computation with explicit clipping and epsilon protection.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Parameters\n    alpha = 0.25\n    eps = 1e-12\n\n    # Pre\u2011compute metrics for all remaining subsets\n    tp_arr = np.array([len(set(sub) & rem_set) for sub in remaining_subsets], dtype=float)\n    len_sub_arr = np.array([len(sub) for sub in remaining_subsets], dtype=float)\n    waste_arr = np.array([len(set(sub) - rem_set) for sub in remaining_subsets], dtype=float)\n\n    # Filter out subsets that cover no new elements\n    valid_mask = tp_arr > 0\n    if not np.any(valid_mask):\n        return None\n\n    tp = tp_arr[valid_mask]\n    len_sub = len_sub_arr[valid_mask]\n    waste = waste_arr[valid_mask]\n\n    # Precision and recall with clipping\n    prec = tp / np.clip(len_sub, a_min=eps, a_max=None)\n    rec  = tp / np.clip(len(rem_set), a_min=eps, a_max=None)\n\n    # F1\u2011score (harmonic mean)\n    f1 = 2.0 * prec * rec / np.clip(prec + rec, a_min=eps, a_max=None)\n\n    # Final score\n    score = f1 + 0.02 * tp - alpha * (waste / np.clip(len_sub, a_min=eps, a_max=None))\n\n    # Choose the subset with the maximum score\n    best_idx = np.argmax(score)\n    return remaining_subsets[valid_mask][best_idx]\n\n",
  "weighted_waste_f1_hybrid_aug_367": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Weighted scoring with deterministic noise and top\u2011k tie handling.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Hyper\u2011parameters\n    alpha = 0.30\n    noise_scale = 1e-6\n    rng = np.random.default_rng(seed=42)          # deterministic noise\n    top_k = 3\n\n    # Compute scores for all subsets\n    scores = []\n    for sub in remaining_subsets:\n        sub_set = set(sub)\n        tp = len(sub_set & rem_set)\n        if tp == 0:\n            continue\n        prec = tp / max(1.0, float(len(sub_set)))\n        rec  = tp / max(1.0, float(len(rem_set)))\n        f1 = (2.0 * prec * rec) / max(1e-12, prec + rec)\n        waste = len(sub_set - rem_set)\n        score = f1 + 0.03 * tp - alpha * (waste / max(1.0, float(len(sub_set))))\n        # Add deterministic noise to break ties\n        score += rng.uniform(-noise_scale, noise_scale)\n        scores.append((score, sub))\n\n    if not scores:\n        return None\n\n    # Sort by score descending\n    scores.sort(key=lambda x: x[0], reverse=True)\n\n    # If several top scores are very close, randomly pick among the top\u2011k\n    best_score = scores[0][0]\n    close_indices = [i for i, (sc, _) in enumerate(scores) if sc >= best_score - 1e-9]\n    if len(close_indices) > 1:\n        chosen_idx = rng.choice(close_indices[:top_k])\n    else:\n        chosen_idx = close_indices[0]\n\n    return scores[chosen_idx][1]\n\n",
  "weighted_waste_f1_hybrid_aug_368": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Soft\u2011min selection with temperature\u2011controlled sampling over top\u2011k candidates.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Hyper\u2011parameters\n    alpha = 0.20\n    temperature = 0.5\n    top_k = 4\n    rng = np.random.default_rng()\n\n    # Compute raw scores\n    raw_scores = []\n    for sub in remaining_subsets:\n        sub_set = set(sub)\n        tp = len(sub_set & rem_set)\n        if tp == 0:\n            continue\n        prec = tp / max(1.0, float(len(sub_set)))\n        rec  = tp / max(1.0, float(len(rem_set)))\n        f1 = (2.0 * prec * rec) / max(1e-12, prec + rec)\n        waste = len(sub_set - rem_set)\n        score = f1 + 0.02 * tp - alpha * (waste / max(1.0, float(len(sub_set))))\n        raw_scores.append((score, sub))\n\n    if not raw_scores:\n        return None\n\n    # Extract scores and convert to numpy array\n    scores_arr = np.array([sc for sc, _ in raw_scores], dtype=float)\n\n    # Soft\u2011min probabilities (lower scores -> higher probability)\n    probs = np.exp(-scores_arr / temperature)\n    probs /= probs.sum()\n\n    # Select top\u2011k indices\n    top_indices = np.argpartition(-scores_arr, top_k - 1)[:top_k]\n    top_probs = probs[top_indices]\n    top_probs /= top_probs.sum()\n\n    # Sample one candidate from the top\u2011k\n    chosen_idx = rng.choice(top_indices, p=top_probs)\n    return raw_scores[chosen_idx][1]\n\n",
  "weighted_waste_f1_hybrid_aug_369": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Approximate F1 using mean of precision and recall, with clipping and noise.\"\"\"\n    rem_set = set(remaining_elements)\n    if not rem_set:\n        return None\n\n    # Parameters\n    alpha = 0.35\n    eps = 1e-12\n    noise_scale = 1e-7\n    rng = np.random.default_rng(seed=7)\n\n    best_candidate = None\n    best_score = -np.inf\n\n    for sub in remaining_subsets:\n        sub_set = set(sub)\n        tp = len(sub_set & rem_set)\n        if tp == 0:\n            continue\n        # Approximate precision and recall\n        prec = tp / np.clip(len(sub_set), a_min=eps, a_max=None)\n        rec  = tp / np.clip(len(rem_set), a_min=eps, a_max=None)\n        # Use mean of prec and rec as proxy for F1\n        proxy_f1 = np.clip((prec + rec) / 2.0, a_min=eps, a_max=None)\n        waste = len(sub_set - rem_set)\n        waste_ratio = waste / np.clip(len(sub_set), a_min=eps, a_max=None)\n        score = proxy_f1 + 0.015 * tp - alpha * waste_ratio\n        # Add tiny noise for deterministic tie\u2011breaking\n        score += rng.uniform(-noise_scale, noise_scale)\n        if score > best_score:\n            best_score, best_candidate = score, sub\n\n    return best_candidate\n\n",
  "rare_then_pack_dense_aug_370": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Select next subset by prioritizing critical elements and using a dense\u2011pack fallback.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Frequency of each uncovered element\n    freq: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in freq:\n                freq[e] += 1\n\n    critical = {e for e, c in freq.items() if 0 < c <= 2}\n\n    best_subset = None\n    best_score = -np.inf\n\n    if critical:\n        for s in remaining_subsets:\n            ss = set(s)\n            gain = len(rem & ss)\n            if gain <= 0:\n                continue\n            crit_cov = len(critical & ss)\n            # deterministic tie\u2011breaker: small noise proportional to sum of elements\n            noise = sum(ss) * 1e-6\n            score = 3.0 * crit_cov + 0.2 * gain - 0.01 * len(ss) + noise\n            score = np.clip(score, -1e6, 1e6)          # keep score bounded\n            if score > best_score:\n                best_score, best_subset = score, s\n        return best_subset\n\n    # Dense\u2011pack fallback: maximize gain / size\n    for s in remaining_subsets:\n        ss = set(s)\n        gain = len(rem & ss)\n        if gain <= 0:\n            continue\n        denom = max(1.0, float(len(ss))) + 1e-12      # epsilon to avoid div\u2011by\u2011zero\n        score = gain / denom\n        if score > best_score:\n            best_score, best_subset = score, s\n    return best_subset\n\n",
  "rare_then_pack_dense_aug_371": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Use a while loop and random choice among the top\u2011k dense subsets.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    freq: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            freq[e] = freq.get(e, 0) + 1\n\n    critical = {e for e, c in freq.items() if 0 < c <= 3}\n\n    best_subset = None\n    best_score = -np.inf\n\n    if critical:\n        idx = 0\n        while idx < len(remaining_subsets):\n            s = remaining_subsets[idx]\n            ss = set(s)\n            gain = len(rem & ss)\n            if gain == 0:\n                idx += 1\n                continue\n            crit_cov = len(critical & ss)\n            score = 2.2 * crit_cov + 0.15 * gain - 0.02 * len(ss)\n            score = np.clip(score, -1e6, 1e6)\n            if score > best_score:\n                best_score, best_subset = score, s\n            idx += 1\n        return best_subset\n\n    # Fallback: pick randomly from the top\u20115 dense subsets\n    scores = []\n    for s in remaining_subsets:\n        ss = set(s)\n        gain = len(rem & ss)\n        if gain == 0:\n            scores.append(-np.inf)\n            continue\n        denom = max(1.0, float(len(ss))) + 1e-12\n        scores.append(gain / denom)\n\n    scores_np = np.array(scores)\n    scores_np = np.clip(scores_np, -1e6, 1e6)\n    top_k = 5\n    top_indices = np.argsort(-scores_np)[:top_k]\n    chosen_idx = np.random.choice(top_indices)\n    return remaining_subsets[chosen_idx]\n\n",
  "rare_then_pack_dense_aug_372": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Use mean\u2011based aggregation and size\u2011based tie breaking.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    freq: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in freq:\n                freq[e] += 1\n\n    critical = {e for e, c in freq.items() if 0 < c <= 2}\n\n    best_subset = None\n    best_score = -np.inf\n\n    if critical:\n        for s in remaining_subsets:\n            ss = set(s)\n            gain = len(rem & ss)\n            if gain == 0:\n                continue\n            crit_cov = len(critical & ss)\n            score = 2.0 * crit_cov + 0.15 * gain - 0.015 * len(ss)\n            score = np.clip(score, 0, 10)            # bound score\n            if score > best_score or (score == best_score and\n                                      (best_subset is None or len(ss) < len(best_subset))):\n                best_score, best_subset = score, s\n        return best_subset\n\n    # Fallback: mean density of new elements\n    scores = []\n    for s in remaining_subsets:\n        ss = set(s)\n        new = len(rem & ss)\n        if new == 0:\n            scores.append(-np.inf)\n            continue\n        scores.append(new / (len(ss) + 1e-12))\n    scores_np = np.array(scores)\n    best_idx = np.argmax(scores_np)\n    return remaining_subsets[best_idx]\n\n",
  "rare_then_pack_dense_aug_373": "import numpy as np\nfrom typing import List, Optional, Dict\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"Apply a soft\u2011max selection over critical subsets and a sqrt\u2011size fallback.\"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    freq: Dict[int, int] = {e: 0 for e in rem}\n    for s in remaining_subsets:\n        for e in set(s):\n            if e in freq:\n                freq[e] += 1\n\n    critical = {e for e, c in freq.items() if 0 < c <= 2}\n\n    if critical:\n        scores = []\n        for s in remaining_subsets:\n            ss = set(s)\n            gain = len(rem & ss)\n            if gain == 0:\n                scores.append(-np.inf)\n                continue\n            crit_cov = len(critical & ss)\n            score = 1.8 * crit_cov + 0.12 * gain - 0.02 * len(ss)\n            scores.append(score)\n        scores_np = np.array(scores)\n        # soft\u2011max probabilities\n        exp_scores = np.exp(scores_np - np.max(scores_np))\n        denom = np.clip(np.sum(exp_scores), 1e-12, None)\n        probs = exp_scores / denom\n        chosen_idx = np.random.choice(len(remaining_subsets), p=probs)\n        return remaining_subsets[chosen_idx]\n\n    # Fallback: density with sqrt size\n    scores = []\n    for s in remaining_subsets:\n        ss = set(s)\n        gain = len(rem & ss)\n        if gain == 0:\n            scores.append(-np.inf)\n            continue\n        denom = np.sqrt(max(1.0, float(len(ss)))) + 1e-12\n        scores.append(gain / denom)\n    scores_np = np.array(scores)\n    best_idx = np.argmax(scores_np)\n    return remaining_subsets[best_idx]\n\n",
  "regret_minimization_proxy_aug_374": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Greedy selection with deterministic noise and vectorized set operations.\n    \"\"\"\n    # Convert remaining elements to a numpy array for fast operations\n    rem = np.array(remaining_elements, dtype=int)\n    if rem.size == 0:\n        return None\n\n    # Prepare containers for gains, waste, and candidate subsets\n    gains = []\n    waste_vals = []\n    candidates = []\n\n    for s in remaining_subsets:\n        s_arr = np.array(s, dtype=int)\n        inter = np.intersect1d(s_arr, rem, assume_unique=False)\n        g = inter.size\n        if g > 0:\n            gains.append(g)\n            # Waste is the number of elements in the subset that are not needed\n            waste_vals.append(np.clip(s_arr.size - g, 0, None))\n            candidates.append(s)\n\n    if not candidates:\n        return None\n\n    best_gain = max(gains)\n    beta = 0.2  # tuned weight for waste\n\n    best, best_regret = None, 1e18\n    for s, g, w in zip(candidates, gains, waste_vals):\n        # Deterministic tie\u2011breaker using the subset hash\n        noise = (hash(tuple(s)) % 1000) * 1e-6\n        regret = (best_gain - g) + beta * w + noise\n        if regret < best_regret:\n            best_regret, best = regret, s\n\n    return best\n\n",
  "regret_minimization_proxy_aug_375": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Uses a while loop, median gain, and random choice among the best k candidates.\n    \"\"\"\n    elems_rem = set(remaining_elements)\n    if not elems_rem:\n        return None\n\n    cand_sets = []\n    gains = []\n    waste = []\n\n    idx = 0\n    while idx < len(remaining_subsets):\n        s = remaining_subsets[idx]\n        s_set = set(s)\n        g = len(s_set & elems_rem)\n        if g > 0:\n            cand_sets.append(s)\n            gains.append(g)\n            waste.append(len(s_set - elems_rem))\n        idx += 1\n\n    if not cand_sets:\n        return None\n\n    best_gain = int(np.median(gains))  # median instead of max\n    beta = 0.25\n\n    regrets = [(best_gain - g) + beta * w for g, w in zip(gains, waste)]\n\n    # Pick one of the top_k lowest regret subsets at random\n    top_k = 5\n    sorted_idx = np.argsort(regrets)\n    top_idx = sorted_idx[:top_k]\n    rng = np.random.default_rng(42)  # deterministic seed\n    chosen = rng.choice([cand_sets[i] for i in top_idx])\n\n    return chosen\n\n",
  "regret_minimization_proxy_aug_376": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    Applies a soft\u2011min weighting scheme based on a proxy waste measure.\n    \"\"\"\n    rem_set = np.array(remaining_elements, dtype=int)\n    if rem_set.size == 0:\n        return None\n\n    gains = []\n    waste = []\n    cand = []\n\n    for s in remaining_subsets:\n        s_arr = np.array(s, dtype=int)\n        inter = np.intersect1d(s_arr, rem_set, assume_unique=False)\n        g = inter.size\n        if g > 0:\n            cand.append(s)\n            gains.append(g)\n            # Proxy waste: mean absolute difference between subset and remaining\n            waste.append(np.mean(np.abs(s_arr - rem_set)))\n\n    if not cand:\n        return None\n\n    best_gain = max(gains)\n    beta = 0.3\n\n    waste_arr = np.array(waste, dtype=float)\n    # Soft\u2011min weighting\n    exp_vals = np.exp(-beta * waste_arr)\n    probs = exp_vals / (np.sum(exp_vals) + 1e-12)  # epsilon to avoid div\u2011by\u2011zero\n    chosen_idx = np.argmax(probs)\n\n    return cand[chosen_idx]\n\n",
  "regret_minimization_proxy_aug_377": "import numpy as np\nfrom typing import List, Optional\n\ndef select_next_subset(selected_subsets: List[List[int]], remaining_subsets: List[List[int]], remaining_elements: List[int]) -> Optional[List[int]]:\n    \"\"\"\n    List\u2011comprehension based candidate extraction and random top\u2011k selection.\n    \"\"\"\n    rem = set(remaining_elements)\n    if not rem:\n        return None\n\n    # Build candidates in one line\n    cand = [s for s in remaining_subsets if len(set(s) & rem) > 0]\n    if not cand:\n        return None\n\n    # Compute gains and waste with clipping to keep non\u2011negative\n    gains = np.array([len(set(s) & rem) for s in cand], dtype=int)\n    waste = np.clip(np.array([len(set(s) - rem) for s in cand], dtype=int), 0, None)\n\n    beta = 0.18\n    best_gain = int(np.max(gains))\n    regrets = (best_gain - gains) + beta * waste\n\n    # Randomly pick among the three lowest\u2011regret subsets\n    top_k = 3\n    top_indices = np.argpartition(regrets, top_k)[:top_k]\n    rng = np.random.default_rng(7)  # deterministic seed\n    chosen = rng.choice([cand[i] for i in top_indices])\n\n    return chosen\n\n"
}